{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "from seqeval.scheme import auto_detect\n",
    "from seqeval.metrics.sequence_labeling import get_entities\n",
    "from seqeval.scheme import Entities\n",
    "\n",
    "\n",
    "class StrictConfusionMatrix:\n",
    "    def __init__(self, y_true, y_pred):\n",
    "        self.y_true = y_true\n",
    "        self.y_pred = y_pred\n",
    "    \n",
    "    def compute(self):\n",
    "        \"\"\"\n",
    "        Compute confusion matrix, false positives, and false negatives for all entities.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing:\n",
    "                - 'confusion_matrix': The confusion matrix for all entity types.\n",
    "                - 'false_negatives': A dictionary with false negative counts categorized by type and subcategory.\n",
    "                - 'false_positives': A dictionary with false positive counts categorized by type and subcategory.\n",
    "        \"\"\"\n",
    "        # Prepare entities (this initializes and formats entities based on the input scheme)\n",
    "        self.prepare_entities()\n",
    "\n",
    "        # Compute the confusion matrix for all entities\n",
    "        confusion_matrix = self.compute_confusion_matrix()\n",
    "\n",
    "        # Initialize dictionaries for false negatives and false positives\n",
    "        false_negatives = defaultdict(Counter)\n",
    "        false_positives = defaultdict(Counter)\n",
    "\n",
    "        # Get all unique entity types from the data\n",
    "        entity_types = set(\n",
    "            ent[1] for ent in self.true_entities\n",
    "        ).union(set(ent[1] for ent in self.pred_entities))\n",
    "\n",
    "        # Iterate over all entity types to calculate false negatives and positives\n",
    "        for entity_type in entity_types:\n",
    "            # Compute false negatives for this type\n",
    "            fn_counts = self.compute_false_negatives(entity_type)\n",
    "\n",
    "            # Compute false positives for this type\n",
    "            fp_counts = self.compute_false_positives(entity_type)\n",
    "\n",
    "            # Merge the results into the global dictionaries\n",
    "            for t_type, counts in fn_counts.items():\n",
    "                for subtype, count in counts.items():\n",
    "                    false_negatives[t_type][subtype] += count\n",
    "\n",
    "            for t_type, counts in fp_counts.items():\n",
    "                for subtype, count in counts.items():\n",
    "                    false_positives[t_type][subtype] += count\n",
    "\n",
    "        # Return the aggregated results\n",
    "        return {\n",
    "            'confusion_matrix': confusion_matrix,\n",
    "            'false_negatives': dict(false_negatives),  # Convert to standard dict for output clarity\n",
    "            'false_positives': dict(false_positives),  # Convert to standard dict for output clarity\n",
    "        }\n",
    "\n",
    "    \n",
    "    \n",
    "    def prepare_entities(self):\n",
    "        # Initialize true and predicted entities\n",
    "        self.scheme = auto_detect(self.y_true, False)\n",
    "        entities_true = self.extract_entities(self.y_true)\n",
    "        entities_pred = self.extract_entities(self.y_pred)\n",
    "        self.true_entities = self.flatten_strict_entities(entities_true)\n",
    "        self.pred_entities = self.flatten_strict_entities(entities_pred)\n",
    "\n",
    "    def extract_entities(self, y_data):\n",
    "        # Replace with the Entities() logic if provided\n",
    "        return Entities(y_data, self.scheme, False)\n",
    "\n",
    "    @staticmethod\n",
    "    def flatten_strict_entities(entities):\n",
    "        \"\"\"Flatten entities extracted in strict mode into tuples.\"\"\"\n",
    "        return [e.to_tuple() for sen in entities.entities for e in sen]\n",
    "\n",
    "    def compute_confusion_matrix(self):\n",
    "        \"\"\"Compute confusion matrix across all entity types.\"\"\"\n",
    "        types = set([ent[1] for ent in self.true_entities]).union(\n",
    "            [ent[1] for ent in self.pred_entities]\n",
    "        )\n",
    "\n",
    "        confusion_matrix = {typ: {'TP': 0, 'FP': 0, 'FN': 0} for typ in types}\n",
    "\n",
    "        for entity_type in types:\n",
    "            TP, FP, FN = self.extract_strict_entity_confusion(entity_type)\n",
    "            confusion_matrix[entity_type]['TP'] = TP\n",
    "            confusion_matrix[entity_type]['FP'] = FP\n",
    "            confusion_matrix[entity_type]['FN'] = FN\n",
    "\n",
    "        return confusion_matrix\n",
    "\n",
    "    def extract_strict_entity_confusion(self, entity):\n",
    "        \"\"\"Extract TP, FP, and FN for a given entity type.\"\"\"\n",
    "        fns = set([e for e in self.true_entities if e[1] == entity]) - set(\n",
    "            [e for e in self.pred_entities if e[1] == entity]\n",
    "        )\n",
    "        fps = set([e for e in self.pred_entities if e[1] == entity]) - set(\n",
    "            [e for e in self.true_entities if e[1] == entity]\n",
    "        )\n",
    "        tps = set([e for e in self.pred_entities if e[1] == entity]).intersection(\n",
    "            set([e for e in self.true_entities if e[1] == entity])\n",
    "        )\n",
    "        return len(tps), len(fps), len(fns)\n",
    "\n",
    "    def compute_false_positives(self, entity_type):\n",
    "        \"\"\"Analyze false positives for a specific entity type.\"\"\"\n",
    "        false_positives = set(\n",
    "            [e for e in self.pred_entities if e[1] == entity_type]\n",
    "        ) - set([e for e in self.true_entities if e[1] == entity_type])\n",
    "\n",
    "        return self.analyze_errors(false_positives, self.true_entities, \"FP\")\n",
    "\n",
    "    def compute_false_negatives(self, entity_type):\n",
    "        \"\"\"Analyze false negatives for a specific entity type.\"\"\"\n",
    "        false_negatives = set(\n",
    "            [e for e in self.true_entities if e[1] == entity_type]\n",
    "        ) - set([e for e in self.pred_entities if e[1] == entity_type])\n",
    "\n",
    "        return self.analyze_errors(false_negatives, self.pred_entities, \"FN\")\n",
    "\n",
    "    def analyze_errors(self, target_entities, comparison_entities, error_type):\n",
    "        \"\"\"Analyze entity-level errors (FP or FN).\"\"\"\n",
    "        counts = defaultdict(Counter)\n",
    "        non_o_errors = set()\n",
    "        indexed_entities = defaultdict(list)\n",
    "\n",
    "        # Index comparison entities by sentence\n",
    "        for entity in comparison_entities:\n",
    "            sen, entity_type, start, end = entity\n",
    "            indexed_entities[sen].append(entity)\n",
    "        \n",
    "        # Track processed pairs to avoid duplicates in counting\n",
    "        processed_pairs = set()\n",
    "        # First pass: entity errors\n",
    "        for target_entity in target_entities:\n",
    "            t_sen, t_type, t_start, t_end = target_entity\n",
    "\n",
    "            for comp_entity in indexed_entities[t_sen]:\n",
    "                c_type, c_start, c_end = comp_entity[1:]\n",
    "\n",
    "                # Check for entity type mismatch with exact boundary match\n",
    "                if (\n",
    "                t_start == c_start\n",
    "                and t_end == c_end\n",
    "                and t_type != c_type\n",
    "                and target_entity not in non_o_errors\n",
    "            ):\n",
    "                    counts[t_type][c_type] += 1\n",
    "                    non_o_errors.add(target_entity)\n",
    "\n",
    "        # Second pass: boundary errors\n",
    "        for target_entity in target_entities - non_o_errors:\n",
    "            t_sen, t_type, t_start, t_end = target_entity\n",
    "\n",
    "            for comp_entity in indexed_entities[t_sen]:\n",
    "                c_type, c_start, c_end = comp_entity[1:]\n",
    "\n",
    "                # Check for boundary issues with the same entity type\n",
    "                if (\n",
    "                t_type == c_type\n",
    "                and (t_start <= c_start <= t_end or t_start <= c_end <= t_end)\n",
    "                and target_entity not in non_o_errors\n",
    "            ):\n",
    "                    counts[t_type]['Boundary'] += 1\n",
    "                    non_o_errors.add(target_entity)\n",
    "\n",
    "        # Third pass: combined entity and boundary errors\n",
    "        for target_entity in target_entities - non_o_errors:\n",
    "            t_sen, t_type, t_start, t_end = target_entity\n",
    "\n",
    "            for comp_entity in indexed_entities[t_sen]:\n",
    "                c_type, c_start, c_end = comp_entity[1:]\n",
    "\n",
    "                # Check for combined entity and boundary issues with different types\n",
    "                if (\n",
    "                c_type != t_type\n",
    "                and (t_start <= c_start <= t_end or t_start <= c_end <= t_end)\n",
    "                and target_entity not in non_o_errors\n",
    "            ):\n",
    "                    counts[t_type]['Entity and Boundary'] += 1\n",
    "                    non_o_errors.add(target_entity)\n",
    "\n",
    "        # Remaining errors are \"O\" errors (completely unmatched)\n",
    "        for target_entity in target_entities - non_o_errors:\n",
    "            t_sen, t_type, t_start, t_end = target_entity\n",
    "            counts[t_type]['O'] += 1\n",
    "\n",
    "        return dict(counts)\n",
    "\n",
    "\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class EntityConfusion:\n",
    "    def __init__(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Initialize the EntityConfusionMatrix class.\n",
    "\n",
    "        Args:\n",
    "            y_true (list): The ground truth entities.\n",
    "            y_pred (list): The predicted entities.\n",
    "        \"\"\"\n",
    "        self.y_true = y_true\n",
    "        self.y_pred = y_pred\n",
    "        \n",
    "\n",
    "    def prepare_entities(self):\n",
    "        \"\"\"\n",
    "        Prepare entities for confusion matrix calculation.\n",
    "        - In non-strict mode, entities are extracted using `get_entities`.\n",
    "        \"\"\"    \n",
    "        # Use non-strict extraction\n",
    "        self.true_entities = get_entities(self.y_true)\n",
    "        self.pred_entities = get_entities(self.y_pred)\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_entity_confusion(entity, true_entities, pred_entities):\n",
    "        fns = set([e for e in true_entities if e[0] == entity]) - set([e for e in pred_entities if e[0] == entity])\n",
    "        fps = set([e for e in pred_entities if e[0] == entity]) - set([e for e in true_entities if e[0] == entity])\n",
    "        tps = set([e for e in pred_entities if e[0] == entity]).intersection(set([e for e in true_entities if e[0] == entity]))\n",
    "        return len(tps), len(fps), len(fns)\n",
    "\n",
    "\n",
    "    def compute(self):\n",
    "        \"\"\"\n",
    "        Compute the confusion matrix, false negatives, and false positives.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing:\n",
    "                  - 'confusion_matrix': The confusion matrix for entity recognition.\n",
    "                  - 'false_negatives': Detailed false negatives.\n",
    "                  - 'false_positives': Detailed false positives.\n",
    "        \"\"\"\n",
    "        self.prepare_entities()\n",
    "        return {\n",
    "            'confusion_matrix': self.compute_confusion_matrix(),\n",
    "            'false_negatives': self.compute_false_negatives_with_boundary(),\n",
    "            'false_positives': self.compute_false_positives_with_boundary()\n",
    "        }\n",
    "    \n",
    "\n",
    "    def compute_confusion_matrix(self):\n",
    "        \"\"\"\n",
    "        Compute a confusion matrix for Named Entity Recognition (NER) predictions.\n",
    "\n",
    "        Returns:\n",
    "            dict: A confusion matrix structured as:\n",
    "                  {entity_type: {'TP': count, 'FP': count, 'FN': count}}\n",
    "        \"\"\"\n",
    "        # Extract all unique entity types from true and predicted entities\n",
    "        types = set([ent[0] for ent in self.true_entities]).union([ent[0] for ent in self.pred_entities])\n",
    "\n",
    "        # Initialize the confusion matrix\n",
    "        confusion_matrix = {typ: {'TP': 0, 'FP': 0, 'FN': 0} for typ in types}\n",
    "\n",
    "        \n",
    "        # Populate the confusion matrix for each entity type\n",
    "        for entity_type in types:\n",
    "            TP, FP, FN = self.extract_entity_confusion(entity_type, self.true_entities, self.pred_entities)\n",
    "            confusion_matrix[entity_type]['TP'] = TP\n",
    "            confusion_matrix[entity_type]['FP'] = FP\n",
    "            confusion_matrix[entity_type]['FN'] = FN\n",
    "\n",
    "        return confusion_matrix\n",
    "    \n",
    "\n",
    "    def compute_false_negatives_with_boundary(self):\n",
    "        \"\"\"\n",
    "        Compute false negatives with detailed categorization:\n",
    "        - 'Boundary': Incorrect boundaries for the same entity type.\n",
    "        - 'Missed': Predicted as O or no match at all.\n",
    "\n",
    "        Returns:\n",
    "            dict: False negatives categorized by entity type.\n",
    "        \"\"\"\n",
    "        fn_counts = defaultdict(Counter)\n",
    "        true_indexed = {(t[1], t[2]): t[0] for t in self.true_entities}  # Index true entities by boundaries\n",
    "        pred_indexed = {(p[1], p[2]): p[0] for p in self.pred_entities}  # Index predicted entities by boundaries\n",
    "        # Iterate through true entities to classify false negatives\n",
    "        for (t_start, t_end), t_type in true_indexed.items():\n",
    "            if (t_start, t_end) in pred_indexed:\n",
    "                if pred_indexed[(t_start, t_end)] != t_type:\n",
    "                    # Type mismatch at the exact position\n",
    "                    matched_type = pred_indexed.get((t_start, t_end))\n",
    "                    fn_counts[t_type][matched_type] += 1\n",
    "            else:\n",
    "                # No exact match found, check for other errors\n",
    "                boundary_error = False\n",
    "                entity_error = False\n",
    "                for (p_start, p_end), p_type in pred_indexed.items():\n",
    "                    if t_type == p_type:\n",
    "                        if (p_start <= t_start <= p_end) or (p_start <= t_end <= p_end):\n",
    "                            # Boundary error for the same type\n",
    "                            fn_counts[t_type]['Boundary'] += 1\n",
    "                            boundary_error = True\n",
    "                            break\n",
    "                    else:\n",
    "                        if (p_start <= t_start <= p_end) or (p_start <= t_end <= p_end):\n",
    "                            # Boundary error with a different entity type\n",
    "                            fn_counts[t_type]['Entity and Boundary'] += 1\n",
    "                            entity_error = True\n",
    "                            break\n",
    "                if not boundary_error and not entity_error:\n",
    "                    # Missed entity entirely\n",
    "                    fn_counts[t_type]['O'] += 1\n",
    "\n",
    "        return dict(fn_counts)\n",
    "\n",
    "\n",
    "    def compute_false_positives_with_boundary(self):\n",
    "        \"\"\"\n",
    "        Compute false positives with boundary categorization:\n",
    "        - 'Boundary': Incorrect boundaries for the same entity type.\n",
    "        - 'Missed': Predicted as O or no match at all.\n",
    "\n",
    "        Returns:\n",
    "            dict: False positives categorized by entity type.\n",
    "        \"\"\"\n",
    "        fp_counts = defaultdict(Counter)\n",
    "        true_indexed = {(t[1], t[2]): t[0] for t in self.true_entities}  # Index true entities by boundaries\n",
    "        pred_indexed = {(p[1], p[2]): p[0] for p in self.pred_entities}  # Index predicted entities by boundaries\n",
    "        # Iterate through predicted entities to find false positives\n",
    "        for (p_start, p_end), p_type in pred_indexed.items():\n",
    "            entity_error = False\n",
    "            if (p_start, p_end) not in true_indexed or true_indexed[(p_start, p_end)] != p_type:\n",
    "                # No matching true entity or type mismatch at the same position\n",
    "                matched_type = true_indexed.get((p_start, p_end))\n",
    "                if matched_type:\n",
    "                    fp_counts[p_type][matched_type] += 1\n",
    "                    entity_error = True\n",
    "                boundary_error = False\n",
    "                entity_boundary_error = False\n",
    "                for (t_start, t_end), t_type in true_indexed.items():\n",
    "                    if t_type == p_type and not (p_start == t_start and p_end == t_end):\n",
    "                        if (p_start <= t_start <= p_end) or (p_start <= t_end <= p_end):\n",
    "                            # Detected boundary error for the same entity type\n",
    "                            fp_counts[p_type]['Boundary'] += 1\n",
    "                            boundary_error = True\n",
    "                            break\n",
    "                    elif t_type != p_type and not (p_start == t_start and p_end == t_end):\n",
    "                        if (p_start <= t_start <= p_end) or (p_start <= t_end <= p_end):\n",
    "                            fp_counts[p_type]['Entity and Boundary'] += 1\n",
    "                            entity_boundary_error = True\n",
    "                            break\n",
    "                if not boundary_error and not entity_error and not entity_boundary_error:\n",
    "                    # Missed entity entirely (e.g., predicted as O)\n",
    "                    fp_counts[p_type]['O'] += 1\n",
    "\n",
    "        return dict(fp_counts)\n",
    "    \n",
    "\n",
    "def save_json(file_path, data):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "def run_confusion(dataset_name, model_name):\n",
    "    \n",
    "\tbase_folder = Path(f'/Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/BaseLineExperiment/{dataset_name}_{model_name}')\n",
    "\tevaluation_metrics_file = base_folder / 'fine_tuning/evaluation_metrics.json'\n",
    "\twith open(evaluation_metrics_file, 'r') as file:\n",
    "\t\tentity_outputs = json.load(file)  # Use json.load() to read file, not json.loads()\n",
    "\ty_true = entity_outputs['entity_outputs']['y_true']\n",
    "\ty_pred = entity_outputs['entity_outputs']['y_pred']\n",
    "\tresults_folder = base_folder / 'extractions/results'  \n",
    "\toriginal_confusion_path = results_folder / 'entity_confusion_data.json'\n",
    "\tscrapped_confusion_path = results_folder / 'entity_confusion_data-backup.json'\n",
    "\tstrict_confusion_path = results_folder / 'strict_entity_confusion_data.json'\n",
    "\tprint(original_confusion_path)\n",
    "\tdf = pd.read_json(\n",
    "\t\toriginal_confusion_path,\n",
    "\t\tlines=True\n",
    "\t)\n",
    "\tdf.to_json(\n",
    "\t\tscrapped_confusion_path,\n",
    "\t\tlines=True,\n",
    "\t\torient='records'\n",
    "\t)\n",
    "\tstrict_confusion = StrictConfusionMatrix(y_true, y_pred)\n",
    "\tconfusion = EntityConfusion(y_true, y_pred)\n",
    "\tstrict_confusion_matrix = strict_confusion.compute()\n",
    "\tconfusion_matrix = confusion.compute()\n",
    "\tprint(f'Saving Confusion for {dataset_name}_{model_name}')\n",
    "\tsave_json(original_confusion_path, confusion_matrix)\n",
    "\tsave_json(strict_confusion_path, strict_confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'arabertv02'\n",
    "dataset_name = 'ANERCorp_CamelLab'\n",
    "\n",
    "run_confusion(dataset_name, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/BaseLineExperiment/conll2003_bert/extractions/results/entity_confusion_data.json\n",
      "Saving Confusion for conll2003_bert\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert'\n",
    "dataset_name = 'conll2003'\n",
    "\n",
    "run_confusion(dataset_name, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folder = Path('/Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/BaseLineExperiment/conll2003_bert/extractions/results/entity_confusion_data.json')\n",
    "import json\n",
    "with open(test_folder, 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "file_name = '/Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/BaseLineExperiment/ANERCorp_CamelLab_arabertv02/fine_tuning/evaluation_metrics.json'\n",
    "with open(file_name, 'r') as file:\n",
    "    entity_outputs = json.load(file)  # Use json.load() to read file, not json.loads()\n",
    "y_true = entity_outputs['entity_outputs']['y_true']\n",
    "y_pred = entity_outputs['entity_outputs']['y_pred']\n",
    "\n",
    "base_folder = Path('/Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/BaseLineExperiment/ANERCorp_CamelLab_arabertv02/extractions/results')\n",
    "original_confusion_path = base_folder / 'entity_confusion_data.json'\n",
    "scrapped_confusion_path = base_folder / 'entity_confusion_data-backup.json'\n",
    "strict_confusion_path = base_folder / 'strict_entity_confusion_data.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\n",
    "    original_confusion_path,\n",
    "\tlines=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\n",
    "\tscrapped_confusion_path,\n",
    "\tlines=True,\n",
    "\torient='records'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strict_confusion = StrictConfusionMatrix(y_true, y_pred)\n",
    "confusion = EntityConfusion(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strict_confusion_matrix = strict_confusion.compute()\n",
    "confusion_matrix = confusion.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(original_confusion_path, confusion_matrix)\n",
    "save_json(strict_confusion_path, strict_confusion_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
