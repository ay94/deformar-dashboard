{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from seqeval.metrics import classification_report as seq_classification\n",
    "from seqeval.metrics import f1_score as seq_f1\n",
    "from seqeval.metrics import precision_score as seq_precision\n",
    "from seqeval.metrics import recall_score as seq_recall\n",
    "from sklearn.metrics import classification_report as skl_classification\n",
    "from sklearn.metrics import f1_score as skl_f1\n",
    "from sklearn.metrics import precision_score as skl_precision\n",
    "from sklearn.metrics import recall_score as skl_recall\n",
    "from torch import nn\n",
    "from seqeval.scheme import IOB1, IOB2, IOE1, IOE2, IOBES, BILOU, auto_detect\n",
    "\n",
    "VALID_SCHEMES = {\n",
    "    'IOB1':IOB1,\n",
    "    'IOB2':IOB2,\n",
    "    'IOE1':IOE1,\n",
    "    'IOE2':IOE2,\n",
    "    'IOBES':IOBES,\n",
    "    'BILOU':BILOU\n",
    "    }\n",
    "class EvaluationStrategy(ABC):\n",
    "    def __init__(self, inv_map):\n",
    "        self.inv_map = inv_map\n",
    "        self.ignore_index = nn.CrossEntropyLoss().ignore_index\n",
    "\n",
    "    def align_predictions(self, predictions, truth):\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "        batch_size, seq_len = predictions.shape\n",
    "\n",
    "        truth_list = [[] for _ in range(batch_size)]\n",
    "        pred_list = [[] for _ in range(batch_size)]\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            for j in range(seq_len):\n",
    "                if truth[i, j] != self.ignore_index:\n",
    "                    truth_list[i].append(self.inv_map[truth[i][j]])\n",
    "                    pred_list[i].append(self.inv_map[predictions[i][j]])\n",
    "                    \n",
    "        if len(truth_list) != len(pred_list):\n",
    "            raise ValueError(\"Aligned predictions and truth have mismatched lengths.\")\n",
    "        return truth_list, pred_list\n",
    "\n",
    "    def create_classification_report(self, results):\n",
    "        lines = []\n",
    "        for line in results.strip().split(\"\\n\")[1:]:\n",
    "            if line.strip():\n",
    "                tokens = line.split()\n",
    "                # Remove intermediate aggregation if exists (multi-class)\n",
    "                if len(tokens) > 5:\n",
    "                    del tokens[1]\n",
    "                lines.append(tokens)\n",
    "        report = pd.DataFrame(\n",
    "            lines, columns=[\"Tag\", \"Precision\", \"Recall\", \"F1\", \"Support\"]\n",
    "        )\n",
    "        return report\n",
    "\n",
    "class TokenEvaluationStrategy(EvaluationStrategy):\n",
    "    def compute_metrics(self, true_labels, predictions):\n",
    "        try:\n",
    "            truth_list, pred_list = self.align_predictions(predictions, true_labels)\n",
    "        except:\n",
    "            logging.info('The labels already aligned, proceed with evaluation')\n",
    "            truth_list, pred_list = true_labels, predictions\n",
    "        \n",
    "        flat_truth = [item for sublist in truth_list for item in sublist]\n",
    "        flat_preds = [item for sublist in pred_list for item in sublist]\n",
    "        report = skl_classification(y_true=flat_truth, y_pred=flat_preds, digits=4)\n",
    "        report = self.create_classification_report(report)\n",
    "        cleaned_report = self.clean_report(report)\n",
    "        return {\n",
    "            \"Precision\": skl_precision(\n",
    "                y_true=flat_truth, y_pred=flat_preds, average=\"macro\"\n",
    "            ),\n",
    "            \"Recall\": skl_recall(\n",
    "                y_true=flat_truth, y_pred=flat_preds, average=\"macro\"\n",
    "                ),\n",
    "            \"F1\": skl_f1(\n",
    "                y_true=flat_truth, y_pred=flat_preds, average=\"macro\"\n",
    "                ),\n",
    "            \"classification\": cleaned_report,\n",
    "            \"output\": {\"y_true\": flat_truth, \"y_pred\": flat_preds},\n",
    "        }\n",
    "\n",
    "    def clean_report(self, report):\n",
    "        report = report.copy()\n",
    "        mask = report[\"Tag\"] == \"accuracy\"\n",
    "        accuracy_row = report[mask]\n",
    "        if not accuracy_row.empty:\n",
    "            # Get the accuracy value\n",
    "            accuracy_value = accuracy_row[\"Precision\"].values[\n",
    "                0\n",
    "            ]  # Assuming accuracy is stored in the 'Precision' column\n",
    "            accuracy_support = accuracy_row[\"Recall\"].values[\n",
    "                0\n",
    "            ]  # Assuming accuracy is stored in the 'Precision' column\n",
    "\n",
    "            # Set the precision, recall, and F1-score to the accuracy value\n",
    "            report.loc[mask, \"Precision\"] = accuracy_value\n",
    "            report.loc[mask, \"Recall\"] = accuracy_value\n",
    "            report.loc[mask, \"F1\"] = accuracy_value\n",
    "            report.loc[mask, \"Support\"] = accuracy_support\n",
    "\n",
    "            # Rename the tag from 'accuracy' to 'accuracy/micro' for clarity\n",
    "            report.loc[report[\"Tag\"] == \"accuracy\", \"Tag\"] = \"accuracy/micro\"\n",
    "        return report\n",
    "    \n",
    "class EntityEvaluationStrategy(EvaluationStrategy):\n",
    "    def compute_metrics(self, true_labels, predictions, entity_config):\n",
    "        scheme = entity_config.get('scheme')  # Default to 'none' if not specified\n",
    "\n",
    "        # Check if the scheme is valid and not 'none'\n",
    "        try:\n",
    "            truth_list, pred_list = self.align_predictions(predictions, true_labels)\n",
    "        except:\n",
    "            logging.info('The labels already aligned, proceed with evaluation')\n",
    "            truth_list, pred_list = true_labels, predictions\n",
    "            \n",
    "        strict_outputs = self._evaluate_strict(truth_list, pred_list, scheme)\n",
    "        non_strict_outputs = self._evaluate_non_strict(truth_list, pred_list)\n",
    "        \n",
    "        return {\n",
    "            \"strict\": strict_outputs,\n",
    "            \"non_strict\": non_strict_outputs,\n",
    "            \"output\": {\"y_true\": truth_list, \"y_pred\": pred_list}\n",
    "        }\n",
    "        \n",
    "        \n",
    "    \n",
    "    def _evaluate_strict(self, truth_list, pred_list, scheme):\n",
    "        \n",
    "        if scheme is not None and scheme in VALID_SCHEMES:\n",
    "            scheme_class = VALID_SCHEMES[scheme]\n",
    "            report = seq_classification(\n",
    "                    y_true=truth_list,\n",
    "                    y_pred=pred_list,\n",
    "                    digits=4,\n",
    "                    mode='strict',\n",
    "                    scheme=scheme_class,\n",
    "                )\n",
    "            precision = seq_precision(\n",
    "                    y_true=truth_list, y_pred=pred_list, average=\"micro\", mode='strict', scheme = scheme_class\n",
    "                )\n",
    "            recall = seq_recall(\n",
    "                    y_true=truth_list, y_pred=pred_list, average=\"micro\", mode='strict', scheme = scheme_class\n",
    "                )\n",
    "            f1 = seq_f1(\n",
    "                    y_true=truth_list, y_pred=pred_list, average=\"micro\", mode='strict', scheme = scheme_class\n",
    "                )\n",
    "            \n",
    "        else:\n",
    "            logging.info(\"The scheme is unspecified; seqeval will auto-detect the scheme.\")\n",
    "            report = seq_classification(\n",
    "                    y_true=truth_list,\n",
    "                    y_pred=pred_list,\n",
    "                    digits=4,\n",
    "                    mode='strict',\n",
    "                )\n",
    "            scheme_class = auto_detect(pred_list, False)\n",
    "            precision = seq_precision(\n",
    "                    y_true=truth_list, y_pred=pred_list, average=\"micro\", mode='strict', scheme = scheme_class\n",
    "                )\n",
    "            recall = seq_recall(\n",
    "                    y_true=truth_list, y_pred=pred_list, average=\"micro\", mode='strict', scheme = scheme_class\n",
    "                )\n",
    "            f1 = seq_f1(\n",
    "                    y_true=truth_list, y_pred=pred_list, average=\"micro\", mode='strict', scheme = scheme_class\n",
    "                )\n",
    "        \n",
    "        return {\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1\": f1,\n",
    "            \"classification\": self.create_classification_report(report),\n",
    "                \n",
    "            }\n",
    "    \n",
    "        \n",
    "    def _evaluate_non_strict(self, truth_list, pred_list,):\n",
    "        \n",
    "        report = seq_classification(\n",
    "                    y_true=truth_list,\n",
    "                    y_pred=pred_list,\n",
    "                    digits=4,\n",
    "                )\n",
    "        precision = seq_precision(\n",
    "                y_true=truth_list, y_pred=pred_list, average=\"micro\"\n",
    "            )\n",
    "        recall = seq_recall(\n",
    "                    y_true=truth_list, y_pred=pred_list, average=\"micro\"\n",
    "                )\n",
    "        f1 = seq_f1(\n",
    "                y_true=truth_list, y_pred=pred_list, average=\"micro\"\n",
    "            )\n",
    "        return {\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1\": f1,\n",
    "            \"classification\": self.create_classification_report(report),\n",
    "                \n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "class Evaluation:\n",
    "    def __init__(self, inv_map, y_true, y_pred, evaluation_config):\n",
    "        self.truths = y_true\n",
    "        self.predictions = y_pred\n",
    "        self.evaluation_config = evaluation_config\n",
    "        self.token_strategy = TokenEvaluationStrategy(inv_map)\n",
    "        self.entity_strategy = EntityEvaluationStrategy(inv_map)\n",
    "\n",
    "    \n",
    "\n",
    "    def evaluate(self):\n",
    "        token_metrics = self.token_strategy.compute_metrics(\n",
    "            self.truths, self.predictions\n",
    "        )\n",
    "        entity_metrics = self.entity_strategy.compute_metrics(\n",
    "            self.truths, self.predictions, self.evaluation_config\n",
    "        )\n",
    "\n",
    "        # Combine or store results as needed\n",
    "        return {\"Token_Level\": token_metrics, \"Entity_Level\": entity_metrics}\n",
    "\n",
    "    def _prepare_results(self, metrics):\n",
    "        results = pd.DataFrame.from_dict(self._round_and_slice(metrics))\n",
    "        report = metrics[\"classification\"]\n",
    "        output = metrics[\"output\"]\n",
    "        return results, report, output\n",
    "    \n",
    "    def _prepare_entity_results(self, metrics):\n",
    "        strict = metrics[\"strict\"]\n",
    "        non_strict = metrics[\"non_strict\"]\n",
    "        entity_strict_results = pd.DataFrame.from_dict(self._round_and_slice(strict))\n",
    "        entity_non_strict_results = pd.DataFrame.from_dict(self._round_and_slice(non_strict))\n",
    "        entity_strict_report = strict['classification']\n",
    "        entity_non_strict_report = non_strict['classification']\n",
    "        output = metrics[\"output\"]\n",
    "        return {\n",
    "            'entity_strict_results': entity_strict_results,\n",
    "            'entity_non_strict_results': entity_non_strict_results,\n",
    "            'entity_strict_report': entity_strict_report,\n",
    "            'entity_non_strict_report': entity_non_strict_report,\n",
    "            'output': output,\n",
    "        }\n",
    "\n",
    "    def _round_and_slice(self, dictionary):\n",
    "        # Slicing and rounding results for cleaner presentation\n",
    "        \n",
    "        keys_for_slicing = [\"Precision\", \"Recall\", \"F1\"]\n",
    "        sliced_dict = {key: [round(dictionary[key], 4)] for key in keys_for_slicing}\n",
    "        return sliced_dict\n",
    "    \n",
    "    def generate_results(self):\n",
    "        metrics = self.evaluate()\n",
    "        token_results, token_report, token_outputs = self._prepare_results(\n",
    "            metrics[\"Token_Level\"]\n",
    "        )\n",
    "        entity_level_outputs = self._prepare_entity_results(\n",
    "            metrics[\"Entity_Level\"]\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "        return {\n",
    "            \"token_results\": token_results,\n",
    "            \"token_report\": token_report,\n",
    "            \"token_outputs\": token_outputs,\n",
    "            \"entity_strict_results\": entity_level_outputs['entity_strict_results'],\n",
    "            \"entity_non_strict_results\": entity_level_outputs['entity_non_strict_results'],\n",
    "            \"entity_strict_report\": entity_level_outputs['entity_strict_report'],\n",
    "            \"entity_non_strict_report\": entity_level_outputs['entity_non_strict_report'],\n",
    "            \"entity_outputs\": entity_level_outputs['output'],\n",
    "        }\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Metrics:\n",
    "    token_results: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    token_report: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    token_outputs: dict = field(default_factory=dict)\n",
    "    entity_strict_results: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    entity_non_strict_results: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    entity_strict_report: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    entity_non_strict_report: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    entity_outputs: dict = field(default_factory=dict)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_dict(data: dict):\n",
    "        \"\"\"Create an instance from a dictionary.\"\"\"\n",
    "        required_keys = [\n",
    "        \"token_results\", \"token_report\", \"token_outputs\",\n",
    "        \"entity_strict_results\", \"entity_non_strict_results\",\n",
    "        \"entity_strict_report\", \"entity_non_strict_report\", \"entity_outputs\"\n",
    "        ]\n",
    "        missing_keys = [key for key in required_keys if key not in data]\n",
    "        if missing_keys:\n",
    "            raise ValueError(f\"Missing required keys in data: {missing_keys}\")\n",
    "        return Metrics(**data)\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"token_results\": self.token_results.to_dict(orient=\"records\"),\n",
    "            \"token_report\": self.token_report.to_dict(orient=\"records\"),\n",
    "            \"token_outputs\": self.token_outputs,\n",
    "            \"entity_strict_results\": self.entity_strict_results.to_dict(orient=\"records\"),     \n",
    "            \"entity_non_strict_results\": self.entity_non_strict_results.to_dict(orient=\"records\"),\n",
    "            \"entity_strict_report\": self.entity_strict_report.to_dict(orient=\"records\"), \n",
    "            \"entity_non_strict_report\": self.entity_non_strict_report.to_dict(orient=\"records\"), \n",
    "            \"entity_outputs\": self.entity_outputs,\n",
    "        }\n",
    "def combine_results(metrics):\n",
    "\n",
    "        entity_results = metrics.entity_non_strict_results.copy()\n",
    "        entity_strict_results = metrics.entity_strict_results.copy()\n",
    "        token_results = metrics.token_results.copy()\n",
    "        entity_results[\"Type\"] = \"Non Strict\"\n",
    "        entity_strict_results[\"Type\"] = \"IOB2\"\n",
    "        token_results[\"Type\"] = \"Token\"\n",
    "        df_combined = pd.concat([entity_results, entity_strict_results, token_results]).reset_index(drop=True)\n",
    "        return df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def run_evaluation(dataset, model):\n",
    "    base_dir = Path('/Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/BaseLineExperiment')\n",
    "    variant = f'{dataset}_{model}'\n",
    "    variant_folder = base_dir / variant\n",
    "\n",
    "    # File paths\n",
    "    evaluation_metrics_file = variant_folder / 'fine_tuning/evaluation_metrics.json'\n",
    "    corpus_file = Path('/Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/ExperimentData/corpora.json')\n",
    "\n",
    "    # Load evaluation metrics and corpus\n",
    "    with open(evaluation_metrics_file, 'r') as file:\n",
    "        entity_outputs = json.load(file)\n",
    "    with open(corpus_file, 'r') as file:\n",
    "        corpora = json.load(file)\n",
    "\n",
    "    # More paths based on the model folder\n",
    "    results_folder = variant_folder / 'extractions/results'\n",
    "    # replace the current ones with a backup\n",
    "    replacements = {\n",
    "        'token_report': 'token_report.json',\n",
    "        'entity_report': 'entity_report.json',\n",
    "        'results': 'results.json'\n",
    "    }\n",
    "    \n",
    "    for key, filename in replacements.items():\n",
    "        original_path = results_folder / filename\n",
    "        backup_path = results_folder / f\"{filename.split('.')[0]}-backup.json\"\n",
    "        df = pd.read_json(original_path, lines=True)\n",
    "        df.to_json(backup_path, lines=True, orient='records')\n",
    "        print(f\"Backed up {filename} to {backup_path}\")\n",
    "\n",
    "\n",
    "    # Extract necessary data for evaluation\n",
    "    y_true = entity_outputs['entity_outputs']['y_true']\n",
    "    y_pred = entity_outputs['entity_outputs']['y_pred']\n",
    "    corpus = corpora[dataset]\n",
    "    labels_map = corpus[\"labels_map\"]\n",
    "    inv_labels_map = {v: k for k, v in labels_map.items()}\n",
    "    \n",
    "    # Simulated evaluation process (replace with your actual evaluation code)\n",
    "    evaluation_config = {'scheme': 'IOB2'}\n",
    "    evaluator = Evaluation(\n",
    "                inv_labels_map, y_true, y_pred, evaluation_config\n",
    "            )\n",
    "    results = evaluator.generate_results()\n",
    "    metrics = Metrics.from_dict(results)\n",
    "\n",
    "\n",
    "    savings = {\n",
    "        'token_report': metrics.token_report,\n",
    "        'entity_non_strict_report': metrics.entity_non_strict_report,\n",
    "        'entity_strict_report': metrics.entity_strict_report,\n",
    "        'results': combine_results(metrics)  # Assuming this DataFrame contains the combined results\n",
    "    }\n",
    "    # Save updated reports\n",
    "    for key, report_df in savings.items():\n",
    "        if report_df.empty:\n",
    "            continue  # Skip if no data, or adapt as necessary\n",
    "        report_path = results_folder / f\"{key}.json\"\n",
    "        report_df.to_json(report_path, lines=True, orient='records')\n",
    "        print(f\"Saved updated {key} to {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backed up token_report.json to /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/BaseLineExperiment/conll2003_bert/extractions/results/token_report-backup.json\n",
      "Backed up entity_report.json to /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/BaseLineExperiment/conll2003_bert/extractions/results/entity_report-backup.json\n",
      "Backed up results.json to /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/BaseLineExperiment/conll2003_bert/extractions/results/results-backup.json\n",
      "Saved updated token_report to /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/BaseLineExperiment/conll2003_bert/extractions/results/token_report.json\n",
      "Saved updated entity_non_strict_report to /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/BaseLineExperiment/conll2003_bert/extractions/results/entity_non_strict_report.json\n",
      "Saved updated entity_strict_report to /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/BaseLineExperiment/conll2003_bert/extractions/results/entity_strict_report.json\n",
      "Saved updated results to /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/BaseLineExperiment/conll2003_bert/extractions/results/results.json\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert'\n",
    "dataset_name = 'conll2003'\n",
    "\n",
    "run_evaluation(dataset_name, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backed up token_report.json to /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/BaseLineExperiment/ANERCorp_CamelLab_arabertv02/extractions/results/token_report-backup.json\n",
      "Backed up entity_report.json to /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/BaseLineExperiment/ANERCorp_CamelLab_arabertv02/extractions/results/entity_report-backup.json\n",
      "Backed up results.json to /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/BaseLineExperiment/ANERCorp_CamelLab_arabertv02/extractions/results/results-backup.json\n",
      "Saved updated token_report to /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/BaseLineExperiment/ANERCorp_CamelLab_arabertv02/extractions/results/token_report.json\n",
      "Saved updated entity_non_strict_report to /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/BaseLineExperiment/ANERCorp_CamelLab_arabertv02/extractions/results/entity_non_strict_report.json\n",
      "Saved updated entity_strict_report to /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/BaseLineExperiment/ANERCorp_CamelLab_arabertv02/extractions/results/entity_strict_report.json\n",
      "Saved updated results to /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/BaseLineExperiment/ANERCorp_CamelLab_arabertv02/extractions/results/results.json\n"
     ]
    }
   ],
   "source": [
    "model_name = 'ANERCorp_CamelLab'\n",
    "dataset_name = 'arabertv02'\n",
    "\n",
    "\n",
    "run_evaluation(model_name, dataset_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_test = Path('/Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/BaseLineExperiment/conll2003_bert/extractions/results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scrapped = pd.read_json(\n",
    "\tbase_test / 'entity_report-backup.json',\n",
    "\tlines=True\n",
    ")\n",
    "\n",
    "\n",
    "entity = pd.read_json(\n",
    "\tbase_test / 'entity_non_strict_report.json',\n",
    "\tlines=True\n",
    ")\n",
    "\n",
    "\n",
    "strict = pd.read_json(\n",
    "\tbase_test / 'entity_strict_report.json',\n",
    "\tlines=True\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    True\n",
       "1    True\n",
       "2    True\n",
       "3    True\n",
       "4    True\n",
       "5    True\n",
       "6    True\n",
       "Name: F1, dtype: bool"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity['F1'] == scrapped['F1'].round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.9206</td>\n",
       "      <td>0.9317</td>\n",
       "      <td>0.9261</td>\n",
       "      <td>1668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.7907</td>\n",
       "      <td>0.8234</td>\n",
       "      <td>0.8067</td>\n",
       "      <td>702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.8886</td>\n",
       "      <td>0.9073</td>\n",
       "      <td>0.8978</td>\n",
       "      <td>1661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.9604</td>\n",
       "      <td>0.9604</td>\n",
       "      <td>0.9604</td>\n",
       "      <td>1617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>micro</td>\n",
       "      <td>0.9058</td>\n",
       "      <td>0.9193</td>\n",
       "      <td>0.9125</td>\n",
       "      <td>5648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>macro</td>\n",
       "      <td>0.8901</td>\n",
       "      <td>0.9057</td>\n",
       "      <td>0.8978</td>\n",
       "      <td>5648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>weighted</td>\n",
       "      <td>0.9064</td>\n",
       "      <td>0.9193</td>\n",
       "      <td>0.9128</td>\n",
       "      <td>5648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Tag  Precision  Recall      F1  Support\n",
       "0       LOC     0.9206  0.9317  0.9261     1668\n",
       "1      MISC     0.7907  0.8234  0.8067      702\n",
       "2       ORG     0.8886  0.9073  0.8978     1661\n",
       "3       PER     0.9604  0.9604  0.9604     1617\n",
       "4     micro     0.9058  0.9193  0.9125     5648\n",
       "5     macro     0.8901  0.9057  0.8978     5648\n",
       "6  weighted     0.9064  0.9193  0.9128     5648"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity['F1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# looose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "corpus_name = 'conll2003'\n",
    "model_name = 'bert'\n",
    "variant = f'{corpus_name}_{model_name}'\n",
    "\n",
    "file_name = f'/Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/BaseLineExperiment/{variant}/fine_tuning/evaluation_metrics.json'\n",
    "corpus_file = '/Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/ExperimentData/corpora.json'\n",
    "with open(file_name, 'r') as file:\n",
    "    entity_outputs = json.load(file)  # Use json.load() to read file, not json.loads()\n",
    "    \n",
    "with open(corpus_file, 'r') as file:\n",
    "    corpora = json.load(file)  # Use json.load() to read file, not json.loads()\n",
    "\n",
    "\n",
    "base_folder = Path('/Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/BaseLineExperiment/ANERCorp_CamelLab_arabertv02/extractions/results')\n",
    "original_token_report_path = base_folder / 'token_report.json'\n",
    "scrapped_token_report_path = base_folder / 'token_report-backup.json'\n",
    "original_entity_report_path = base_folder / 'entity_report.json'\n",
    "scrapped_entity_report_path = base_folder / 'entity_report-backup.json'\n",
    "strict_entity_report_path = base_folder / 'strict_entity_report.json'\n",
    "original_results_path = base_folder / 'results.json'\n",
    "scrapped_results_path = base_folder / 'results-backup.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = entity_outputs['entity_outputs']['y_true']\n",
    "y_pred = entity_outputs['entity_outputs']['y_pred']\n",
    "corpus = corpora[corpus_name]\n",
    "labels_map = corpus[\"labels_map\"]\n",
    "inv_labels_map = {v: k for k, v in labels_map.items()}\n",
    "evaluation_config = {\n",
    "\t'scheme': 'IOB2'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluation(\n",
    "                inv_labels_map, y_true, y_pred, evaluation_config\n",
    "            )\n",
    "results = evaluator.generate_results()\n",
    "metrics = Metrics.from_dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9058</td>\n",
       "      <td>0.9193</td>\n",
       "      <td>0.9125</td>\n",
       "      <td>Non Strict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9126</td>\n",
       "      <td>0.9184</td>\n",
       "      <td>0.9155</td>\n",
       "      <td>IOB2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.8945</td>\n",
       "      <td>0.9156</td>\n",
       "      <td>0.9047</td>\n",
       "      <td>Token</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision  Recall      F1        Type\n",
       "0     0.9058  0.9193  0.9125  Non Strict\n",
       "1     0.9126  0.9184  0.9155        IOB2\n",
       "2     0.8945  0.9156  0.9047       Token"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_results(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-LOC</td>\n",
       "      <td>0.9287</td>\n",
       "      <td>0.9371</td>\n",
       "      <td>0.9329</td>\n",
       "      <td>1668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B-MISC</td>\n",
       "      <td>0.8296</td>\n",
       "      <td>0.8390</td>\n",
       "      <td>0.8343</td>\n",
       "      <td>702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B-ORG</td>\n",
       "      <td>0.9082</td>\n",
       "      <td>0.9169</td>\n",
       "      <td>0.9125</td>\n",
       "      <td>1661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B-PER</td>\n",
       "      <td>0.9665</td>\n",
       "      <td>0.9647</td>\n",
       "      <td>0.9656</td>\n",
       "      <td>1617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I-LOC</td>\n",
       "      <td>0.8495</td>\n",
       "      <td>0.9222</td>\n",
       "      <td>0.8843</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I-MISC</td>\n",
       "      <td>0.6880</td>\n",
       "      <td>0.7454</td>\n",
       "      <td>0.7156</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.8962</td>\n",
       "      <td>0.9305</td>\n",
       "      <td>0.9130</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I-PER</td>\n",
       "      <td>0.9871</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>0.9888</td>\n",
       "      <td>1156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>O</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>0.9939</td>\n",
       "      <td>0.9953</td>\n",
       "      <td>38323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>accuracy/micro</td>\n",
       "      <td>0.9829</td>\n",
       "      <td>0.9829</td>\n",
       "      <td>0.9829</td>\n",
       "      <td>46435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>macro</td>\n",
       "      <td>0.8945</td>\n",
       "      <td>0.9156</td>\n",
       "      <td>0.9047</td>\n",
       "      <td>46435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>weighted</td>\n",
       "      <td>0.9833</td>\n",
       "      <td>0.9829</td>\n",
       "      <td>0.9831</td>\n",
       "      <td>46435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Tag Precision  Recall      F1 Support\n",
       "0            B-LOC    0.9287  0.9371  0.9329    1668\n",
       "1           B-MISC    0.8296  0.8390  0.8343     702\n",
       "2            B-ORG    0.9082  0.9169  0.9125    1661\n",
       "3            B-PER    0.9665  0.9647  0.9656    1617\n",
       "4            I-LOC    0.8495  0.9222  0.8843     257\n",
       "5           I-MISC    0.6880  0.7454  0.7156     216\n",
       "6            I-ORG    0.8962  0.9305  0.9130     835\n",
       "7            I-PER    0.9871  0.9905  0.9888    1156\n",
       "8                O    0.9968  0.9939  0.9953   38323\n",
       "9   accuracy/micro    0.9829  0.9829  0.9829   46435\n",
       "10           macro    0.8945  0.9156  0.9047   46435\n",
       "11        weighted    0.9833  0.9829  0.9831   46435"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.token_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.9234</td>\n",
       "      <td>0.9317</td>\n",
       "      <td>0.9275</td>\n",
       "      <td>1668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.8099</td>\n",
       "      <td>0.8191</td>\n",
       "      <td>0.8144</td>\n",
       "      <td>702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.8974</td>\n",
       "      <td>0.9061</td>\n",
       "      <td>0.9017</td>\n",
       "      <td>1661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.9622</td>\n",
       "      <td>0.9604</td>\n",
       "      <td>0.9613</td>\n",
       "      <td>1617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>micro</td>\n",
       "      <td>0.9126</td>\n",
       "      <td>0.9184</td>\n",
       "      <td>0.9155</td>\n",
       "      <td>5648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>macro</td>\n",
       "      <td>0.8982</td>\n",
       "      <td>0.9043</td>\n",
       "      <td>0.9012</td>\n",
       "      <td>5648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>weighted</td>\n",
       "      <td>0.9127</td>\n",
       "      <td>0.9184</td>\n",
       "      <td>0.9155</td>\n",
       "      <td>5648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Tag Precision  Recall      F1 Support\n",
       "0       LOC    0.9234  0.9317  0.9275    1668\n",
       "1      MISC    0.8099  0.8191  0.8144     702\n",
       "2       ORG    0.8974  0.9061  0.9017    1661\n",
       "3       PER    0.9622  0.9604  0.9613    1617\n",
       "4     micro    0.9126  0.9184  0.9155    5648\n",
       "5     macro    0.8982  0.9043  0.9012    5648\n",
       "6  weighted    0.9127  0.9184  0.9155    5648"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.entity_strict_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.9206</td>\n",
       "      <td>0.9317</td>\n",
       "      <td>0.9261</td>\n",
       "      <td>1668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.7907</td>\n",
       "      <td>0.8234</td>\n",
       "      <td>0.8067</td>\n",
       "      <td>702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.8886</td>\n",
       "      <td>0.9073</td>\n",
       "      <td>0.8978</td>\n",
       "      <td>1661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.9604</td>\n",
       "      <td>0.9604</td>\n",
       "      <td>0.9604</td>\n",
       "      <td>1617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>micro</td>\n",
       "      <td>0.9058</td>\n",
       "      <td>0.9193</td>\n",
       "      <td>0.9125</td>\n",
       "      <td>5648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>macro</td>\n",
       "      <td>0.8901</td>\n",
       "      <td>0.9057</td>\n",
       "      <td>0.8978</td>\n",
       "      <td>5648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>weighted</td>\n",
       "      <td>0.9064</td>\n",
       "      <td>0.9193</td>\n",
       "      <td>0.9128</td>\n",
       "      <td>5648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Tag Precision  Recall      F1 Support\n",
       "0       LOC    0.9206  0.9317  0.9261    1668\n",
       "1      MISC    0.7907  0.8234  0.8067     702\n",
       "2       ORG    0.8886  0.9073  0.8978    1661\n",
       "3       PER    0.9604  0.9604  0.9604    1617\n",
       "4     micro    0.9058  0.9193  0.9125    5648\n",
       "5     macro    0.8901  0.9057  0.8978    5648\n",
       "6  weighted    0.9064  0.9193  0.9128    5648"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.entity_non_strict_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_report = pd.read_json(\n",
    "\tbase_folder / 'token_report.json',\n",
    "\tlines=True\n",
    ")\n",
    "token_report.to_json(\n",
    "\tbase_folder / scrapped_token_report_path,\n",
    "\tlines=True, orient='records'\n",
    ")\n",
    "entity_report = pd.read_json(\n",
    "\tbase_folder / 'entity_report.json',\n",
    "\tlines=True\n",
    ")\n",
    "entity_report.to_json(\n",
    "\tbase_folder / scrapped_entity_report_path,\n",
    "\tlines=True, orient='records'\n",
    ")\n",
    "results = pd.read_json(\n",
    "\tbase_folder / 'results.json',\n",
    "\tlines=True\n",
    ")\n",
    "results.to_json(\n",
    "\tbase_folder / scrapped_results_path,\n",
    "\tlines=True, orient='records'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_results(metrics).to_json(\n",
    "\tbase_folder / original_results_path,\n",
    "\tlines=True, orient='records'\n",
    ")\n",
    "metrics.token_report.to_json(\n",
    "\tbase_folder / original_token_report_path,\n",
    "\tlines=True, orient='records'\n",
    ")\n",
    "\n",
    "metrics.entity_non_strict_report.to_json(\n",
    "\tbase_folder / original_entity_report_path,\n",
    "\tlines=True, orient='records'\n",
    ")\n",
    "metrics.entity_strict_report.to_json(\n",
    "\tbase_folder / strict_entity_report_path,\n",
    "\tlines=True, orient='records'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_results(metrics).to_json(\n",
    "\tbase_folder / original_results_path,\n",
    "\tlines=True, orient='records'\n",
    ")\n",
    "metrics.token_report.to_json(\n",
    "\tbase_folder / original_token_report_path,\n",
    "\tlines=True, orient='records'\n",
    ")\n",
    "\n",
    "metrics.entity_non_strict_report.to_json(\n",
    "\tbase_folder / original_entity_report_path,\n",
    "\tlines=True, orient='records'\n",
    ")\n",
    "metrics.entity_strict_report.to_json(\n",
    "\tbase_folder / strict_entity_report_path,\n",
    "\tlines=True, orient='records'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
