{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "# This appends the directory one level up (the root of your project) to the sys.path.\n",
    "# Modify the path depending on the location of modules you want to import.\n",
    "sys.path.append(os.path.abspath('../'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.config_managers import DashboardConfigManager\n",
    "from dataManager import DataManager\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = Path(\"/Users/ay227/Desktop/Final-Year/Thesis-Experiments/Online-Dashboard-Phase/dashboard-config.yaml\")\n",
    "config_manager = DashboardConfigManager(CONFIG_PATH)\n",
    "dev_config = config_manager.development_config    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dash import Dash, dcc, html, Output, Input, State\n",
    "app = Dash(__name__, suppress_callback_exceptions=True)\n",
    "\n",
    "app_config = config_manager.app_config\n",
    "server = app.server  # Flask server instance for caching\n",
    "variants_data = None\n",
    "\n",
    "data_manager = DataManager(config_manager, server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_manager.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_data = data_manager.variants_data['ANERCorp_CamelLab_arabertv02'].analysis_data\n",
    "errors = analysis_data[analysis_data['Error Type'] != 'No Errors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from seqeval.scheme import auto_detect, Entities\n",
    "from seqeval.metrics.sequence_labeling import get_entities\n",
    "\n",
    "class EntityAnnotator:\n",
    "    def __init__(self, y_true, y_pred):\n",
    "        \"\"\"Initialize the annotator with true and predicted labels.\"\"\"\n",
    "        self.y_true = y_true\n",
    "        self.y_pred = y_pred\n",
    "        self.scheme = auto_detect(self.y_true, False)\n",
    "    \n",
    "    def extract_entities(self):\n",
    "        \"\"\"Extract entities based on the scheme detected.\"\"\"\n",
    "        self.entities_strict_true = Entities(self.y_true, self.scheme, False)\n",
    "        self.entities_strict_pred = Entities(self.y_pred, self.scheme, False)\n",
    "        self.entities_true = get_entities(self.y_true)\n",
    "        self.entities_pred = get_entities(self.y_pred)\n",
    "\n",
    "    def process_strict_entities(self, y_true, entities_true, sen_id):\n",
    "        \"\"\"Process entities strictly, labeling full spans in a sentence.\"\"\"\n",
    "        max_len = len(y_true[sen_id])\n",
    "        results = ['O'] * max_len\n",
    "        for idx in range(max_len):\n",
    "            for entity in entities_true[sen_id]:\n",
    "                _, t, s, e = entity.to_tuple()\n",
    "                if s == idx and (e-s) > 0:\n",
    "                    for i in range(e-s):\n",
    "                        results[s + i] = t\n",
    "                elif (e-s) == 0:\n",
    "                    results[s] = t\n",
    "        return results\n",
    "    \n",
    "    def process_non_strict_entities(self, y_true, sen_id):\n",
    "        \"\"\"Process entities non-strictly, marking only the start and end of each entity.\"\"\"\n",
    "        max_len = len(y_true[sen_id])\n",
    "        results = ['O'] * max_len\n",
    "        for entity in get_entities(y_true[sen_id]):\n",
    "            t, s, e = entity\n",
    "            if s == e:\n",
    "                # If start and end are the same, only mark the start\n",
    "                results[s] = t\n",
    "            else:\n",
    "                # Mark all indices from start to end inclusive\n",
    "                for i in range(s, e + 1):\n",
    "                    results[i] = t\n",
    "        return results\n",
    "\n",
    "    def process_sentences(self, analysis_data, y_data, entities, label_column, strict=False):\n",
    "        \"\"\"Annotate sentences with entity information, either strictly or non-strictly.\"\"\"\n",
    "        entity_annotations = []\n",
    "        for sentence_id, sentence_df in analysis_data.groupby('Sentence Ids'):\n",
    "            if strict: \n",
    "                results = self.process_strict_entities(y_data, entities, sentence_id)\n",
    "            else:\n",
    "                results = self.process_non_strict_entities(y_data, sentence_id)\n",
    "            original_series = sentence_df[label_column]\n",
    "            is_metadata = original_series.apply(lambda x: x not in ['[CLS]', '[SEP]', 'IGNORED'])\n",
    "            new_series = original_series.copy()\n",
    "            new_series.loc[is_metadata] = results\n",
    "            entity_annotations.append(new_series)\n",
    "        return pd.concat(entity_annotations)\n",
    "\n",
    "    def annotate_entity_info(self, analysis_data):\n",
    "        \"\"\"Add annotated entity information to the analysis data for both true and predicted labels.\"\"\"\n",
    "        self.extract_entities()  # Ensure entities are extracted before processing\n",
    "        analysis_data['Strict True Entities'] = self.process_sentences(analysis_data, self.y_true, self.entities_strict_true.entities, 'True Labels', True)\n",
    "        analysis_data['Strict Pred Entities'] = self.process_sentences(analysis_data, self.y_pred, self.entities_strict_pred.entities, 'Pred Labels', True)\n",
    "        analysis_data['True Entities'] = self.process_sentences(analysis_data, self.y_true, self.entities_true, 'True Labels')\n",
    "        analysis_data['Pred Entities'] = self.process_sentences(analysis_data, self.y_pred, self.entities_pred, 'Pred Labels')\n",
    "        \n",
    "        return analysis_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "from seqeval.scheme import auto_detect\n",
    "from seqeval.metrics.sequence_labeling import get_entities\n",
    "from seqeval.scheme import Entities\n",
    "\n",
    "file_name = '/Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/BaseLineExperiment/ANERCorp_CamelLab_arabertv02/fine_tuning/evaluation_metrics.json'\n",
    "with open(file_name, 'r') as file:\n",
    "    entity_outputs = json.load(file)  # Use json.load() to read file, not json.loads()\n",
    "\n",
    "def extract_entities(y_data, scheme):\n",
    "        # Replace with the Entities() logic if provided\n",
    "        return Entities(y_data, scheme, False)\n",
    "core_data = analysis_data[analysis_data['Labels']!= -100].copy()\n",
    "\n",
    "\n",
    "# y_true = entity_outputs['entity_outputs']['y_true']\n",
    "y_true = core_data.groupby('Sentence Ids')['True Labels'].apply(list).tolist()\n",
    "# y_pred = entity_outputs['entity_outputs']['y_pred']\n",
    "y_pred = core_data.groupby('Sentence Ids')['Pred Labels'].apply(list).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator = EntityAnnotator(y_true, y_pred)\n",
    "updated_analysis_data = annotator.annotate_entity_info(analysis_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_analysis_data[['True Entities', 'TR Entity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(updated_analysis_data['True Entities'] != updated_analysis_data['TR Entity']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_analysis_data[updated_analysis_data['True Entities'] != updated_analysis_data['Strict True Entities']][['Sentence Ids', 'True Entities', 'TR Entity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_id = 15\n",
    "get_entities(y_true[sen_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = updated_analysis_data[updated_analysis_data['Sentence Ids'] == sen_id]\n",
    "\n",
    "ts[~ts['True Labels'].isin(['[CLS]', '[SEP]', 'IGNORED'])][['Sentence Ids', 'True Entities', 'Strict True Entities', 'Pred Entities', 'Strict Pred Entities','Pred Labels', 'True Labels']].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme = auto_detect(y_true, False)\n",
    "entities_true = extract_entities(y_true)\n",
    "entities_pred = extract_entities(y_pred)\n",
    "true_entities = get_entities(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_strict_entities(y_true, entities_true, sen_id):\n",
    "    \"\"\"\n",
    "    Process entities for a given sentence ID and return a list of results where entities are labeled,\n",
    "    and non-entity indices are marked as 'O'.\n",
    "\n",
    "    Args:\n",
    "    y_true (dict): Dictionary where keys are sentence IDs and values are lists of true labels.\n",
    "    entities_true (dict): Dictionary where keys are sentence IDs and values are lists of entity objects with a to_tuple method.\n",
    "    sen_id (int): Sentence ID to process.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of labels with entities labeled accordingly and other indices as 'O'.\n",
    "    \"\"\"\n",
    "    # Determine the length needed for the results list\n",
    "    max_len = len(y_true[sen_id])\n",
    "\n",
    "    # Initialize results with 'O' for all expected indices\n",
    "    results = ['O'] * max_len\n",
    "\n",
    "    # Iterate over each index in the range of y_true for the given sentence ID\n",
    "    for idx in range(max_len):\n",
    "        # Check for entities at this index\n",
    "        for entity in entities_true[sen_id]:\n",
    "            _, t, s, e = entity.to_tuple()\n",
    "            # Check if the entity starts at this index and has length\n",
    "            if s == idx and (e-s) > 0:\n",
    "                for i in range(e-s):\n",
    "                    results[s + i] = t  # Replace 'some_label' with your intended label\n",
    "            elif (e-s) == 0:\n",
    "                results[s] = t  # Set your intended label for zero-length entities\n",
    "            # If none of the conditions are met, it will retain 'O'\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_annotations = []\n",
    "for sentence_id, sentence_df in analysis_data.groupby('Sentence Ids'):\n",
    "    results = process_strict_entities(y_true, entities_true.entities, sentence_id)\n",
    "    original_series = sentence_df['True Labels']\n",
    "    is_metadata = original_series.apply(lambda x: x not in ['[CLS]', '[SEP]', 'IGNORED'])\n",
    "    new_series = original_series.copy()\n",
    "    new_series.loc[is_metadata] = results\n",
    "    entity_annotations.append(new_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(entity_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_id = 18\n",
    "data = analysis_data[~analysis_data['TR Entity'].isin(['[CLS]', '[SEP]', 'IGNORED'])]\n",
    "sen_data = data[data['Sentence Ids'] == sen_id].reset_index()\n",
    "sen_data[['Sentence Ids', 'True Labels', 'TR Entity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the length needed for the results list\n",
    "max_len = len(y_true[sen_id])\n",
    "\n",
    "# Initialize results with 'O' for all expected indices\n",
    "results = ['O'] * max_len\n",
    "\n",
    "# Iterate over each index in the range of y_true[694]\n",
    "for idx in range(max_len):\n",
    "    # Check for entities at this index\n",
    "    for entity in entities_true.entities[sen_id]:\n",
    "        sen_id, t, s, e = entity.to_tuple()\n",
    "        # Check if the entity starts at this index and has length\n",
    "        if s == idx and (e-s) > 0:\n",
    "            for i in range(e-s):\n",
    "                results[s + i] = t  # Replace 'some_label' with your intended label\n",
    "        elif (e-s) == 0:\n",
    "            results[s] = t  # Again, set your intended label for zero-length entities\n",
    "        # If none of the conditions are met, it will retain 'O'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the length needed for the results list\n",
    "max_len = len(y_true[sen_id])\n",
    "\n",
    "# Initialize results with 'O' for all expected indices\n",
    "lresults = ['O'] * max_len\n",
    "\n",
    "# Iterate over each index in the range of y_true[694]\n",
    "for idx in range(max_len):\n",
    "    # Check for entities at this index\n",
    "    for entity in get_entities(y_true[sen_id]):\n",
    "        t, s, e = entity\n",
    "        # Check if the entity starts at this index and has length\n",
    "        \n",
    "        if s == e:\n",
    "            # If start and end are the same, only mark the start\n",
    "            lresults[s] = t\n",
    "        else:\n",
    "            # Mark all indices from start to end inclusive\n",
    "            for i in range(s, e + 1):\n",
    "                lresults[i] = t\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_series = analysis_data[analysis_data[\"Sentence Ids\"] == sen_id]['True Labels']\n",
    "is_metadata = original_series.apply(lambda x: x not in ['[CLS]', '[SEP]', 'IGNORED'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_series = original_series.copy()\n",
    "non_metadata_indices = original_series.index[is_metadata]\n",
    "new_series.loc[non_metadata_indices] = results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_data['TR Entity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_entities(y_true[sen_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_true.entities[sen_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, e in enumerate(lresults\n",
    "):\n",
    "    print(i, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the length needed for the results list\n",
    "max_len = len(y_true[0])\n",
    "\n",
    "# Initialize results with 'O' for all expected indices\n",
    "results = ['O'] * max_len\n",
    "\n",
    "# Iterate over each index in the range of y_true[694]\n",
    "for idx in range(max_len):\n",
    "    # Check for entities at this index\n",
    "    for entity in entities_true.entities[0]:\n",
    "        sen_id, t, s, e = entity.to_tuple()\n",
    "        # Check if the entity starts at this index and has length\n",
    "        if s == idx and (e-s) > 0:\n",
    "            for i in range(e-s):\n",
    "                results[s + i] = t  # Replace 'some_label' with your intended label\n",
    "        elif (e-s) == 0:\n",
    "            results[s] = t  # Again, set your intended label for zero-length entities\n",
    "        # If none of the conditions are met, it will retain 'O'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors[errors['Error Type'] == 'Type'][['True Labels', 'Pred Labels', 'Error Type']].sample(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors[['True Labels', 'Pred Labels', 'Error Type']].sample(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_error(true_label, pred_label):\n",
    "        # If both are the same, it's correct (no error)\n",
    "        if true_label == pred_label:\n",
    "            return \"No Errors\"\n",
    "        \n",
    "        # Handle cases where one or both labels are 'O'\n",
    "        if true_label == 'O' and pred_label != 'O':\n",
    "            return \"Chunk\"  # False entity predicted\n",
    "        if true_label != 'O' and pred_label == 'O':\n",
    "            return \"Exclusion\"  # Missed entity and chunk boundary\n",
    "        \n",
    "        # Extract entity types without position tags (like \"B-\", \"I-\")\n",
    "        true_entity = true_label.split(\"-\")[-1] if \"-\" in true_label else true_label\n",
    "        pred_entity = pred_label.split(\"-\")[-1] if \"-\" in pred_label else pred_label\n",
    "\n",
    "        # If entity types are different (e.g., LOC vs. PER)\n",
    "        if true_entity != pred_entity:\n",
    "            # If both entity type and position (B- vs I-) are wrong\n",
    "            return \"Type and Chunk\" if true_label[0] != pred_label[0] else \"Type\"\n",
    "\n",
    "        # If entity types are the same but position tags (B- vs I-) are wrong\n",
    "        return \"Chunk\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_name = '/Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/BaseLineExperiment/ANERCorp_CamelLab_arabertv02/fine_tuning/evaluation_metrics.json'\n",
    "with open(file_name, 'r') as file:\n",
    "    entity_outputs = json.load(file)  # Use json.load() to read file, not json.loads()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = entity_outputs['entity_outputs']['y_true']\n",
    "y_pred = entity_outputs['entity_outputs']['y_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, classification_report, f1_score\n",
    "from seqeval.scheme import IOB2\n",
    "\n",
    "print(classification_report(y_true, y_pred, mode='strict', digits=4))\n",
    "\n",
    "f1_score(y_true, y_pred, mode='strict', scheme=IOB2,  average='micro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred, mode=None, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "\n",
    "# Ground truth (true labels)\n",
    "y_true = [\n",
    "    ['O', 'B-PER', 'I-PER', 'O', 'B-LOC', 'O'],\n",
    "    ['O', 'B-ORG', 'I-ORG', 'O']\n",
    "]\n",
    "\n",
    "# Predicted labels (with minor errors)\n",
    "y_pred = [\n",
    "    ['O', 'B-PER', 'O', 'O', 'B-LOC', 'O'],  # Misses I-PER\n",
    "    ['O', 'B-ORG', 'O', 'O']                # Misses I-ORG\n",
    "]\n",
    "\n",
    "# Default mode\n",
    "f1_default = f1_score(y_true, y_pred, average='micro', mode=None)\n",
    "\n",
    "# Strict mode\n",
    "f1_strict = f1_score(y_true, y_pred, average='micro', mode='strict')\n",
    "\n",
    "print(\"Default Mode F1 Score:\", f1_default)\n",
    "print(\"Strict Mode F1 Score:\", f1_strict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, classification_report\n",
    "from seqeval.scheme import IOB1\n",
    "# Define data\n",
    "y_true = [['B-PER', 'I-PER', 'O', 'B-ORG', 'I-ORG', 'O']]\n",
    "y_pred = [['B-PER', 'I-ORG', 'O', 'B-ORG', 'I-ORG', 'O']]\n",
    "\n",
    "# Strict mode, no scheme\n",
    "# f1_no_scheme_strict = f1_score(y_true, y_pred, average='micro', mode='strict', scheme=None)\n",
    "f1_no_scheme_strict = classification_report(y_true, y_pred, mode='strict', zero_division='Warn')\n",
    "no_strict = classification_report(y_true, y_pred, mode=None,)\n",
    "\n",
    "print(\"Strict Mode with No Scheme F1 Score:\", f1_no_scheme_strict)\n",
    "\n",
    "print(\"No Strict Mode with F1 Score:\", no_strict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entity_y_true = get_entities(y_true)\n",
    "entity_y_pred = get_entities(y_pred)\n",
    "\n",
    "# Example usage\n",
    "# conf_matrix = calculate_confusion_matrix([e.to_tuple()[1:] for sen in entities_true.entities for e in sen], [e.to_tuple()[1:] for sen in entities_pred.entities for e in sen])\n",
    "\n",
    "\n",
    "\n",
    "conf_matrix = calculate_confusion_matrix(entity_y_true, entity_y_pred)\n",
    "print(conf_matrix)\n",
    "fn_errors = compute_false_negatives(entity_y_true, entity_y_pred)\n",
    "fp_errors = compute_false_positives(entity_y_true, entity_y_pred)\n",
    "\n",
    "print(\"False Negatives:\", dict(fn_errors))\n",
    "print(\"False Positives:\", dict(fp_errors))\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "entities_true = Entities(y_true, scheme, False)\n",
    "entities_pred = Entities(y_pred, scheme, False)\n",
    "true_entity_type = flatten_strict_entities(entities_true)\n",
    "pred_entity_type = flatten_strict_entities(entities_pred)\n",
    "# Example usage\n",
    "# conf_matrix = calculate_confusion_matrix([e.to_tuple()[1:] for sen in entities_true.entities for e in sen], [e.to_tuple()[1:] for sen in entities_pred.entities for e in sen])\n",
    "\n",
    "\n",
    "\n",
    "conf_matrix = calculate_confusion_matrix(true_entity_type, pred_entity_type)\n",
    "print(conf_matrix)\n",
    "fn_errors = compute_false_negatives(true_entity_type, pred_entity_type)\n",
    "fp_errors = compute_false_positives(true_entity_type, pred_entity_type)\n",
    "\n",
    "print(\"False Negatives:\", dict(fn_errors))\n",
    "print(\"False Positives:\", dict(fp_errors))\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_true.entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_pred.entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(no_strict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.scheme import IOB2, Tokens, auto_detect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme = auto_detect(y_true, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = Tokens(['B-PER', 'I-PER', 'O', 'B-LOC'], scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = {\n",
    "#     'y_true': [['O', 'O', 'B-MISC', 'I-MISC', 'B-MISC', 'O', 'O'], ['B-PER', 'I-PER', 'O']],\n",
    "# \t'y_pred': [['O', 'O', 'B-MISC', 'I-LOC', 'B-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics.sequence_labeling import get_entities\n",
    "# Example usage\n",
    "entity_y_true = get_entities(entity_outputs['entity_outputs']['y_true'])\n",
    "entity_y_pred = get_entities(entity_outputs['entity_outputs']['y_pred'])\n",
    "\n",
    "from seqeval.scheme import Entities\n",
    "entities_true = Entities(entity_outputs['entity_outputs']['y_true'], scheme, False)\n",
    "entities_pred = Entities(entity_outputs['entity_outputs']['y_pred'], scheme, False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "def flatten_strict_entities(entities):\n",
    "    return [e.to_tuple()[1:] for sen in entities.entities for e in sen]\n",
    "\n",
    "def calculate_confusion_matrix(y_true, y_pred):\n",
    "    # Initialize confusion matrix data structure\n",
    "    types = set([ent[0] for ent in y_true]).union([ent[0] for ent in y_pred])\n",
    "    confusion_matrix = {typ: {'TP': 0, 'FP': 0, 'FN': 0} for typ in types}\n",
    "\n",
    "\n",
    "    # Track matched predictions to avoid counting them more than once\n",
    "    matched_pred_indices = set()\n",
    "\n",
    "    # Check each true entity against predicted entities\n",
    "    for true_ent in y_true:\n",
    "        true_type, true_start, true_end = true_ent\n",
    "        match_found = False\n",
    "\n",
    "        for idx, pred_ent in enumerate(y_pred):\n",
    "            pred_type, pred_start, pred_end = pred_ent\n",
    "\n",
    "            if idx not in matched_pred_indices and true_type == pred_type and true_start == pred_start and true_end == pred_end:\n",
    "                confusion_matrix[true_type]['TP'] += 1\n",
    "                matched_pred_indices.add(idx)\n",
    "                match_found = True\n",
    "                break\n",
    "        \n",
    "        if not match_found:\n",
    "            confusion_matrix[true_type]['FN'] += 1\n",
    "\n",
    "\n",
    "    # Any unmatched prediction is a false positive\n",
    "    for idx, pred_ent in enumerate(y_pred):\n",
    "        if idx not in matched_pred_indices:\n",
    "            pred_type = pred_ent[0]\n",
    "            confusion_matrix[pred_type]['FP'] += 1\n",
    "\n",
    "    return confusion_matrix\n",
    "\n",
    "entities_true = Entities(entity_outputs['entity_outputs']['y_true'], scheme, False)\n",
    "entities_pred = Entities(entity_outputs['entity_outputs']['y_pred'], scheme, False)\n",
    "true_entity_type = flatten_strict_entities(entities_true)\n",
    "pred_entity_type = flatten_strict_entities(entities_pred)\n",
    "# Example usage\n",
    "# conf_matrix = calculate_confusion_matrix([e.to_tuple()[1:] for sen in entities_true.entities for e in sen], [e.to_tuple()[1:] for sen in entities_pred.entities for e in sen])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = calculate_confusion_matrix(entity_y_true, entity_y_pred)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(627+151+751+338) / ((627+92)+(151+154)+(751+49)+(338+121))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1867 / 2201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = calculate_confusion_matrix(true_entity_type, pred_entity_type)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_metrics = {}\n",
    "for metric in ['TP', 'FP', 'FN']:\n",
    "    total_metrics[metric] = sum(details[metric] for details in conf_matrix.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1976 / (1976+189)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "def compute_false_negatives(y_true, y_pred):\n",
    "    fn_counts = defaultdict(Counter)\n",
    "    true_indexed = {(t[1], t[2]): t[0] for t in y_true}  # Index true entities by boundaries\n",
    "    pred_indexed = {(p[1], p[2]): p[0] for p in y_pred}  # Index predicted entities by boundaries\n",
    "\n",
    "    # Iterate through true entities to find false negatives\n",
    "    for (t_start, t_end), t_type in true_indexed.items():\n",
    "        if (t_start, t_end) not in pred_indexed or pred_indexed[(t_start, t_end)] != t_type:\n",
    "            # No matching prediction or type mismatch at the same position\n",
    "            matched_type = pred_indexed.get((t_start, t_end), 'Boundary')\n",
    "            fn_counts[t_type][matched_type] += 1\n",
    "\n",
    "    return fn_counts\n",
    "\n",
    "def compute_false_positives(y_true, y_pred):\n",
    "    fp_counts = defaultdict(Counter)\n",
    "    true_indexed = {(t[1], t[2]): t[0] for t in y_true}  # Index true entities by boundaries\n",
    "    pred_indexed = {(p[1], p[2]): p[0] for p in y_pred}  # Index predicted entities by boundaries\n",
    "\n",
    "    # Iterate through predicted entities to find false positives\n",
    "    for (p_start, p_end), p_type in pred_indexed.items():\n",
    "        if (p_start, p_end) not in true_indexed or true_indexed[(p_start, p_end)] != p_type:\n",
    "            # No matching true entity or type mismatch at the same position\n",
    "            matched_type = true_indexed.get((p_start, p_end), 'Boundary')\n",
    "            fp_counts[p_type][matched_type] += 1\n",
    "\n",
    "    return fp_counts\n",
    "\n",
    "# Example usage\n",
    "fn_errors = compute_false_negatives(entity_y_true, entity_y_pred)\n",
    "fp_errors = compute_false_positives(entity_y_true, entity_y_pred)\n",
    "\n",
    "print(\"False Negatives:\", dict(fn_errors))\n",
    "print(\"False Positives:\", dict(fp_errors))\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Your original data\n",
    "data = conf_matrix\n",
    "\n",
    "# Prepare lists for DataFrame construction\n",
    "actual = []\n",
    "predicted = []\n",
    "counts = []\n",
    "\n",
    "for (act, pred), count in data.items():\n",
    "    actual.append(act)\n",
    "    predicted.append('None' if pred is None else pred)  # Replace None with 'None' for better visualization\n",
    "    counts.append(count)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'Actual': actual, 'Predicted': predicted, 'Count': counts})\n",
    "\n",
    "# Pivot to format suitable for heatmap\n",
    "pivot_table = df.pivot(index='Actual', columns='Predicted', values='Count').fillna(0)\n",
    "\n",
    "# Generate heatmap\n",
    "fig = px.imshow(pivot_table,\n",
    "                labels=dict(x=\"Predicted Entity Type\", y=\"Actual Entity Type\", color=\"Count\"),\n",
    "                x=pivot_table.columns,\n",
    "                y=pivot_table.index,\n",
    "                text_auto=True,\n",
    "                aspect=\"auto\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Entity Recognition Confusion Matrix\",\n",
    "    xaxis_title=\"Predicted Entity Type\",\n",
    "    yaxis_title=\"Actual Entity Type\"\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Your original data\n",
    "data = conf_matrix1\n",
    "\n",
    "# Prepare lists for DataFrame construction\n",
    "actual = []\n",
    "predicted = []\n",
    "counts = []\n",
    "\n",
    "for (act, pred), count in data.items():\n",
    "    actual.append(act)\n",
    "    predicted.append('None' if pred is None else pred)  # Replace None with 'None' for better visualization\n",
    "    counts.append(count)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'Actual': actual, 'Predicted': predicted, 'Count': counts})\n",
    "\n",
    "# Pivot to format suitable for heatmap\n",
    "pivot_table = df.pivot(index='Actual', columns='Predicted', values='Count').fillna(0)\n",
    "\n",
    "# Generate heatmap\n",
    "fig = px.imshow(pivot_table,\n",
    "                labels=dict(x=\"Predicted Entity Type\", y=\"Actual Entity Type\", color=\"Count\"),\n",
    "                x=pivot_table.columns,\n",
    "                y=pivot_table.index,\n",
    "                text_auto=True,\n",
    "                aspect=\"auto\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Entity Recognition Confusion Matrix\",\n",
    "    xaxis_title=\"Predicted Entity Type\",\n",
    "    yaxis_title=\"Actual Entity Type\"\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITY = 'LOC'\n",
    "entity_false_negatives = {ENTITY: Counter()}\n",
    "false_negatives = set([e for e in entity_y_true if e[0] == 'LOC']) - set([e for e in entity_y_pred if e[0] == 'LOC'])\n",
    "for e in false_negatives:\n",
    "    t_type, t_start, t_end = e\n",
    "    for pred_ent in entity_y_pred:\n",
    "        p_type, p_start, p_end = pred_ent\n",
    "        if t_start == p_start and t_start == p_end:\n",
    "            if p_type == 'LOC':\n",
    "                print(pred_ent)\n",
    "            entity_false_negatives[t_type][p_type]+=1\n",
    "            \n",
    "\n",
    "ENTITY = 'LOC'\n",
    "entity_false_positives = {ENTITY: Counter()}\n",
    "false_positive = set([e for e in entity_y_pred if e[0] == ENTITY]) - set([e for e in entity_y_true if e[0] == ENTITY]) \n",
    "for e in false_positive:\n",
    "    p_type, p_start, p_end = e\n",
    "    for true_ent in entity_y_true:\n",
    "        t_type, t_start, t_end = true_ent\n",
    "        if t_start == p_start and t_end == p_end:\n",
    "            # if p_type == 'ORG':\n",
    "            #     # if t_type == 'ORG':\n",
    "            #         print(true_ent)\n",
    "            if p_type == t_type:\n",
    "                entity_false_positives[p_type][t_type]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 8786\n",
    "for entity in entity_y_true:\n",
    "    t, s, e = entity\n",
    "    if s == id:\n",
    "        print(entity)\n",
    "for entity in entity_y_pred:\n",
    "    t, s, e = entity\n",
    "    if s == id or e == id+1:\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in false_negatives:\n",
    "    t, s, e = entity\n",
    "    # if t == 'LOC':\n",
    "    #     print(entity)\n",
    "    if s == 8786:\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in false_positive:\n",
    "    t, s, e = entity\n",
    "   \n",
    "    if s == 16466 or e == 16467:\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in entity_y_true:\n",
    "    t, s, e = entity\n",
    "    if s == 16963:\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in entity_y_pred:\n",
    "    t, s, e = entity\n",
    "    if s == 16963:\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_data.iloc[8780:8790]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
