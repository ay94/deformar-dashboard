{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "# This appends the directory one level up (the root of your project) to the sys.path.\n",
    "# Modify the path depending on the location of modules you want to import.\n",
    "sys.path.append(os.path.abspath('../'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.config_managers import DashboardConfigManager\n",
    "from dataManager import DataManager\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = Path(\"/Users/ay227/Desktop/Final-Year/Thesis-Experiments/Online-Dashboard-Phase/dashboard-config.yaml\")\n",
    "config_manager = DashboardConfigManager(CONFIG_PATH)\n",
    "dev_config = config_manager.development_config    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dash import Dash, dcc, html, Output, Input, State\n",
    "app = Dash(__name__, suppress_callback_exceptions=True)\n",
    "\n",
    "app_config = config_manager.app_config\n",
    "server = app.server  # Flask server instance for caching\n",
    "variants_data = None\n",
    "\n",
    "data_manager = DataManager(config_manager, server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_manager.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_data = data_manager.variants_data['ANERCorp_CamelLab_arabertv02'].analysis_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_name = '/Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/BaseLineExperiment/ANERCorp_CamelLab_arabertv02/fine_tuning/evaluation_metrics.json'\n",
    "corpus_file = '/Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/ExperimentData/corpora.json'\n",
    "with open(file_name, 'r') as file:\n",
    "    entity_outputs = json.load(file)  # Use json.load() to read file, not json.loads()\n",
    "    \n",
    "with open(corpus_file, 'r') as file:\n",
    "    corpora = json.load(file)  # Use json.load() to read file, not json.loads()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from seqeval.metrics import classification_report as seq_classification\n",
    "from seqeval.metrics import f1_score as seq_f1\n",
    "from seqeval.metrics import precision_score as seq_precision\n",
    "from seqeval.metrics import recall_score as seq_recall\n",
    "from sklearn.metrics import classification_report as skl_classification\n",
    "from sklearn.metrics import f1_score as skl_f1\n",
    "from sklearn.metrics import precision_score as skl_precision\n",
    "from sklearn.metrics import recall_score as skl_recall\n",
    "from torch import nn\n",
    "from seqeval.scheme import IOB1, IOB2, IOE1, IOE2, IOBES, BILOU, auto_detect\n",
    "\n",
    "VALID_SCHEMES = {\n",
    "    'IOB1':IOB1,\n",
    "    'IOB2':IOB2,\n",
    "    'IOE1':IOE1,\n",
    "    'IOE2':IOE2,\n",
    "    'IOBES':IOBES,\n",
    "    'BILOU':BILOU\n",
    "    }\n",
    "class EvaluationStrategy(ABC):\n",
    "    def __init__(self, inv_map):\n",
    "        self.inv_map = inv_map\n",
    "        self.ignore_index = nn.CrossEntropyLoss().ignore_index\n",
    "\n",
    "    def align_predictions(self, predictions, truth):\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "        batch_size, seq_len = predictions.shape\n",
    "\n",
    "        truth_list = [[] for _ in range(batch_size)]\n",
    "        pred_list = [[] for _ in range(batch_size)]\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            for j in range(seq_len):\n",
    "                if truth[i, j] != self.ignore_index:\n",
    "                    truth_list[i].append(self.inv_map[truth[i][j]])\n",
    "                    pred_list[i].append(self.inv_map[predictions[i][j]])\n",
    "                    \n",
    "        if len(truth_list) != len(pred_list):\n",
    "            raise ValueError(\"Aligned predictions and truth have mismatched lengths.\")\n",
    "        return truth_list, pred_list\n",
    "\n",
    "    def create_classification_report(self, results):\n",
    "        lines = []\n",
    "        for line in results.strip().split(\"\\n\")[1:]:\n",
    "            if line.strip():\n",
    "                tokens = line.split()\n",
    "                # Remove intermediate aggregation if exists (multi-class)\n",
    "                if len(tokens) > 5:\n",
    "                    del tokens[1]\n",
    "                lines.append(tokens)\n",
    "        report = pd.DataFrame(\n",
    "            lines, columns=[\"Tag\", \"Precision\", \"Recall\", \"F1\", \"Support\"]\n",
    "        )\n",
    "        return report\n",
    "\n",
    "class TokenEvaluationStrategy(EvaluationStrategy):\n",
    "    def compute_metrics(self, true_labels, predictions):\n",
    "        try:\n",
    "            truth_list, pred_list = self.align_predictions(predictions, true_labels)\n",
    "        except:\n",
    "            logging.info('The labels already aligned, proceed with evaluation')\n",
    "            truth_list, pred_list = true_labels, predictions\n",
    "        \n",
    "        flat_truth = [item for sublist in truth_list for item in sublist]\n",
    "        flat_preds = [item for sublist in pred_list for item in sublist]\n",
    "        report = skl_classification(y_true=flat_truth, y_pred=flat_preds, digits=4)\n",
    "        report = self.create_classification_report(report)\n",
    "        cleaned_report = self.clean_report(report)\n",
    "        return {\n",
    "            \"Precision\": skl_precision(\n",
    "                y_true=flat_truth, y_pred=flat_preds, average=\"macro\"\n",
    "            ),\n",
    "            \"Recall\": skl_recall(\n",
    "                y_true=flat_truth, y_pred=flat_preds, average=\"macro\"\n",
    "                ),\n",
    "            \"F1\": skl_f1(\n",
    "                y_true=flat_truth, y_pred=flat_preds, average=\"macro\"\n",
    "                ),\n",
    "            \"classification\": cleaned_report,\n",
    "            \"output\": {\"y_true\": flat_truth, \"y_pred\": flat_preds},\n",
    "        }\n",
    "\n",
    "    def clean_report(self, report):\n",
    "        report = report.copy()\n",
    "        mask = report[\"Tag\"] == \"accuracy\"\n",
    "        accuracy_row = report[mask]\n",
    "        if not accuracy_row.empty:\n",
    "            # Get the accuracy value\n",
    "            accuracy_value = accuracy_row[\"Precision\"].values[\n",
    "                0\n",
    "            ]  # Assuming accuracy is stored in the 'Precision' column\n",
    "            accuracy_support = accuracy_row[\"Recall\"].values[\n",
    "                0\n",
    "            ]  # Assuming accuracy is stored in the 'Precision' column\n",
    "\n",
    "            # Set the precision, recall, and F1-score to the accuracy value\n",
    "            report.loc[mask, \"Precision\"] = accuracy_value\n",
    "            report.loc[mask, \"Recall\"] = accuracy_value\n",
    "            report.loc[mask, \"F1\"] = accuracy_value\n",
    "            report.loc[mask, \"Support\"] = accuracy_support\n",
    "\n",
    "            # Rename the tag from 'accuracy' to 'accuracy/micro' for clarity\n",
    "            report.loc[report[\"Tag\"] == \"accuracy\", \"Tag\"] = \"accuracy/micro\"\n",
    "        return report\n",
    "    \n",
    "class EntityEvaluationStrategy(EvaluationStrategy):\n",
    "    def compute_metrics(self, true_labels, predictions, entity_config):\n",
    "        scheme = entity_config.get('scheme')  # Default to 'none' if not specified\n",
    "\n",
    "        # Check if the scheme is valid and not 'none'\n",
    "        try:\n",
    "            truth_list, pred_list = self.align_predictions(predictions, true_labels)\n",
    "        except:\n",
    "            logging.info('The labels already aligned, proceed with evaluation')\n",
    "            truth_list, pred_list = true_labels, predictions\n",
    "            \n",
    "        strict_outputs = self._evaluate_strict(truth_list, pred_list, scheme)\n",
    "        non_strict_outputs = self._evaluate_non_strict(truth_list, pred_list)\n",
    "        \n",
    "        return {\n",
    "            \"strict\": strict_outputs,\n",
    "            \"non_strict\": non_strict_outputs,\n",
    "            \"output\": {\"y_true\": truth_list, \"y_pred\": pred_list}\n",
    "        }\n",
    "        \n",
    "        \n",
    "    \n",
    "    def _evaluate_strict(self, truth_list, pred_list, scheme):\n",
    "        \n",
    "        if scheme is not None and scheme in VALID_SCHEMES:\n",
    "            scheme_class = VALID_SCHEMES[scheme]\n",
    "            report = seq_classification(\n",
    "                    y_true=truth_list,\n",
    "                    y_pred=pred_list,\n",
    "                    digits=4,\n",
    "                    mode='strict',\n",
    "                    scheme=scheme_class,\n",
    "                )\n",
    "            precision = seq_precision(\n",
    "                    y_true=truth_list, y_pred=pred_list, average=\"micro\", mode='strict', scheme = scheme_class\n",
    "                )\n",
    "            recall = seq_recall(\n",
    "                    y_true=truth_list, y_pred=pred_list, average=\"micro\", mode='strict', scheme = scheme_class\n",
    "                )\n",
    "            f1 = seq_f1(\n",
    "                    y_true=truth_list, y_pred=pred_list, average=\"micro\", mode='strict', scheme = scheme_class\n",
    "                )\n",
    "            \n",
    "        else:\n",
    "            logging.info(\"The scheme is unspecified; seqeval will auto-detect the scheme.\")\n",
    "            report = seq_classification(\n",
    "                    y_true=truth_list,\n",
    "                    y_pred=pred_list,\n",
    "                    digits=4,\n",
    "                    mode='strict',\n",
    "                )\n",
    "            scheme_class = auto_detect(pred_list, False)\n",
    "            precision = seq_precision(\n",
    "                    y_true=truth_list, y_pred=pred_list, average=\"micro\", mode='strict', scheme = scheme_class\n",
    "                )\n",
    "            recall = seq_recall(\n",
    "                    y_true=truth_list, y_pred=pred_list, average=\"micro\", mode='strict', scheme = scheme_class\n",
    "                )\n",
    "            f1 = seq_f1(\n",
    "                    y_true=truth_list, y_pred=pred_list, average=\"micro\", mode='strict', scheme = scheme_class\n",
    "                )\n",
    "        \n",
    "        return {\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1\": f1,\n",
    "            \"classification\": self.create_classification_report(report),\n",
    "                \n",
    "            }\n",
    "    \n",
    "        \n",
    "    def _evaluate_non_strict(self, truth_list, pred_list,):\n",
    "        \n",
    "        report = seq_classification(\n",
    "                    y_true=truth_list,\n",
    "                    y_pred=pred_list,\n",
    "                    digits=4,\n",
    "                )\n",
    "        precision = seq_precision(\n",
    "                y_true=truth_list, y_pred=pred_list, average=\"micro\"\n",
    "            )\n",
    "        recall = seq_recall(\n",
    "                    y_true=truth_list, y_pred=pred_list, average=\"micro\"\n",
    "                )\n",
    "        f1 = seq_f1(\n",
    "                y_true=truth_list, y_pred=pred_list, average=\"micro\"\n",
    "            )\n",
    "        return {\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1\": f1,\n",
    "            \"classification\": self.create_classification_report(report),\n",
    "                \n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "class Evaluation:\n",
    "    def __init__(self, inv_map, y_true, y_pred, evaluation_config):\n",
    "        self.truths = y_true\n",
    "        self.predictions = y_pred\n",
    "        self.evaluation_config = evaluation_config\n",
    "        self.token_strategy = TokenEvaluationStrategy(inv_map)\n",
    "        self.entity_strategy = EntityEvaluationStrategy(inv_map)\n",
    "\n",
    "    \n",
    "\n",
    "    def evaluate(self):\n",
    "        token_metrics = self.token_strategy.compute_metrics(\n",
    "            self.truths, self.predictions\n",
    "        )\n",
    "        entity_metrics = self.entity_strategy.compute_metrics(\n",
    "            self.truths, self.predictions, self.evaluation_config\n",
    "        )\n",
    "\n",
    "        # Combine or store results as needed\n",
    "        return {\"Token_Level\": token_metrics, \"Entity_Level\": entity_metrics}\n",
    "\n",
    "    def _prepare_results(self, metrics):\n",
    "        results = pd.DataFrame.from_dict(self._round_and_slice(metrics))\n",
    "        report = metrics[\"classification\"]\n",
    "        output = metrics[\"output\"]\n",
    "        return results, report, output\n",
    "    \n",
    "    def _prepare_entity_results(self, metrics):\n",
    "        strict = metrics[\"strict\"]\n",
    "        non_strict = metrics[\"non_strict\"]\n",
    "        entity_strict_results = pd.DataFrame.from_dict(self._round_and_slice(strict))\n",
    "        entity_non_strict_results = pd.DataFrame.from_dict(self._round_and_slice(non_strict))\n",
    "        entity_strict_report = strict['classification']\n",
    "        entity_non_strict_report = non_strict['classification']\n",
    "        output = metrics[\"output\"]\n",
    "        return {\n",
    "            'entity_strict_results': entity_strict_results,\n",
    "            'entity_non_strict_results': entity_non_strict_results,\n",
    "            'entity_strict_report': entity_strict_report,\n",
    "            'entity_non_strict_report': entity_non_strict_report,\n",
    "            'output': output,\n",
    "        }\n",
    "\n",
    "    def _round_and_slice(self, dictionary):\n",
    "        # Slicing and rounding results for cleaner presentation\n",
    "        \n",
    "        keys_for_slicing = [\"Precision\", \"Recall\", \"F1\"]\n",
    "        sliced_dict = {key: [round(dictionary[key], 4)] for key in keys_for_slicing}\n",
    "        return sliced_dict\n",
    "    \n",
    "    def generate_results(self):\n",
    "        metrics = self.evaluate()\n",
    "        token_results, token_report, token_outputs = self._prepare_results(\n",
    "            metrics[\"Token_Level\"]\n",
    "        )\n",
    "        entity_level_outputs = self._prepare_entity_results(\n",
    "            metrics[\"Entity_Level\"]\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "        return {\n",
    "            \"token_results\": token_results,\n",
    "            \"token_report\": token_report,\n",
    "            \"token_outputs\": token_outputs,\n",
    "            \"entity_strict_results\": entity_level_outputs['entity_strict_results'],\n",
    "            \"entity_non_strict_results\": entity_level_outputs['entity_non_strict_results'],\n",
    "            \"entity_strict_report\": entity_level_outputs['entity_strict_report'],\n",
    "            \"entity_non_strict_report\": entity_level_outputs['entity_non_strict_report'],\n",
    "            \"entity_outputs\": entity_level_outputs['output'],\n",
    "        }\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Metrics:\n",
    "    token_results: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    token_report: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    token_outputs: dict = field(default_factory=dict)\n",
    "    entity_strict_results: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    entity_non_strict_results: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    entity_strict_report: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    entity_non_strict_report: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    entity_outputs: dict = field(default_factory=dict)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_dict(data: dict):\n",
    "        \"\"\"Create an instance from a dictionary.\"\"\"\n",
    "        required_keys = [\n",
    "        \"token_results\", \"token_report\", \"token_outputs\",\n",
    "        \"entity_strict_results\", \"entity_non_strict_results\",\n",
    "        \"entity_strict_report\", \"entity_non_strict_report\", \"entity_outputs\"\n",
    "        ]\n",
    "        missing_keys = [key for key in required_keys if key not in data]\n",
    "        if missing_keys:\n",
    "            raise ValueError(f\"Missing required keys in data: {missing_keys}\")\n",
    "        return Metrics(**data)\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"token_results\": self.token_results.to_dict(orient=\"records\"),\n",
    "            \"token_report\": self.token_report.to_dict(orient=\"records\"),\n",
    "            \"token_outputs\": self.token_outputs,\n",
    "            \"entity_strict_results\": self.entity_strict_results.to_dict(orient=\"records\"),     \n",
    "            \"entity_non_strict_results\": self.entity_non_strict_results.to_dict(orient=\"records\"),\n",
    "            \"entity_strict_report\": self.entity_strict_report.to_dict(orient=\"records\"), \n",
    "            \"entity_non_strict_report\": self.entity_non_strict_report.to_dict(orient=\"records\"), \n",
    "            \"entity_outputs\": self.entity_outputs,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = entity_outputs['entity_outputs']['y_true']\n",
    "y_pred = entity_outputs['entity_outputs']['y_pred']\n",
    "corpus = corpora['ANERCorp_CamelLab']\n",
    "labels_map = corpus[\"labels_map\"]\n",
    "inv_labels_map = {v: k for k, v in labels_map.items()}\n",
    "evaluation_config = {\n",
    "\t'scheme': 'IOB2'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_labels_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluation(\n",
    "                inv_labels_map, y_true, y_pred, evaluation_config\n",
    "            )\n",
    "results = evaluator.generate_results()\n",
    "metrics = Metrics.from_dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.entity_strict_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = evaluator.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outs['Entity_Level']['strict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, classification_report, f1_score\n",
    "from seqeval.scheme import IOB2\n",
    "\n",
    "print(classification_report(y_true, y_pred, mode='strict', digits=4))\n",
    "\n",
    "f1_score(y_true, y_pred, mode='strict', scheme=IOB2,  average='micro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred, mode=None, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "\n",
    "# Ground truth (true labels)\n",
    "y_true = [\n",
    "    ['O', 'B-PER', 'I-PER', 'O', 'B-LOC', 'O'],\n",
    "    ['O', 'B-ORG', 'I-ORG', 'O']\n",
    "]\n",
    "\n",
    "# Predicted labels (with minor errors)\n",
    "y_pred = [\n",
    "    ['O', 'B-PER', 'O', 'O', 'B-LOC', 'O'],  # Misses I-PER\n",
    "    ['O', 'B-ORG', 'O', 'O']                # Misses I-ORG\n",
    "]\n",
    "\n",
    "# Default mode\n",
    "f1_default = f1_score(y_true, y_pred, average='micro', mode=None)\n",
    "\n",
    "# Strict mode\n",
    "f1_strict = f1_score(y_true, y_pred, average='micro', mode='strict')\n",
    "\n",
    "print(\"Default Mode F1 Score:\", f1_default)\n",
    "print(\"Strict Mode F1 Score:\", f1_strict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, classification_report\n",
    "from seqeval.scheme import IOB1\n",
    "# Define data\n",
    "y_true = [['B-PER', 'I-PER', 'O', 'B-ORG', 'I-ORG', 'O']]\n",
    "y_pred = [['B-PER', 'I-ORG', 'O', 'B-ORG', 'I-ORG', 'O']]\n",
    "\n",
    "# Strict mode, no scheme\n",
    "# f1_no_scheme_strict = f1_score(y_true, y_pred, average='micro', mode='strict', scheme=None)\n",
    "f1_no_scheme_strict = classification_report(y_true, y_pred, mode='strict', zero_division='Warn')\n",
    "no_strict = classification_report(y_true, y_pred, mode=None,)\n",
    "\n",
    "print(\"Strict Mode with No Scheme F1 Score:\", f1_no_scheme_strict)\n",
    "\n",
    "print(\"No Strict Mode with F1 Score:\", no_strict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from seqeval.scheme import IOB2, Tokens, auto_detect\n",
    "from seqeval.metrics.sequence_labeling import get_entities\n",
    "from seqeval.scheme import Entities\n",
    "\n",
    "\n",
    "def flatten_strict_entities(entities):\n",
    "    return [e.to_tuple()[1:] for sen in entities.entities for e in sen]\n",
    "\n",
    "def calculate_confusion_matrix(y_true, y_pred):\n",
    "    # Initialize confusion matrix data structure\n",
    "    types = set([ent[0] for ent in y_true]).union([ent[0] for ent in y_pred])\n",
    "    confusion_matrix = {typ: {'TP': 0, 'FP': 0, 'FN': 0} for typ in types}\n",
    "\n",
    "\n",
    "    # Track matched predictions to avoid counting them more than once\n",
    "    matched_pred_indices = set()\n",
    "\n",
    "    # Check each true entity against predicted entities\n",
    "    for true_ent in y_true:\n",
    "        true_type, true_start, true_end = true_ent\n",
    "        match_found = False\n",
    "\n",
    "        for idx, pred_ent in enumerate(y_pred):\n",
    "            pred_type, pred_start, pred_end = pred_ent\n",
    "\n",
    "            if idx not in matched_pred_indices and true_type == pred_type and true_start == pred_start and true_end == pred_end:\n",
    "                confusion_matrix[true_type]['TP'] += 1\n",
    "                matched_pred_indices.add(idx)\n",
    "                match_found = True\n",
    "                break\n",
    "        \n",
    "        if not match_found:\n",
    "            confusion_matrix[true_type]['FN'] += 1\n",
    "\n",
    "\n",
    "    # Any unmatched prediction is a false positive\n",
    "    for idx, pred_ent in enumerate(y_pred):\n",
    "        if idx not in matched_pred_indices:\n",
    "            pred_type = pred_ent[0]\n",
    "            confusion_matrix[pred_type]['FP'] += 1\n",
    "\n",
    "    return confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "def compute_false_negatives(y_true, y_pred):\n",
    "    fn_counts = defaultdict(Counter)\n",
    "    true_indexed = {(t[1], t[2]): t[0] for t in y_true}  # Index true entities by boundaries\n",
    "    pred_indexed = {(p[1], p[2]): p[0] for p in y_pred}  # Index predicted entities by boundaries\n",
    "\n",
    "    # Iterate through true entities to find false negatives\n",
    "    for (t_start, t_end), t_type in true_indexed.items():\n",
    "        if (t_start, t_end) not in pred_indexed or pred_indexed[(t_start, t_end)] != t_type:\n",
    "            # No matching prediction or type mismatch at the same position\n",
    "            matched_type = pred_indexed.get((t_start, t_end), 'Boundary')\n",
    "            fn_counts[t_type][matched_type] += 1\n",
    "\n",
    "    return fn_counts\n",
    "\n",
    "def compute_false_positives(y_true, y_pred):\n",
    "    fp_counts = defaultdict(Counter)\n",
    "    true_indexed = {(t[1], t[2]): t[0] for t in y_true}  # Index true entities by boundaries\n",
    "    pred_indexed = {(p[1], p[2]): p[0] for p in y_pred}  # Index predicted entities by boundaries\n",
    "\n",
    "    # Iterate through predicted entities to find false positives\n",
    "    for (p_start, p_end), p_type in pred_indexed.items():\n",
    "        if (p_start, p_end) not in true_indexed or true_indexed[(p_start, p_end)] != p_type:\n",
    "            # No matching true entity or type mismatch at the same position\n",
    "            matched_type = true_indexed.get((p_start, p_end), 'Boundary')\n",
    "            fp_counts[p_type][matched_type] += 1\n",
    "\n",
    "    return fp_counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme = auto_detect(y_true, False)\n",
    "\n",
    "entities_true = Entities(y_true, scheme, False)\n",
    "entities_pred = Entities(y_pred, scheme, False)\n",
    "true_entity_type = flatten_strict_entities(entities_true)\n",
    "pred_entity_type = flatten_strict_entities(entities_pred)\n",
    "# Example usage\n",
    "# conf_matrix = calculate_confusion_matrix([e.to_tuple()[1:] for sen in entities_true.entities for e in sen], [e.to_tuple()[1:] for sen in entities_pred.entities for e in sen])\n",
    "\n",
    "\n",
    "\n",
    "conf_matrix = calculate_confusion_matrix(true_entity_type, pred_entity_type)\n",
    "print(conf_matrix)\n",
    "fn_errors = compute_false_negatives(true_entity_type, pred_entity_type)\n",
    "fp_errors = compute_false_positives(true_entity_type, pred_entity_type)\n",
    "\n",
    "print(\"False Negatives:\", dict(fn_errors))\n",
    "print(\"False Positives:\", dict(fp_errors))\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_y_true = get_entities(y_true)\n",
    "entity_y_pred = get_entities(y_pred)\n",
    "\n",
    "conf_matrix = calculate_confusion_matrix(entity_y_true, entity_y_pred)\n",
    "print(conf_matrix)\n",
    "fn_errors = compute_false_negatives(entity_y_true, entity_y_pred)\n",
    "fp_errors = compute_false_positives(entity_y_true, entity_y_pred)\n",
    "\n",
    "print(\"False Negatives:\", dict(fn_errors))\n",
    "print(\"False Positives:\", dict(fp_errors))\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "entity_y_true = get_entities(entity_outputs['entity_outputs']['y_true'])\n",
    "entity_y_pred = get_entities(entity_outputs['entity_outputs']['y_pred'])\n",
    "\n",
    "\n",
    "entities_true = Entities(entity_outputs['entity_outputs']['y_true'], scheme, False)\n",
    "entities_pred = Entities(entity_outputs['entity_outputs']['y_pred'], scheme, False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = calculate_confusion_matrix(entity_y_true, entity_y_pred)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(627+151+751+338) / ((627+92)+(151+154)+(751+49)+(338+121))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1867 / 2201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = calculate_confusion_matrix(entity_y_true, entity_y_pred)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_metrics = {}\n",
    "for metric in ['TP', 'FP', 'FN']:\n",
    "    total_metrics[metric] = sum(details[metric] for details in conf_matrix.values())\n",
    "print(total_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1867 / (1867+334)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Example usage\n",
    "fn_errors = compute_false_negatives(entity_y_true, entity_y_pred)\n",
    "fp_errors = compute_false_positives(entity_y_true, entity_y_pred)\n",
    "\n",
    "print(\"False Negatives:\", dict(fn_errors))\n",
    "print(\"False Positives:\", dict(fp_errors))\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Your original data\n",
    "data = conf_matrix\n",
    "\n",
    "# Prepare lists for DataFrame construction\n",
    "actual = []\n",
    "predicted = []\n",
    "counts = []\n",
    "\n",
    "for (act, pred), count in data.items():\n",
    "    actual.append(act)\n",
    "    predicted.append('None' if pred is None else pred)  # Replace None with 'None' for better visualization\n",
    "    counts.append(count)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'Actual': actual, 'Predicted': predicted, 'Count': counts})\n",
    "\n",
    "# Pivot to format suitable for heatmap\n",
    "pivot_table = df.pivot(index='Actual', columns='Predicted', values='Count').fillna(0)\n",
    "\n",
    "# Generate heatmap\n",
    "fig = px.imshow(pivot_table,\n",
    "                labels=dict(x=\"Predicted Entity Type\", y=\"Actual Entity Type\", color=\"Count\"),\n",
    "                x=pivot_table.columns,\n",
    "                y=pivot_table.index,\n",
    "                text_auto=True,\n",
    "                aspect=\"auto\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Entity Recognition Confusion Matrix\",\n",
    "    xaxis_title=\"Predicted Entity Type\",\n",
    "    yaxis_title=\"Actual Entity Type\"\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Your original data\n",
    "data = conf_matrix1\n",
    "\n",
    "# Prepare lists for DataFrame construction\n",
    "actual = []\n",
    "predicted = []\n",
    "counts = []\n",
    "\n",
    "for (act, pred), count in data.items():\n",
    "    actual.append(act)\n",
    "    predicted.append('None' if pred is None else pred)  # Replace None with 'None' for better visualization\n",
    "    counts.append(count)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'Actual': actual, 'Predicted': predicted, 'Count': counts})\n",
    "\n",
    "# Pivot to format suitable for heatmap\n",
    "pivot_table = df.pivot(index='Actual', columns='Predicted', values='Count').fillna(0)\n",
    "\n",
    "# Generate heatmap\n",
    "fig = px.imshow(pivot_table,\n",
    "                labels=dict(x=\"Predicted Entity Type\", y=\"Actual Entity Type\", color=\"Count\"),\n",
    "                x=pivot_table.columns,\n",
    "                y=pivot_table.index,\n",
    "                text_auto=True,\n",
    "                aspect=\"auto\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Entity Recognition Confusion Matrix\",\n",
    "    xaxis_title=\"Predicted Entity Type\",\n",
    "    yaxis_title=\"Actual Entity Type\"\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_tag = 'LOC'\n",
    "\n",
    "false_negatives = set([e for e in entity_y_true if e[0] == entity_tag]) - set([e for e in entity_y_pred if e[0] == entity_tag])\n",
    "for fn in false_negatives:\n",
    "  t, fn_s, fn_e = fn\n",
    "  for entity in entity_y_true:\n",
    "      t_t, t_s, t_e = entity\n",
    "      if fn_s == t_s or fn_e == t_e:\n",
    "        if t_s!=t_e:\n",
    "          print(entity)\n",
    "id = 5594\n",
    "for entity in entity_y_pred:\n",
    "    t, s, e = entity\n",
    "    if s == id:\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_tag = 'LOC'\n",
    "\n",
    "false_postive = set([e for e in entity_y_pred if e[0] == entity_tag]) - set([e for e in entity_y_true if e[0] == entity_tag])\n",
    "\n",
    "for fn in false_postive:\n",
    "  t, fn_s, fn_e = fn\n",
    "  for entity in entity_y_pred:\n",
    "      t_t, t_s, t_e = entity\n",
    "      if fn_s == t_s or fn_e == t_e:\n",
    "        if t_s!=t_e:\n",
    "          print(entity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so the false positive doesn't have to be false positive. \n",
    "id = 6445\n",
    "for entity in entity_y_true:\n",
    "    t, s, e = entity\n",
    "    if s == id:\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr =[\n",
    "        tok for sen in entity_outputs['entity_outputs']['y_pred']\n",
    "        for tok in sen\n",
    "        ]\n",
    "\n",
    "\n",
    "tr =[\n",
    "        tok for sen in entity_outputs['entity_outputs']['y_true']\n",
    "        for tok in sen\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr[8760:8790] == tr[8760:8790]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr[8760:8790]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "('LOC', 8786, 8787)\n",
    "('LOC', 8864, 8865)\n",
    "('LOC', 16466, 16467)\n",
    "('LOC', 5593, 5594)\n",
    "('LOC', 2545, 2546)\n",
    "('LOC', 25446, 25447)\n",
    "('LOC', 1615, 1616)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITY = 'LOC'\n",
    "entity_false_negatives = {ENTITY: Counter()}\n",
    "false_negatives = set([e for e in entity_y_true if e[0] == 'LOC']) - set([e for e in entity_y_pred if e[0] == 'LOC'])\n",
    "for e in false_negatives:\n",
    "    t_type, t_start, t_end = e\n",
    "    for pred_ent in entity_y_pred:\n",
    "        p_type, p_start, p_end = pred_ent\n",
    "        if t_start == p_start and t_start == p_end:\n",
    "            if p_type == 'LOC':\n",
    "                print(pred_ent)\n",
    "            entity_false_negatives[t_type][p_type]+=1\n",
    "            \n",
    "\n",
    "ENTITY = 'LOC'\n",
    "entity_false_positives = {ENTITY: Counter()}\n",
    "false_positive = set([e for e in entity_y_pred if e[0] == ENTITY]) - set([e for e in entity_y_true if e[0] == ENTITY]) \n",
    "for e in false_positive:\n",
    "    p_type, p_start, p_end = e\n",
    "    for true_ent in entity_y_true:\n",
    "        t_type, t_start, t_end = true_ent\n",
    "        if t_start == p_start and t_end == p_end:\n",
    "            # if p_type == 'ORG':\n",
    "            #     # if t_type == 'ORG':\n",
    "            #         print(true_ent)\n",
    "            if p_type == t_type:\n",
    "                entity_false_positives[p_type][t_type]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 8786\n",
    "for entity in entity_y_true:\n",
    "    t, s, e = entity\n",
    "    if s == id:\n",
    "        print(entity)\n",
    "for entity in entity_y_pred:\n",
    "    t, s, e = entity\n",
    "    if s == id or e == id+1:\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in false_negatives:\n",
    "    t, s, e = entity\n",
    "    # if t == 'LOC':\n",
    "    #     print(entity)\n",
    "    if s == 8786:\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in false_positive:\n",
    "    t, s, e = entity\n",
    "   \n",
    "    if s == 16466 or e == 16467:\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in entity_y_true:\n",
    "    t, s, e = entity\n",
    "    if s == 16963:\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in entity_y_pred:\n",
    "    t, s, e = entity\n",
    "    if s == 16963:\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
