{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_name = '/Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/BaseLineExperiment/ANERCorp_CamelLab_arabertv02/fine_tuning/evaluation_metrics.json'\n",
    "corpus_file = '/Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/ExperimentData/corpora.json'\n",
    "with open(file_name, 'r') as file:\n",
    "    entity_outputs = json.load(file)  # Use json.load() to read file, not json.loads()\n",
    "    \n",
    "with open(corpus_file, 'r') as file:\n",
    "    corpora = json.load(file)  # Use json.load() to read file, not json.loads()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from seqeval.metrics import classification_report as seq_classification\n",
    "from seqeval.metrics import f1_score as seq_f1\n",
    "from seqeval.metrics import precision_score as seq_precision\n",
    "from seqeval.metrics import recall_score as seq_recall\n",
    "from sklearn.metrics import classification_report as skl_classification\n",
    "from sklearn.metrics import f1_score as skl_f1\n",
    "from sklearn.metrics import precision_score as skl_precision\n",
    "from sklearn.metrics import recall_score as skl_recall\n",
    "from torch import nn\n",
    "from seqeval.scheme import IOB1, IOB2, IOE1, IOE2, IOBES, BILOU, auto_detect\n",
    "\n",
    "VALID_SCHEMES = {\n",
    "    'IOB1':IOB1,\n",
    "    'IOB2':IOB2,\n",
    "    'IOE1':IOE1,\n",
    "    'IOE2':IOE2,\n",
    "    'IOBES':IOBES,\n",
    "    'BILOU':BILOU\n",
    "    }\n",
    "class EvaluationStrategy(ABC):\n",
    "    def __init__(self, inv_map):\n",
    "        self.inv_map = inv_map\n",
    "        self.ignore_index = nn.CrossEntropyLoss().ignore_index\n",
    "\n",
    "    def align_predictions(self, predictions, truth):\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "        batch_size, seq_len = predictions.shape\n",
    "\n",
    "        truth_list = [[] for _ in range(batch_size)]\n",
    "        pred_list = [[] for _ in range(batch_size)]\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            for j in range(seq_len):\n",
    "                if truth[i, j] != self.ignore_index:\n",
    "                    truth_list[i].append(self.inv_map[truth[i][j]])\n",
    "                    pred_list[i].append(self.inv_map[predictions[i][j]])\n",
    "                    \n",
    "        if len(truth_list) != len(pred_list):\n",
    "            raise ValueError(\"Aligned predictions and truth have mismatched lengths.\")\n",
    "        return truth_list, pred_list\n",
    "\n",
    "    def create_classification_report(self, results):\n",
    "        lines = []\n",
    "        for line in results.strip().split(\"\\n\")[1:]:\n",
    "            if line.strip():\n",
    "                tokens = line.split()\n",
    "                # Remove intermediate aggregation if exists (multi-class)\n",
    "                if len(tokens) > 5:\n",
    "                    del tokens[1]\n",
    "                lines.append(tokens)\n",
    "        report = pd.DataFrame(\n",
    "            lines, columns=[\"Tag\", \"Precision\", \"Recall\", \"F1\", \"Support\"]\n",
    "        )\n",
    "        return report\n",
    "\n",
    "class TokenEvaluationStrategy(EvaluationStrategy):\n",
    "    def compute_metrics(self, true_labels, predictions):\n",
    "        try:\n",
    "            truth_list, pred_list = self.align_predictions(predictions, true_labels)\n",
    "        except:\n",
    "            logging.info('The labels already aligned, proceed with evaluation')\n",
    "            truth_list, pred_list = true_labels, predictions\n",
    "        \n",
    "        flat_truth = [item for sublist in truth_list for item in sublist]\n",
    "        flat_preds = [item for sublist in pred_list for item in sublist]\n",
    "        report = skl_classification(y_true=flat_truth, y_pred=flat_preds, digits=4)\n",
    "        report = self.create_classification_report(report)\n",
    "        cleaned_report = self.clean_report(report)\n",
    "        return {\n",
    "            \"Precision\": skl_precision(\n",
    "                y_true=flat_truth, y_pred=flat_preds, average=\"macro\"\n",
    "            ),\n",
    "            \"Recall\": skl_recall(\n",
    "                y_true=flat_truth, y_pred=flat_preds, average=\"macro\"\n",
    "                ),\n",
    "            \"F1\": skl_f1(\n",
    "                y_true=flat_truth, y_pred=flat_preds, average=\"macro\"\n",
    "                ),\n",
    "            \"classification\": cleaned_report,\n",
    "            \"output\": {\"y_true\": flat_truth, \"y_pred\": flat_preds},\n",
    "        }\n",
    "\n",
    "    def clean_report(self, report):\n",
    "        report = report.copy()\n",
    "        mask = report[\"Tag\"] == \"accuracy\"\n",
    "        accuracy_row = report[mask]\n",
    "        if not accuracy_row.empty:\n",
    "            # Get the accuracy value\n",
    "            accuracy_value = accuracy_row[\"Precision\"].values[\n",
    "                0\n",
    "            ]  # Assuming accuracy is stored in the 'Precision' column\n",
    "            accuracy_support = accuracy_row[\"Recall\"].values[\n",
    "                0\n",
    "            ]  # Assuming accuracy is stored in the 'Precision' column\n",
    "\n",
    "            # Set the precision, recall, and F1-score to the accuracy value\n",
    "            report.loc[mask, \"Precision\"] = accuracy_value\n",
    "            report.loc[mask, \"Recall\"] = accuracy_value\n",
    "            report.loc[mask, \"F1\"] = accuracy_value\n",
    "            report.loc[mask, \"Support\"] = accuracy_support\n",
    "\n",
    "            # Rename the tag from 'accuracy' to 'accuracy/micro' for clarity\n",
    "            report.loc[report[\"Tag\"] == \"accuracy\", \"Tag\"] = \"accuracy/micro\"\n",
    "        return report\n",
    "    \n",
    "class EntityEvaluationStrategy(EvaluationStrategy):\n",
    "    def compute_metrics(self, true_labels, predictions, entity_config):\n",
    "        scheme = entity_config.get('scheme')  # Default to 'none' if not specified\n",
    "\n",
    "        # Check if the scheme is valid and not 'none'\n",
    "        try:\n",
    "            truth_list, pred_list = self.align_predictions(predictions, true_labels)\n",
    "        except:\n",
    "            logging.info('The labels already aligned, proceed with evaluation')\n",
    "            truth_list, pred_list = true_labels, predictions\n",
    "            \n",
    "        strict_outputs = self._evaluate_strict(truth_list, pred_list, scheme)\n",
    "        non_strict_outputs = self._evaluate_non_strict(truth_list, pred_list)\n",
    "        \n",
    "        return {\n",
    "            \"strict\": strict_outputs,\n",
    "            \"non_strict\": non_strict_outputs,\n",
    "            \"output\": {\"y_true\": truth_list, \"y_pred\": pred_list}\n",
    "        }\n",
    "        \n",
    "        \n",
    "    \n",
    "    def _evaluate_strict(self, truth_list, pred_list, scheme):\n",
    "        \n",
    "        if scheme is not None and scheme in VALID_SCHEMES:\n",
    "            scheme_class = VALID_SCHEMES[scheme]\n",
    "            report = seq_classification(\n",
    "                    y_true=truth_list,\n",
    "                    y_pred=pred_list,\n",
    "                    digits=4,\n",
    "                    mode='strict',\n",
    "                    scheme=scheme_class,\n",
    "                )\n",
    "            precision = seq_precision(\n",
    "                    y_true=truth_list, y_pred=pred_list, average=\"micro\", mode='strict', scheme = scheme_class\n",
    "                )\n",
    "            recall = seq_recall(\n",
    "                    y_true=truth_list, y_pred=pred_list, average=\"micro\", mode='strict', scheme = scheme_class\n",
    "                )\n",
    "            f1 = seq_f1(\n",
    "                    y_true=truth_list, y_pred=pred_list, average=\"micro\", mode='strict', scheme = scheme_class\n",
    "                )\n",
    "            \n",
    "        else:\n",
    "            logging.info(\"The scheme is unspecified; seqeval will auto-detect the scheme.\")\n",
    "            report = seq_classification(\n",
    "                    y_true=truth_list,\n",
    "                    y_pred=pred_list,\n",
    "                    digits=4,\n",
    "                    mode='strict',\n",
    "                )\n",
    "            scheme_class = auto_detect(pred_list, False)\n",
    "            precision = seq_precision(\n",
    "                    y_true=truth_list, y_pred=pred_list, average=\"micro\", mode='strict', scheme = scheme_class\n",
    "                )\n",
    "            recall = seq_recall(\n",
    "                    y_true=truth_list, y_pred=pred_list, average=\"micro\", mode='strict', scheme = scheme_class\n",
    "                )\n",
    "            f1 = seq_f1(\n",
    "                    y_true=truth_list, y_pred=pred_list, average=\"micro\", mode='strict', scheme = scheme_class\n",
    "                )\n",
    "        \n",
    "        return {\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1\": f1,\n",
    "            \"classification\": self.create_classification_report(report),\n",
    "                \n",
    "            }\n",
    "    \n",
    "        \n",
    "    def _evaluate_non_strict(self, truth_list, pred_list,):\n",
    "        \n",
    "        report = seq_classification(\n",
    "                    y_true=truth_list,\n",
    "                    y_pred=pred_list,\n",
    "                    digits=4,\n",
    "                )\n",
    "        precision = seq_precision(\n",
    "                y_true=truth_list, y_pred=pred_list, average=\"micro\"\n",
    "            )\n",
    "        recall = seq_recall(\n",
    "                    y_true=truth_list, y_pred=pred_list, average=\"micro\"\n",
    "                )\n",
    "        f1 = seq_f1(\n",
    "                y_true=truth_list, y_pred=pred_list, average=\"micro\"\n",
    "            )\n",
    "        return {\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1\": f1,\n",
    "            \"classification\": self.create_classification_report(report),\n",
    "                \n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "class Evaluation:\n",
    "    def __init__(self, inv_map, y_true, y_pred, evaluation_config):\n",
    "        self.truths = y_true\n",
    "        self.predictions = y_pred\n",
    "        self.evaluation_config = evaluation_config\n",
    "        self.token_strategy = TokenEvaluationStrategy(inv_map)\n",
    "        self.entity_strategy = EntityEvaluationStrategy(inv_map)\n",
    "\n",
    "    \n",
    "\n",
    "    def evaluate(self):\n",
    "        token_metrics = self.token_strategy.compute_metrics(\n",
    "            self.truths, self.predictions\n",
    "        )\n",
    "        entity_metrics = self.entity_strategy.compute_metrics(\n",
    "            self.truths, self.predictions, self.evaluation_config\n",
    "        )\n",
    "\n",
    "        # Combine or store results as needed\n",
    "        return {\"Token_Level\": token_metrics, \"Entity_Level\": entity_metrics}\n",
    "\n",
    "    def _prepare_results(self, metrics):\n",
    "        results = pd.DataFrame.from_dict(self._round_and_slice(metrics))\n",
    "        report = metrics[\"classification\"]\n",
    "        output = metrics[\"output\"]\n",
    "        return results, report, output\n",
    "    \n",
    "    def _prepare_entity_results(self, metrics):\n",
    "        strict = metrics[\"strict\"]\n",
    "        non_strict = metrics[\"non_strict\"]\n",
    "        entity_strict_results = pd.DataFrame.from_dict(self._round_and_slice(strict))\n",
    "        entity_non_strict_results = pd.DataFrame.from_dict(self._round_and_slice(non_strict))\n",
    "        entity_strict_report = strict['classification']\n",
    "        entity_non_strict_report = non_strict['classification']\n",
    "        output = metrics[\"output\"]\n",
    "        return {\n",
    "            'entity_strict_results': entity_strict_results,\n",
    "            'entity_non_strict_results': entity_non_strict_results,\n",
    "            'entity_strict_report': entity_strict_report,\n",
    "            'entity_non_strict_report': entity_non_strict_report,\n",
    "            'output': output,\n",
    "        }\n",
    "\n",
    "    def _round_and_slice(self, dictionary):\n",
    "        # Slicing and rounding results for cleaner presentation\n",
    "        \n",
    "        keys_for_slicing = [\"Precision\", \"Recall\", \"F1\"]\n",
    "        sliced_dict = {key: [round(dictionary[key], 4)] for key in keys_for_slicing}\n",
    "        return sliced_dict\n",
    "    \n",
    "    def generate_results(self):\n",
    "        metrics = self.evaluate()\n",
    "        token_results, token_report, token_outputs = self._prepare_results(\n",
    "            metrics[\"Token_Level\"]\n",
    "        )\n",
    "        entity_level_outputs = self._prepare_entity_results(\n",
    "            metrics[\"Entity_Level\"]\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "        return {\n",
    "            \"token_results\": token_results,\n",
    "            \"token_report\": token_report,\n",
    "            \"token_outputs\": token_outputs,\n",
    "            \"entity_strict_results\": entity_level_outputs['entity_strict_results'],\n",
    "            \"entity_non_strict_results\": entity_level_outputs['entity_non_strict_results'],\n",
    "            \"entity_strict_report\": entity_level_outputs['entity_strict_report'],\n",
    "            \"entity_non_strict_report\": entity_level_outputs['entity_non_strict_report'],\n",
    "            \"entity_outputs\": entity_level_outputs['output'],\n",
    "        }\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Metrics:\n",
    "    token_results: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    token_report: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    token_outputs: dict = field(default_factory=dict)\n",
    "    entity_strict_results: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    entity_non_strict_results: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    entity_strict_report: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    entity_non_strict_report: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    entity_outputs: dict = field(default_factory=dict)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_dict(data: dict):\n",
    "        \"\"\"Create an instance from a dictionary.\"\"\"\n",
    "        required_keys = [\n",
    "        \"token_results\", \"token_report\", \"token_outputs\",\n",
    "        \"entity_strict_results\", \"entity_non_strict_results\",\n",
    "        \"entity_strict_report\", \"entity_non_strict_report\", \"entity_outputs\"\n",
    "        ]\n",
    "        missing_keys = [key for key in required_keys if key not in data]\n",
    "        if missing_keys:\n",
    "            raise ValueError(f\"Missing required keys in data: {missing_keys}\")\n",
    "        return Metrics(**data)\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"token_results\": self.token_results.to_dict(orient=\"records\"),\n",
    "            \"token_report\": self.token_report.to_dict(orient=\"records\"),\n",
    "            \"token_outputs\": self.token_outputs,\n",
    "            \"entity_strict_results\": self.entity_strict_results.to_dict(orient=\"records\"),     \n",
    "            \"entity_non_strict_results\": self.entity_non_strict_results.to_dict(orient=\"records\"),\n",
    "            \"entity_strict_report\": self.entity_strict_report.to_dict(orient=\"records\"), \n",
    "            \"entity_non_strict_report\": self.entity_non_strict_report.to_dict(orient=\"records\"), \n",
    "            \"entity_outputs\": self.entity_outputs,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = entity_outputs['entity_outputs']['y_true']\n",
    "y_pred = entity_outputs['entity_outputs']['y_pred']\n",
    "corpus = corpora['ANERCorp_CamelLab']\n",
    "labels_map = corpus[\"labels_map\"]\n",
    "inv_labels_map = {v: k for k, v in labels_map.items()}\n",
    "evaluation_config = {\n",
    "\t'scheme': 'IOB2'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluation(\n",
    "                inv_labels_map, y_true, y_pred, evaluation_config\n",
    "            )\n",
    "results = evaluator.generate_results()\n",
    "metrics = Metrics.from_dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.entity_strict_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = evaluator.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outs['Entity_Level']['strict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, classification_report, f1_score\n",
    "from seqeval.scheme import IOB2\n",
    "\n",
    "print(classification_report(y_true, y_pred, mode='strict', digits=4))\n",
    "\n",
    "f1_score(y_true, y_pred, mode='strict', scheme=IOB2,  average='micro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred, mode=None, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
