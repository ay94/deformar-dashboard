{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "from seqeval.scheme import auto_detect\n",
    "from seqeval.metrics.sequence_labeling import get_entities\n",
    "from seqeval.scheme import Entities\n",
    "\n",
    "file_name = '/Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/BaseLineExperiment/ANERCorp_CamelLab_arabertv02/fine_tuning/evaluation_metrics.json'\n",
    "with open(file_name, 'r') as file:\n",
    "    entity_outputs = json.load(file)  # Use json.load() to read file, not json.loads()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = entity_outputs['entity_outputs']['y_true']\n",
    "y_pred = entity_outputs['entity_outputs']['y_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class StrictConfusionMatrix:\n",
    "    def __init__(self, y_true, y_pred):\n",
    "        self.y_true = y_true\n",
    "        self.y_pred = y_pred\n",
    "    \n",
    "    def compute(self):\n",
    "        \"\"\"\n",
    "        Compute confusion matrix, false positives, and false negatives for all entities.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing:\n",
    "                - 'confusion_matrix': The confusion matrix for all entity types.\n",
    "                - 'false_negatives': A dictionary with false negative counts categorized by type and subcategory.\n",
    "                - 'false_positives': A dictionary with false positive counts categorized by type and subcategory.\n",
    "        \"\"\"\n",
    "        # Prepare entities (this initializes and formats entities based on the input scheme)\n",
    "        self.prepare_entities()\n",
    "\n",
    "        # Compute the confusion matrix for all entities\n",
    "        confusion_matrix = self.compute_confusion_matrix()\n",
    "\n",
    "        # Initialize dictionaries for false negatives and false positives\n",
    "        false_negatives = defaultdict(Counter)\n",
    "        false_positives = defaultdict(Counter)\n",
    "\n",
    "        # Get all unique entity types from the data\n",
    "        entity_types = set(\n",
    "            ent[1] for ent in self.true_entities\n",
    "        ).union(set(ent[1] for ent in self.pred_entities))\n",
    "\n",
    "        # Iterate over all entity types to calculate false negatives and positives\n",
    "        for entity_type in entity_types:\n",
    "            # Compute false negatives for this type\n",
    "            fn_counts = self.compute_false_negatives(entity_type)\n",
    "\n",
    "            # Compute false positives for this type\n",
    "            fp_counts = self.compute_false_positives(entity_type)\n",
    "\n",
    "            # Merge the results into the global dictionaries\n",
    "            for t_type, counts in fn_counts.items():\n",
    "                for subtype, count in counts.items():\n",
    "                    false_negatives[t_type][subtype] += count\n",
    "\n",
    "            for t_type, counts in fp_counts.items():\n",
    "                for subtype, count in counts.items():\n",
    "                    false_positives[t_type][subtype] += count\n",
    "\n",
    "        # Return the aggregated results\n",
    "        return {\n",
    "            'confusion_matrix': confusion_matrix,\n",
    "            'false_negatives': dict(false_negatives),  # Convert to standard dict for output clarity\n",
    "            'false_positives': dict(false_positives),  # Convert to standard dict for output clarity\n",
    "        }\n",
    "\n",
    "    \n",
    "    \n",
    "    def prepare_entities(self):\n",
    "        # Initialize true and predicted entities\n",
    "        self.scheme = auto_detect(self.y_true, False)\n",
    "        entities_true = self.extract_entities(self.y_true)\n",
    "        entities_pred = self.extract_entities(self.y_pred)\n",
    "        self.true_entities = self.flatten_strict_entities(entities_true)\n",
    "        self.pred_entities = self.flatten_strict_entities(entities_pred)\n",
    "\n",
    "    def extract_entities(self, y_data):\n",
    "        # Replace with the Entities() logic if provided\n",
    "        return Entities(y_data, self.scheme, False)\n",
    "\n",
    "    @staticmethod\n",
    "    def flatten_strict_entities(entities):\n",
    "        \"\"\"Flatten entities extracted in strict mode into tuples.\"\"\"\n",
    "        return [e.to_tuple() for sen in entities.entities for e in sen]\n",
    "\n",
    "    def compute_confusion_matrix(self):\n",
    "        \"\"\"Compute confusion matrix across all entity types.\"\"\"\n",
    "        types = set([ent[1] for ent in self.true_entities]).union(\n",
    "            [ent[1] for ent in self.pred_entities]\n",
    "        )\n",
    "\n",
    "        confusion_matrix = {typ: {'TP': 0, 'FP': 0, 'FN': 0} for typ in types}\n",
    "\n",
    "        for entity_type in types:\n",
    "            TP, FP, FN = self.extract_strict_entity_confusion(entity_type)\n",
    "            confusion_matrix[entity_type]['TP'] = TP\n",
    "            confusion_matrix[entity_type]['FP'] = FP\n",
    "            confusion_matrix[entity_type]['FN'] = FN\n",
    "\n",
    "        return confusion_matrix\n",
    "\n",
    "    def extract_strict_entity_confusion(self, entity):\n",
    "        \"\"\"Extract TP, FP, and FN for a given entity type.\"\"\"\n",
    "        fns = set([e for e in self.true_entities if e[1] == entity]) - set(\n",
    "            [e for e in self.pred_entities if e[1] == entity]\n",
    "        )\n",
    "        fps = set([e for e in self.pred_entities if e[1] == entity]) - set(\n",
    "            [e for e in self.true_entities if e[1] == entity]\n",
    "        )\n",
    "        tps = set([e for e in self.pred_entities if e[1] == entity]).intersection(\n",
    "            set([e for e in self.true_entities if e[1] == entity])\n",
    "        )\n",
    "        return len(tps), len(fps), len(fns)\n",
    "\n",
    "    def compute_false_positives(self, entity_type):\n",
    "        \"\"\"Analyze false positives for a specific entity type.\"\"\"\n",
    "        false_positives = set(\n",
    "            [e for e in self.pred_entities if e[1] == entity_type]\n",
    "        ) - set([e for e in self.true_entities if e[1] == entity_type])\n",
    "\n",
    "        return self.analyze_errors(false_positives, self.true_entities, \"FP\")\n",
    "\n",
    "    def compute_false_negatives(self, entity_type):\n",
    "        \"\"\"Analyze false negatives for a specific entity type.\"\"\"\n",
    "        false_negatives = set(\n",
    "            [e for e in self.true_entities if e[1] == entity_type]\n",
    "        ) - set([e for e in self.pred_entities if e[1] == entity_type])\n",
    "\n",
    "        return self.analyze_errors(false_negatives, self.pred_entities, \"FN\")\n",
    "\n",
    "    def analyze_errors(self, target_entities, comparison_entities, error_type):\n",
    "        \"\"\"Analyze entity-level errors (FP or FN).\"\"\"\n",
    "        counts = defaultdict(Counter)\n",
    "        non_o_errors = set()\n",
    "        indexed_entities = defaultdict(list)\n",
    "\n",
    "        # Index comparison entities by sentence\n",
    "        for entity in comparison_entities:\n",
    "            sen, entity_type, start, end = entity\n",
    "            indexed_entities[sen].append(entity)\n",
    "        \n",
    "        # Track processed pairs to avoid duplicates in counting\n",
    "        processed_pairs = set()\n",
    "        # First pass: entity errors\n",
    "        for target_entity in target_entities:\n",
    "            t_sen, t_type, t_start, t_end = target_entity\n",
    "\n",
    "            for comp_entity in indexed_entities[t_sen]:\n",
    "                c_type, c_start, c_end = comp_entity[1:]\n",
    "\n",
    "                # Check for entity type mismatch with exact boundary match\n",
    "                if (\n",
    "                t_start == c_start\n",
    "                and t_end == c_end\n",
    "                and t_type != c_type\n",
    "                and target_entity not in non_o_errors\n",
    "            ):\n",
    "                    counts[t_type][c_type] += 1\n",
    "                    non_o_errors.add(target_entity)\n",
    "\n",
    "        # Second pass: boundary errors\n",
    "        for target_entity in target_entities - non_o_errors:\n",
    "            t_sen, t_type, t_start, t_end = target_entity\n",
    "\n",
    "            for comp_entity in indexed_entities[t_sen]:\n",
    "                c_type, c_start, c_end = comp_entity[1:]\n",
    "\n",
    "                # Check for boundary issues with the same entity type\n",
    "                if (\n",
    "                t_type == c_type\n",
    "                and (t_start <= c_start <= t_end or t_start <= c_end <= t_end)\n",
    "                and target_entity not in non_o_errors\n",
    "            ):\n",
    "                    counts[t_type]['Boundary'] += 1\n",
    "                    non_o_errors.add(target_entity)\n",
    "\n",
    "        # Third pass: combined entity and boundary errors\n",
    "        for target_entity in target_entities - non_o_errors:\n",
    "            t_sen, t_type, t_start, t_end = target_entity\n",
    "\n",
    "            for comp_entity in indexed_entities[t_sen]:\n",
    "                c_type, c_start, c_end = comp_entity[1:]\n",
    "\n",
    "                # Check for combined entity and boundary issues with different types\n",
    "                if (\n",
    "                c_type != t_type\n",
    "                and (t_start <= c_start <= t_end or t_start <= c_end <= t_end)\n",
    "                and target_entity not in non_o_errors\n",
    "            ):\n",
    "                    counts[t_type]['Entity and Boundary'] += 1\n",
    "                    non_o_errors.add(target_entity)\n",
    "\n",
    "        # Remaining errors are \"O\" errors (completely unmatched)\n",
    "        for target_entity in target_entities - non_o_errors:\n",
    "            t_sen, t_type, t_start, t_end = target_entity\n",
    "            counts[t_type]['O'] += 1\n",
    "\n",
    "        return dict(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strict_confusion = StrictConfusionMatrix(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strict_confusion.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strict_confusion.compute_false_negatives('ORG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORG': Counter({'O': 49, 'Boundary': 27, 'LOC': 19, 'PERS': 11, 'MISC': 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_confusion_matrix(true_entities, pred_entities):\n",
    "        # Extract all unique entity types from true and predicted entities\n",
    "        types = set([ent[1] for ent in true_entities]).union([ent[1] for ent in pred_entities])\n",
    "\n",
    "        # Initialize the confusion matrix\n",
    "        confusion_matrix = {typ: {'TP': 0, 'FP': 0, 'FN': 0} for typ in types}\n",
    "        \n",
    "        def extract_strict_entity_confusion(entity, true_entities, pred_entities):\n",
    "            fns = set([e for e in true_entities if e[1] == entity]) - set([e for e in pred_entities if e[1] == entity])\n",
    "            fps = set([e for e in pred_entities if e[1] == entity]) - set([e for e in true_entities if e[1] == entity])\n",
    "            tps = set([e for e in pred_entities if e[1] == entity]).intersection(set([e for e in true_entities if e[1] == entity]))\n",
    "            return len(tps), len(fps), len(fns)\n",
    "\n",
    "        # Populate the confusion matrix for each entity type\n",
    "        for entity_type in types:\n",
    "            TP, FP, FN = extract_strict_entity_confusion(entity_type, true_entities, pred_entities)\n",
    "            confusion_matrix[entity_type]['TP'] = TP\n",
    "            confusion_matrix[entity_type]['FP'] = FP\n",
    "            confusion_matrix[entity_type]['FN'] = FN\n",
    "\n",
    "        return confusion_matrix\n",
    "\n",
    "\n",
    "def flatten_strict_entities(entities):\n",
    "        \"\"\"\n",
    "        Flatten entities extracted in strict mode into tuples.\n",
    "\n",
    "        Args:\n",
    "            entities (Entities): The strict-mode entities.\n",
    "\n",
    "        Returns:\n",
    "            list: A flattened list of tuples representing the entities.\n",
    "        \"\"\"\n",
    "        return [e.to_tuple() for sen in entities.entities for e in sen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scheme = auto_detect(y_true, False)\n",
    "\n",
    "entities_true = Entities(y_true, scheme, False)\n",
    "entities_pred = Entities(y_pred, scheme, False)\n",
    "true_entity_type = flatten_strict_entities(entities_true)\n",
    "pred_entity_type = flatten_strict_entities(entities_pred)\n",
    "# Example usage\n",
    "# conf_matrix = calculate_confusion_matrix([e.to_tuple()[1:] for sen in entities_true.entities for e in sen], [e.to_tuple()[1:] for sen in entities_pred.entities for e in sen])\n",
    "\n",
    "\n",
    "\n",
    "confusion_matrix = compute_confusion_matrix(true_entity_type, pred_entity_type)\n",
    "fn_errors = compute_false_negatives_with_boundary(true_entity_type, pred_entity_type, confusion_matrix)\n",
    "fp_errors = compute_false_positives_with_boundary(true_entity_type, pred_entity_type, confusion_matrix)\n",
    "\n",
    "\n",
    "print(\"Confusion Matrix:\", confusion_matrix)\n",
    "print(\"False Negatives:\", dict(fn_errors))\n",
    "print(\"False Positives:\", dict(fp_errors))\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "24+17+13+8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e, m in confusion_matrix.items():\n",
    "    print(e, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_entity_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ORG', sum({'O': 50, 'Boundary': 27, 'LOC': 19, 'PERS': 11, 'MISC': 6}.values()))\n",
    "print('MISC', sum({'O': 58, 'Boundary': 15, 'ORG': 9, 'LOC': 4}.values()))\n",
    "print('PERS', sum({'Boundary': 70, 'O': 44, 'ORG': 11, 'LOC': 6, 'MISC': 3}.values()))\n",
    "print('LOC', sum({'O': 26, 'Boundary': 12, 'MISC': 3, 'ORG': 3, 'PERS': 1}.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_id = 44\n",
    "for i, (t, p) in enumerate(zip(y_true[sen_id], y_pred[sen_id])):\n",
    "    if i == 34:\n",
    "        print('from here')\n",
    "    print(i, t, p)\n",
    "\n",
    "print(entities_true.entities[sen_id])\n",
    "print(entities_pred.entities[sen_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have just found that ANERCorp doesn't even align completely with IOB2 see example 167 there was I-MISC: 163, 167, 504, 623, 694"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITY = 'ORG'\n",
    "false_positives = set([e for e in pred_entity_type if e[1] == ENTITY]) - set([e for e in true_entity_type if e[1] == ENTITY])\n",
    "\n",
    "# Initial sets for storing different types of errors\n",
    "entity_errors = set()\n",
    "boundary_errors = set()\n",
    "entity_and_boundary_errors = set()\n",
    "non_o_errors = set()\n",
    "# Using defaultdicts to handle grouped by sentence and type categorization more effectively\n",
    "true_entities_indexed = defaultdict(list)\n",
    "\n",
    "# Index true entities by sentence for efficient comparison\n",
    "for entity in true_entity_type:\n",
    "    true_sen, true_type, true_start, true_end = entity\n",
    "    true_entities_indexed[true_sen].append(entity)\n",
    "\n",
    "# First pass to identify entity type errors\n",
    "for predicted_entity in false_positives:\n",
    "    pred_sen, pred_type, pred_start, pred_end = predicted_entity\n",
    "\n",
    "    # Only compare within the same sentence for efficiency\n",
    "    for true_entity in true_entities_indexed[pred_sen]:\n",
    "        true_type, true_start, true_end = true_entity[1:]\n",
    "\n",
    "        # Check for entity type mismatch with exact boundary match\n",
    "        if true_start == pred_start and true_end == pred_end and true_type != pred_type:\n",
    "            entity_errors.add(predicted_entity)\n",
    "            non_o_errors.add(predicted_entity)\n",
    "\n",
    "# Second pass to identify boundary and combined errors\n",
    "for predicted_entity in false_positives - non_o_errors:\n",
    "    pred_sen, pred_type, pred_start, pred_end = predicted_entity\n",
    "\n",
    "    if predicted_entity not in entity_errors:  # Skip already identified type errors\n",
    "        for true_entity in true_entities_indexed[pred_sen]:\n",
    "            true_type, true_start, true_end = true_entity[1:]\n",
    "\n",
    "            # Check for boundary issues with the same entity type\n",
    "            if true_type == pred_type and (pred_start <= true_start <= pred_end or pred_start <= true_end <= pred_end):\n",
    "                # boundary_errors1.add((true_entity, predicted_entity))\n",
    "                boundary_errors.add( predicted_entity)\n",
    "                non_o_errors.add(predicted_entity)\n",
    "                \n",
    "for predicted_entity in false_positives - non_o_errors:\n",
    "    pred_sen, pred_type, pred_start, pred_end = predicted_entity\n",
    "\n",
    "    if predicted_entity not in entity_errors:  # Skip already identified type errors\n",
    "        for true_entity in true_entities_indexed[pred_sen]:\n",
    "            true_type, true_start, true_end = true_entity[1:]\n",
    "\n",
    "            # Entity and boundary errors for different entity types\n",
    "            if (true_type != pred_type) and pred_start <= true_start <= pred_end or pred_start <= true_end <= pred_end:\n",
    "                entity_and_boundary_errors.add(predicted_entity)\n",
    "                non_o_errors.add(predicted_entity)\n",
    "o_error = set(false_positives) - non_o_errors\n",
    "\n",
    "# Debugging prints to check set contents\n",
    "print(f\"Entity Errors: {len(entity_errors)}\")\n",
    "print(f\"Boundary Errors: {len(boundary_errors)}\")\n",
    "print(f\"Entity and Boundary Errors: {len(entity_and_boundary_errors)}\")\n",
    "print(f\"Outside Named Entity: {len(o_error)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITY = 'LOC'\n",
    "false_negatives = set([e for e in true_entity_type if e[1] == ENTITY]) - set([e for e in pred_entity_type if e[1] == ENTITY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Initial sets for storing different types of errors\n",
    "entity_errors = set()\n",
    "boundary_errors = set()\n",
    "entity_and_boundary_errors = set()\n",
    "non_o_errors = set()\n",
    "# Using defaultdicts to handle grouped by sentence and type categorization more effectively\n",
    "pred_entities_indexed = defaultdict(list)\n",
    "\n",
    "# Index true entities by sentence for efficient comparison\n",
    "for entity in pred_entity_type:\n",
    "    pred_sen, pred_type, pred_start, pred_end = entity\n",
    "    pred_entities_indexed[pred_sen].append(entity)\n",
    "\n",
    "# First pass to identify entity type errors\n",
    "for true_entity in false_negatives:\n",
    "    true_sen, true_type, true_start, true_end = true_entity\n",
    "\n",
    "    # Only compare within the same sentence for efficiency\n",
    "    for pred_entity in pred_entities_indexed[true_sen]:\n",
    "        pred_type, pred_start, pred_end = pred_entity[1:]\n",
    "\n",
    "        # Check for entity type mismatch with exact boundary match\n",
    "        if pred_start == true_start and pred_end == true_end and pred_type != true_type:\n",
    "            entity_errors.add(true_entity)\n",
    "            non_o_errors.add(true_entity)\n",
    "\n",
    "# Second pass to identify boundary and combined errors\n",
    "for true_entity in false_negatives - non_o_errors:\n",
    "    true_sen, true_type, true_start, true_end = true_entity\n",
    "\n",
    "    # if true_entity not in entity_errors:  # Skip already identified type errors\n",
    "    for pred_entity in pred_entities_indexed[true_sen]:\n",
    "        pred_type, pred_start, pred_end = pred_entity[1:]\n",
    "\n",
    "        # Check for boundary issues with the same entity type\n",
    "        if pred_type == true_type and (true_start <= pred_start <= true_end or true_start <= pred_end <= true_end):\n",
    "            boundary_errors.add(true_entity)\n",
    "            non_o_errors.add(true_entity)\n",
    "            \n",
    "for true_entity in false_negatives - non_o_errors:\n",
    "    true_sen, true_type, true_start, true_end = true_entity\n",
    "\n",
    "    \n",
    "    for pred_entity in pred_entities_indexed[true_sen]:\n",
    "        pred_type, pred_start, pred_end = pred_entity[1:]\n",
    "\n",
    "        # Entity and boundary errors for different entity types\n",
    "        if (pred_type != true_type) and true_start <= pred_start <= true_end or true_start <= pred_end <= true_end:\n",
    "            entity_and_boundary_errors.add(true_entity)\n",
    "            non_o_errors.add(true_entity)\n",
    "o_error = set(false_negatives) - non_o_errors\n",
    "\n",
    "# Debugging prints to check set contents\n",
    "print(f\"Entity Errors: {len(entity_errors)}\")\n",
    "print(f\"Boundary Errors: {len(boundary_errors)}\")\n",
    "print(f\"Entity and Boundary Errors: {len(entity_and_boundary_errors)}\")\n",
    "print(f\"Outside Named Entity: {len(o_error)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_id = 232\n",
    "for i, (t, p) in enumerate(zip(y_true[sen_id], y_pred[sen_id])):\n",
    "    if i == 34:\n",
    "        print('from here')\n",
    "    print(i, t, p)\n",
    "\n",
    "print(entities_true.entities[sen_id])\n",
    "print(entities_pred.entities[sen_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "29+14+13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "19+29+21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(entity_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example manual check\n",
    "test_entity = (708, 'LOC', 15, 16)\n",
    "if test_entity in entity_errors:\n",
    "    print(\"Found in errors\")\n",
    "else:\n",
    "    print(\"Not found in errors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(708, 'LOC', 15, 16) in x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(708, 'LOC', 15, 16)   in entity_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(false_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(entity_errors) + len(boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = 'LOC' \n",
    "false_negatives = set(e for e in true_entity_type if e[1] == entity) -  set([e for e in pred_entity_type if e[1] == entity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_errors = []\n",
    "missing_errors = set()\n",
    "boundary = set()\n",
    "for entity in false_negatives:\n",
    "    o = []\n",
    "    sen, t, s, e = entity\n",
    "    \n",
    "    for p_entity in pred_entity_type:\n",
    "        \n",
    "        p_sen, p_t, p_s, p_e = p_entity\n",
    "        if p_sen == sen and p_t == t and not (p_s == s and p_e == e):\n",
    "            if (s <= p_s <= e) or (s <= p_e <= e):\n",
    "                print(entity)\n",
    "                print(p_entity)\n",
    "                boundary.add((sen, s, e))\n",
    "                print('end ####')\n",
    "                \n",
    "        elif p_sen == sen and p_s == s and p_e == e:\n",
    "            entity_errors.append((sen, s, e))\n",
    "    if (sen, s, e) not in boundary and  (sen, s, e) not in entity_errors:\n",
    "            missing_errors.add((sen, s, e))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(missing_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(boundary))\n",
    "print(len(entity_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_id = 94\n",
    "for i, (t, p) in enumerate(zip(y_true[sen_id], y_pred[sen_id])):\n",
    "    if i == 34:\n",
    "        print('from here')\n",
    "    print(i, t, p)\n",
    "\n",
    "print(entities_true.entities[sen_id])\n",
    "print(entities_pred.entities[sen_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for error in missing_errors:\n",
    "\tif error[0] in [error[0] for error in boundary]:\n",
    "\t\tprint(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "class EntityConfusion:\n",
    "    def __init__(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Initialize the EntityConfusionMatrix class.\n",
    "\n",
    "        Args:\n",
    "            y_true (list): The ground truth entities.\n",
    "            y_pred (list): The predicted entities.\n",
    "        \"\"\"\n",
    "        self.y_true = y_true\n",
    "        self.y_pred = y_pred\n",
    "        \n",
    "\n",
    "    def prepare_entities(self):\n",
    "        \"\"\"\n",
    "        Prepare entities for confusion matrix calculation.\n",
    "        - In non-strict mode, entities are extracted using `get_entities`.\n",
    "        \"\"\"    \n",
    "        # Use non-strict extraction\n",
    "        self.true_entities = get_entities(self.y_true)\n",
    "        self.pred_entities = get_entities(self.y_pred)\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_entity_confusion(entity, true_entities, pred_entities):\n",
    "        fns = set([e for e in true_entities if e[0] == entity]) - set([e for e in pred_entities if e[0] == entity])\n",
    "        fps = set([e for e in pred_entities if e[0] == entity]) - set([e for e in true_entities if e[0] == entity])\n",
    "        tps = set([e for e in pred_entities if e[0] == entity]).intersection(set([e for e in true_entities if e[0] == entity]))\n",
    "        return len(tps), len(fps), len(fns)\n",
    "\n",
    "\n",
    "    def compute(self):\n",
    "        \"\"\"\n",
    "        Compute the confusion matrix, false negatives, and false positives.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing:\n",
    "                  - 'confusion_matrix': The confusion matrix for entity recognition.\n",
    "                  - 'false_negatives': Detailed false negatives.\n",
    "                  - 'false_positives': Detailed false positives.\n",
    "        \"\"\"\n",
    "        self.prepare_entities()\n",
    "        return {\n",
    "            'confusion_matrix': self.compute_confusion_matrix(),\n",
    "            'false_negatives': self.compute_false_negatives_with_boundary(),\n",
    "            'false_positives': self.compute_false_positives_with_boundary()\n",
    "        }\n",
    "    \n",
    "\n",
    "    def compute_confusion_matrix(self):\n",
    "        \"\"\"\n",
    "        Compute a confusion matrix for Named Entity Recognition (NER) predictions.\n",
    "\n",
    "        Returns:\n",
    "            dict: A confusion matrix structured as:\n",
    "                  {entity_type: {'TP': count, 'FP': count, 'FN': count}}\n",
    "        \"\"\"\n",
    "        # Extract all unique entity types from true and predicted entities\n",
    "        types = set([ent[0] for ent in self.true_entities]).union([ent[0] for ent in self.pred_entities])\n",
    "\n",
    "        # Initialize the confusion matrix\n",
    "        confusion_matrix = {typ: {'TP': 0, 'FP': 0, 'FN': 0} for typ in types}\n",
    "\n",
    "        \n",
    "        # Populate the confusion matrix for each entity type\n",
    "        for entity_type in types:\n",
    "            TP, FP, FN = self.extract_entity_confusion(entity_type, self.true_entities, self.pred_entities)\n",
    "            confusion_matrix[entity_type]['TP'] = TP\n",
    "            confusion_matrix[entity_type]['FP'] = FP\n",
    "            confusion_matrix[entity_type]['FN'] = FN\n",
    "\n",
    "        return confusion_matrix\n",
    "    \n",
    "\n",
    "    def compute_false_negatives_with_boundary(self):\n",
    "        \"\"\"\n",
    "        Compute false negatives with detailed categorization:\n",
    "        - 'Boundary': Incorrect boundaries for the same entity type.\n",
    "        - 'Missed': Predicted as O or no match at all.\n",
    "\n",
    "        Returns:\n",
    "            dict: False negatives categorized by entity type.\n",
    "        \"\"\"\n",
    "        fn_counts = defaultdict(Counter)\n",
    "        true_indexed = {(t[1], t[2]): t[0] for t in self.true_entities}  # Index true entities by boundaries\n",
    "        pred_indexed = {(p[1], p[2]): p[0] for p in self.pred_entities}  # Index predicted entities by boundaries\n",
    "        # Iterate through true entities to classify false negatives\n",
    "        for (t_start, t_end), t_type in true_indexed.items():\n",
    "            if (t_start, t_end) in pred_indexed:\n",
    "                if pred_indexed[(t_start, t_end)] != t_type:\n",
    "                    # Type mismatch at the exact position\n",
    "                    matched_type = pred_indexed.get((t_start, t_end))\n",
    "                    fn_counts[t_type][matched_type] += 1\n",
    "            else:\n",
    "                # No exact match found, check for other errors\n",
    "                boundary_error = False\n",
    "                entity_error = False\n",
    "                for (p_start, p_end), p_type in pred_indexed.items():\n",
    "                    if t_type == p_type:\n",
    "                        if (p_start <= t_start <= p_end) or (p_start <= t_end <= p_end):\n",
    "                            # Boundary error for the same type\n",
    "                            fn_counts[t_type]['Boundary'] += 1\n",
    "                            boundary_error = True\n",
    "                            break\n",
    "                    else:\n",
    "                        if (p_start <= t_start <= p_end) or (p_start <= t_end <= p_end):\n",
    "                            # Boundary error with a different entity type\n",
    "                            fn_counts[t_type]['Entity and Boundary'] += 1\n",
    "                            entity_error = True\n",
    "                            break\n",
    "                if not boundary_error and not entity_error:\n",
    "                    # Missed entity entirely\n",
    "                    fn_counts[t_type]['O'] += 1\n",
    "\n",
    "        return dict(fn_counts)\n",
    "        # t = []\n",
    "        # # Iterate through true entities to classify false negatives\n",
    "        # for (t_start, t_end), t_type in true_indexed.items():\n",
    "        #     entity_error = False\n",
    "        #     if (t_start, t_end) not in pred_indexed or pred_indexed[(t_start, t_end)] != t_type:\n",
    "        #         # No matching prediction or type mismatch at the same position\n",
    "        #         matched_type = pred_indexed.get((t_start, t_end))\n",
    "        #         if matched_type:\n",
    "        #             fn_counts[t_type][matched_type] += 1\n",
    "        #             entity_error = True\n",
    "        #         # Check if there's a predicted entity of the same type with incorrect boundaries\n",
    "        #         boundary_error = False\n",
    "        #         entity_boundary_error = False\n",
    "        #         for (p_start, p_end), p_type in pred_indexed.items():\n",
    "        #             if p_type == t_type and not (p_start == t_start and p_end == t_end):\n",
    "        #                 if (t_start <= p_start <= t_end) or (t_start <= p_end <= t_end):\n",
    "        #                     # Detected boundary error for the same entity type\n",
    "        #                     print('boundary')\n",
    "        #                     print(t_start, t_end, t_type)\n",
    "        #                     print(p_start, p_end, p_type)\n",
    "        #                     print('######')\n",
    "        #                     fn_counts[t_type]['Boundary'] += 1\n",
    "        #                     boundary_error = True\n",
    "        #                     break\n",
    "        #             elif t_type != p_type and not (p_start == t_start and p_end == t_end):\n",
    "        #                 if (p_start <= t_start <= p_end) or (p_start <= t_end <= p_end):\n",
    "        #                     # Detected boundary error for the same entity type\n",
    "        #                     fn_counts[p_type]['Entity and Boundary'] += 1\n",
    "        #                     print('entity and boundary')\n",
    "        #                     print(t_start, t_end, t_type)\n",
    "        #                     print(p_start, p_end, p_type)\n",
    "        #                     print('######')\n",
    "        #                     entity_boundary_error = True\n",
    "        #                     break\n",
    "        #         if not boundary_error and not entity_error and not entity_boundary_error:\n",
    "        #             if (p_start == t_start and p_end == t_end):\n",
    "        #                 # Missed entity entirely (e.g., predicted as O)\n",
    "        #                 fn_counts[t_type]['O'] += 1\n",
    "        # return dict(fn_counts)\n",
    "\n",
    "    def compute_false_positives_with_boundary(self):\n",
    "        \"\"\"\n",
    "        Compute false positives with boundary categorization:\n",
    "        - 'Boundary': Incorrect boundaries for the same entity type.\n",
    "        - 'Missed': Predicted as O or no match at all.\n",
    "\n",
    "        Returns:\n",
    "            dict: False positives categorized by entity type.\n",
    "        \"\"\"\n",
    "        fp_counts = defaultdict(Counter)\n",
    "        true_indexed = {(t[1], t[2]): t[0] for t in self.true_entities}  # Index true entities by boundaries\n",
    "        pred_indexed = {(p[1], p[2]): p[0] for p in self.pred_entities}  # Index predicted entities by boundaries\n",
    "        # Iterate through predicted entities to find false positives\n",
    "        for (p_start, p_end), p_type in pred_indexed.items():\n",
    "            entity_error = False\n",
    "            if (p_start, p_end) not in true_indexed or true_indexed[(p_start, p_end)] != p_type:\n",
    "                # No matching true entity or type mismatch at the same position\n",
    "                matched_type = true_indexed.get((p_start, p_end))\n",
    "                if matched_type:\n",
    "                    fp_counts[p_type][matched_type] += 1\n",
    "                    entity_error = True\n",
    "                boundary_error = False\n",
    "                entity_boundary_error = False\n",
    "                for (t_start, t_end), t_type in true_indexed.items():\n",
    "                    if t_type == p_type and not (p_start == t_start and p_end == t_end):\n",
    "                        if (p_start <= t_start <= p_end) or (p_start <= t_end <= p_end):\n",
    "                            # Detected boundary error for the same entity type\n",
    "                            fp_counts[p_type]['Boundary'] += 1\n",
    "                            boundary_error = True\n",
    "                            break\n",
    "                    elif t_type != p_type and not (p_start == t_start and p_end == t_end):\n",
    "                        if (p_start <= t_start <= p_end) or (p_start <= t_end <= p_end):\n",
    "                            fp_counts[p_type]['Entity and Boundary'] += 1\n",
    "                            entity_boundary_error = True\n",
    "                            break\n",
    "                if not boundary_error and not entity_error and not entity_boundary_error:\n",
    "                    # Missed entity entirely (e.g., predicted as O)\n",
    "                    fp_counts[p_type]['O'] += 1\n",
    "\n",
    "        return dict(fp_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_false_negatives_with_boundary(true_entities, pred_entities):\n",
    "    fn_counts = defaultdict(Counter)\n",
    "    true_indexed = {(t[1], t[2]): t[0] for t in true_entities}  # Index true entities by boundaries\n",
    "    pred_indexed = {(p[1], p[2]): p[0] for p in pred_entities}  # Index predicted entities by boundaries\n",
    "\n",
    "    # Iterate through true entities to classify false negatives\n",
    "    for (t_start, t_end), t_type in true_indexed.items():\n",
    "        if (t_start, t_end) in pred_indexed:\n",
    "            if pred_indexed[(t_start, t_end)] != t_type:\n",
    "                # Type mismatch at the exact position\n",
    "                matched_type = pred_indexed.get((t_start, t_end))\n",
    "                fn_counts[t_type][matched_type] += 1\n",
    "        else:\n",
    "            # No exact match found, check for other errors\n",
    "            boundary_error = False\n",
    "            entity_error = False\n",
    "            for (p_start, p_end), p_type in pred_indexed.items():\n",
    "                if t_type == p_type:\n",
    "                    if (p_start <= t_start <= p_end) or (p_start <= t_end <= p_end):\n",
    "                        # Boundary error for the same type\n",
    "                        fn_counts[t_type]['Boundary'] += 1\n",
    "                        boundary_error = True\n",
    "                        break\n",
    "                else:\n",
    "                    if (p_start <= t_start <= p_end) or (p_start <= t_end <= p_end):\n",
    "                        # Boundary error with a different entity type\n",
    "                        fn_counts[t_type]['Entity and Boundary'] += 1\n",
    "                        entity_error = True\n",
    "                        break\n",
    "            if not boundary_error and not entity_error:\n",
    "                # Missed entity entirely\n",
    "                fn_counts[t_type]['O'] += 1\n",
    "\n",
    "    return dict(fn_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_false_positives_with_boundary(true_entities, pred_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_entities = get_entities(y_true)\n",
    "pred_entities = get_entities(y_pred)\n",
    "compute_false_negatives_with_boundary(true_entities, pred_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare entities\n",
    "confusion = EntityConfusion(y_true, y_pred)\n",
    "\n",
    "result = confusion.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(list({'Boundary': 76, 'O': 54, 'ORG': 13, 'LOC': 6, 'MISC': 5}.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum({'Boundary': 74,\n",
    "           'O': 51,\n",
    "           'ORG': 13,\n",
    "           'Entity and Boundary': 10,\n",
    "           'LOC': 6,\n",
    "           'MISC': 5}.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'PERS': Counter({'Boundary': 74,\n",
    "           'O': 51,\n",
    "           'ORG': 13,\n",
    "           'Entity and Boundary': 10,\n",
    "           'LOC': 6,\n",
    "           'MISC': 5}),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "29+10+7+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'false_negatives': {'LOC': Counter({'O': 32,\n",
    "           'Boundary': 10,\n",
    "           'MISC': 3,\n",
    "           'ORG': 3,\n",
    "           'PERS': 1}),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "def compute_false_negatives_with_boundary(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute false negatives with detailed categorization:\n",
    "    - 'Boundary': Incorrect boundaries for the same entity type.\n",
    "    - 'Missed': Predicted as O or no match at all.\n",
    "    \"\"\"\n",
    "    fn_counts = defaultdict(Counter)\n",
    "    true_indexed = {(t[1], t[2]): t[0] for t in y_true}  # Index true entities by boundaries\n",
    "    pred_indexed = {(p[1], p[2]): p[0] for p in y_pred}  # Index predicted entities by boundaries\n",
    "\n",
    "    # Iterate through true entities to classify false negatives\n",
    "    \n",
    "    for (t_start, t_end), t_type in true_indexed.items():\n",
    "        entity_error = False\n",
    "        if (t_start, t_end) not in pred_indexed or pred_indexed[(t_start, t_end)] != t_type:\n",
    "            # No matching prediction or type mismatch at the same position\n",
    "            matched_type = pred_indexed.get((t_start, t_end))\n",
    "            if matched_type:\n",
    "                fn_counts[t_type][matched_type] += 1\n",
    "                entity_error = True\n",
    "            # Check if there's a predicted entity of the same type with incorrect boundaries\n",
    "            boundary_error = False\n",
    "            for (p_start, p_end), p_type in pred_indexed.items():\n",
    "                if p_type == t_type and not (p_start == t_start and p_end == t_end):\n",
    "                    if (t_start <= p_start <= t_end) or (t_start <= p_end <= t_end):\n",
    "                        # Detected boundary error for the same entity type\n",
    "                        fn_counts[t_type]['Boundary'] += 1\n",
    "                        boundary_error = True\n",
    "                        break\n",
    "            \n",
    "            if not boundary_error and not entity_error:\n",
    "                # Missed entity entirely (e.g., predicted as O)\n",
    "                fn_counts[t_type]['O'] += 1\n",
    "    return dict(fn_counts)\n",
    "\n",
    "\n",
    "\n",
    "def compute_false_positives(y_true, y_pred):\n",
    "    fp_counts = defaultdict(Counter)\n",
    "    true_indexed = {(t[1], t[2]): t[0] for t in y_true}  # Index true entities by boundaries\n",
    "    pred_indexed = {(p[1], p[2]): p[0] for p in y_pred}  # Index predicted entities by boundaries\n",
    "\n",
    "    # Iterate through predicted entities to find false positives\n",
    "    for (p_start, p_end), p_type in pred_indexed.items():\n",
    "        entity_error = False\n",
    "        if (p_start, p_end) not in true_indexed or true_indexed[(p_start, p_end)] != p_type:\n",
    "            # No matching true entity or type mismatch at the same position\n",
    "            matched_type = true_indexed.get((p_start, p_end))\n",
    "            if matched_type:\n",
    "                fp_counts[p_type][matched_type] += 1\n",
    "                entity_error = True\n",
    "            boundary_error = False\n",
    "            for (t_start, t_end), t_type in true_indexed.items():\n",
    "                if t_type == p_type and not (p_start == t_start and p_end == t_end):\n",
    "                    if (p_start <= t_start <= p_end) or (p_start <= t_end <= p_end):\n",
    "                        # Detected boundary error for the same entity type\n",
    "                        fp_counts[p_type]['Boundary'] += 1\n",
    "                        boundary_error = True\n",
    "                        break\n",
    "            if not boundary_error and not entity_error:\n",
    "                # Missed entity entirely (e.g., predicted as O)\n",
    "                fp_counts[p_type]['O'] += 1\n",
    "\n",
    "    return dict(fp_counts)\n",
    "\n",
    "{'LOC': {'TP': 627, 'FP': 76, 'FN': 49}, 'ORG': {'TP': 338, 'FP': 105, 'FN': 121}, 'PERS': {'TP': 751, 'FP': 99, 'FN': 154}, 'MISC': {'TP': 151, 'FP': 54, 'FN': 92}}\n",
    "\n",
    "def compute_confusion_matrix(true_entities, pred_entities):\n",
    "    \"\"\"\n",
    "    Compute a confusion matrix for Named Entity Recognition (NER) predictions.\n",
    "\n",
    "    Parameters:\n",
    "        true_entities (list): List of ground truth entities, each represented as a tuple (type, start, end).\n",
    "        pred_entities (list): List of predicted entities, each represented as a tuple (type, start, end).\n",
    "\n",
    "    Returns:\n",
    "        dict: A confusion matrix structured as:\n",
    "              {entity_type: {'TP': count, 'FP': count, 'FN': count}}\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract all unique entity types from true and predicted entities\n",
    "    types = set([ent[0] for ent in true_entities]).union([ent[0] for ent in pred_entities])\n",
    "\n",
    "    # Initialize the confusion matrix\n",
    "    confusion_matrix = {typ: {'TP': 0, 'FP': 0, 'FN': 0} for typ in types}\n",
    "\n",
    "    def extract_entity_confusion(entity, true_entities, pred_entities):\n",
    "        \"\"\"\n",
    "        Helper function to calculate TP, FP, FN for a specific entity type.\n",
    "        \"\"\"\n",
    "        fns = set([e for e in true_entities if e[0] == entity]) - set([e for e in pred_entities if e[0] == entity])\n",
    "        fps = set([e for e in pred_entities if e[0] == entity]) - set([e for e in true_entities if e[0] == entity])\n",
    "        tps = set([e for e in pred_entities if e[0] == entity]).intersection(set([e for e in true_entities if e[0] == entity]))\n",
    "        return len(tps), len(fps), len(fns)\n",
    "\n",
    "    # Populate the confusion matrix for each entity type\n",
    "    for entity_type in types:\n",
    "        TP, FP, FN = extract_entity_confusion(entity_type, true_entities, pred_entities)\n",
    "        confusion_matrix[entity_type]['TP'] = TP\n",
    "        confusion_matrix[entity_type]['FP'] = FP\n",
    "        confusion_matrix[entity_type]['FN'] = FN\n",
    "\n",
    "    return confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "# Calculate false negatives\n",
    "false_negatives = compute_false_negatives_with_boundary(true_entities, pred_entities)\n",
    "false_positives = compute_false_positives(true_entities, pred_entities)\n",
    "print(false_negatives)\n",
    "print(false_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'LOC': {'TP': 627, 'FP': 76, 'FN': 49}, 'ORG': {'TP': 338, 'FP': 105, 'FN': 121}, 'PERS': {'TP': 751, 'FP': 99, 'FN': 154}, 'MISC': {'TP': 151, 'FP': 54, 'FN': 92}}\n",
    "\n",
    "ENTITY = 'LOC'\n",
    "def compute_confusion_matrix(true_entities, pred_entities):\n",
    "    \"\"\"\n",
    "    Compute a confusion matrix for Named Entity Recognition (NER) predictions.\n",
    "\n",
    "    Parameters:\n",
    "        true_entities (list): List of ground truth entities, each represented as a tuple (type, start, end).\n",
    "        pred_entities (list): List of predicted entities, each represented as a tuple (type, start, end).\n",
    "\n",
    "    Returns:\n",
    "        dict: A confusion matrix structured as:\n",
    "              {entity_type: {'TP': count, 'FP': count, 'FN': count}}\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract all unique entity types from true and predicted entities\n",
    "    types = set([ent[0] for ent in true_entities]).union([ent[0] for ent in pred_entities])\n",
    "\n",
    "    # Initialize the confusion matrix\n",
    "    confusion_matrix = {typ: {'TP': 0, 'FP': 0, 'FN': 0} for typ in types}\n",
    "\n",
    "    def extract_entity_confusion(entity, true_entities, pred_entities):\n",
    "        \"\"\"\n",
    "        Helper function to calculate TP, FP, FN for a specific entity type.\n",
    "        \"\"\"\n",
    "        fns = set([e for e in true_entities if e[0] == entity]) - set([e for e in pred_entities if e[0] == entity])\n",
    "        fps = set([e for e in pred_entities if e[0] == entity]) - set([e for e in true_entities if e[0] == entity])\n",
    "        tps = set([e for e in pred_entities if e[0] == entity]).intersection(set([e for e in true_entities if e[0] == entity]))\n",
    "        return len(tps), len(fps), len(fns)\n",
    "\n",
    "    # Populate the confusion matrix for each entity type\n",
    "    for entity_type in types:\n",
    "        TP, FP, FN = extract_entity_confusion(entity_type, true_entities, pred_entities)\n",
    "        confusion_matrix[entity_type]['TP'] = TP\n",
    "        confusion_matrix[entity_type]['FP'] = FP\n",
    "        confusion_matrix[entity_type]['FN'] = FN\n",
    "\n",
    "    return confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = compute_confusion_matrix(true_entities, pred_entities)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "conf_matrix = conf_matrix_obj.compute_confusion_matrix()\n",
    "print(\"Confusion Matrix:\", conf_matrix)\n",
    "\n",
    "false_negatives = conf_matrix_obj.compute_false_negatives_with_boundary()\n",
    "print(\"False Negatives:\", false_negatives)\n",
    "\n",
    "# Compute false positives with boundary\n",
    "false_positives = conf_matrix_obj.compute_false_positives()\n",
    "print(\"False Positives:\", false_positives)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Prepare data for bar chart\n",
    "bar_data = []\n",
    "for entity, counts in conf_matrix.items():\n",
    "    bar_data.append({'Entity': entity, 'Metric': 'TP', 'Count': counts['TP']})\n",
    "    bar_data.append({'Entity': entity, 'Metric': 'FP', 'Count': counts['FP']})\n",
    "    bar_data.append({'Entity': entity, 'Metric': 'FN', 'Count': counts['FN']})\n",
    "\n",
    "df_bar = pd.DataFrame(bar_data)\n",
    "\n",
    "# Plot stacked bar chart\n",
    "fig = px.bar(\n",
    "    df_bar,\n",
    "    x=\"Entity\",\n",
    "    y=\"Count\",\n",
    "    color=\"Metric\",\n",
    "    title=\"Distribution of TP, FP, and FN by Entity\",\n",
    "    text_auto=True,\n",
    "    barmode=\"stack\",\n",
    ")\n",
    "fig.update_layout(xaxis_title=\"Entity Type\", yaxis_title=\"Count\", legend_title=\"Metric\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "# False Negatives\n",
    "false_negatives = {\n",
    "    'LOC': {'O': 32, 'Boundary': 10, 'MISC': 3, 'ORG': 3, 'PERS': 1},\n",
    "    'PERS': {'Boundary': 76, 'O': 54, 'ORG': 13, 'LOC': 6, 'MISC': 5},\n",
    "    'ORG': {'O': 57, 'Boundary': 27, 'LOC': 20, 'PERS': 11, 'MISC': 6},\n",
    "    'MISC': {'O': 59, 'Boundary': 18, 'ORG': 9, 'LOC': 6},\n",
    "}\n",
    "\n",
    "# Convert to long format for Plotly\n",
    "df_false_negatives = pd.DataFrame(false_negatives).fillna(0)\n",
    "df_false_negatives_long = df_false_negatives.reset_index().melt(\n",
    "    id_vars='index', var_name='Entity Type', value_name='Count'\n",
    ")\n",
    "df_false_negatives_long.rename(columns={'index': 'Error Type'}, inplace=True)\n",
    "\n",
    "# Stacked Bar Chart\n",
    "fig = px.bar(\n",
    "    df_false_negatives_long,\n",
    "    x='Entity Type',\n",
    "    y='Count',\n",
    "    color='Error Type',\n",
    "    title='False Negatives by Entity Type',\n",
    "    barmode='group',\n",
    "    text='Count',\n",
    ")\n",
    "fig.update_layout(xaxis_title=\"Entity Type\", yaxis_title=\"Count\", legend_title=\"Error Type\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_false_negatives['LOC'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False Positives\n",
    "false_positives = {\n",
    "    'LOC': {'O': 30, 'ORG': 20, 'Boundary': 14, 'MISC': 6, 'PERS': 6},\n",
    "    'MISC': {'Boundary': 21, 'O': 19, 'ORG': 6, 'PERS': 5, 'LOC': 3},\n",
    "    'ORG': {'O': 49, 'Boundary': 31, 'PERS': 13, 'MISC': 9, 'LOC': 3},\n",
    "    'PERS': {'Boundary': 51, 'O': 36, 'ORG': 11, 'LOC': 1},\n",
    "}\n",
    "\n",
    "# Choose entity to visualize\n",
    "entity = 'LOC'\n",
    "data = false_positives[entity]\n",
    "data = pd.DataFrame(false_positives).fillna(0)[entity]\n",
    "\n",
    "# Pie Chart\n",
    "fig = px.pie(\n",
    "    names=list(data.index),\n",
    "    values=list(data.values),\n",
    "    title=f\"False Positives Distribution for {entity}\",\n",
    ")\n",
    "fig.update_traces(textinfo='percent+label')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confirming Boundary and O calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_entities = get_entities(y_true)\n",
    "pred_entities = get_entities(y_pred)\n",
    "true_indexed = {(t[1], t[2]): t[0] for t in true_entities}\n",
    "pred_indexed = {(t[1], t[2]): t[0] for t in pred_entities}\n",
    "entity = 'LOC'\n",
    "false_positives = set([e for e in pred_entities if e[0] == entity]) - set([e for e in true_entities if e[0] == entity])\n",
    "false_negatives = set([e for e in true_entities if e[0] == entity]) - set([e for e in pred_entities if e[0] == entity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_errors = []\n",
    "missing_errors = []\n",
    "for entity in false_positives:\n",
    "    o = []\n",
    "    t, s, e = entity\n",
    "    if (s, e) not in true_indexed:\n",
    "            missing_errors.append((s, e))\n",
    "    for p_entity in true_entities:\n",
    "        \n",
    "        p_t, p_s, p_e = p_entity\n",
    "        if p_t == t and not (p_s == s and p_e == e):\n",
    "            if (s <= p_s <= e) or (s <= p_e <= e):\n",
    "                print(entity)\n",
    "                print(p_entity)\n",
    "                print('end ####')\n",
    "                \n",
    "        if p_s == s and p_e == e:\n",
    "            entity_errors.append(p_entity)\n",
    "print(len(entity_errors))\n",
    "print(len(missing_errors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_errors = []\n",
    "missing_errors = []\n",
    "for entity in false_negatives:\n",
    "    o = []\n",
    "    t, s, e = entity\n",
    "    if (s, e) not in pred_indexed:\n",
    "            missing_errors.append((s, e))\n",
    "    for p_entity in pred_entities:\n",
    "        \n",
    "        p_t, p_s, p_e = p_entity\n",
    "        if p_t == t and not (p_s == s and p_e == e):\n",
    "            if (s <= p_s <= e) or (s <= p_e <= e):\n",
    "                print(entity)\n",
    "                print(p_entity)\n",
    "                print('end ####')\n",
    "                \n",
    "        if p_s == s and p_e == e:\n",
    "            entity_errors.append(p_entity)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(missing_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# strict test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "def flatten_strict_entities(entities):\n",
    "        \"\"\"\n",
    "        Flatten entities extracted in strict mode into tuples.\n",
    "\n",
    "        Args:\n",
    "            entities (Entities): The strict-mode entities.\n",
    "\n",
    "        Returns:\n",
    "            list: A flattened list of tuples representing the entities.\n",
    "        \"\"\"\n",
    "        return [e.to_tuple() for sen in entities.entities for e in sen]\n",
    "    \n",
    "scheme = auto_detect(y_true, False)\n",
    "\n",
    "entities_true = Entities(y_true, scheme, False)\n",
    "entities_pred = Entities(y_pred, scheme, False)\n",
    "true_entities = flatten_strict_entities(entities_true)\n",
    "pred_entities = flatten_strict_entities(entities_pred)\n",
    "entity = 'LOC'\n",
    "entity_true = [e for e in true_entities if e[1] == entity]\n",
    "entity_pred = [e for e in pred_entities if e[1] == entity]\n",
    "print(len(set(entity_true).intersection(set(entity_pred))))\n",
    "print(len(set(entity_true) - set(entity_pred)))\n",
    "print(len(set(entity_pred) - set(entity_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def flatten_strict_entities(entities):\n",
    "    return [e.to_tuple()[1:] for sen in entities.entities for e in sen]\n",
    "\n",
    "def calculate_confusion_matrix(y_true, y_pred):\n",
    "    # Initialize confusion matrix data structure\n",
    "    types = set([ent[0] for ent in y_true]).union([ent[0] for ent in y_pred])\n",
    "    confusion_matrix = {typ: {'TP': 0, 'FP': 0, 'FN': 0} for typ in types}\n",
    "\n",
    "\n",
    "    # Track matched predictions to avoid counting them more than once\n",
    "    matched_pred_indices = set()\n",
    "\n",
    "    # Check each true entity against predicted entities\n",
    "    for true_ent in y_true:\n",
    "        true_type, true_start, true_end = true_ent\n",
    "        match_found = False\n",
    "\n",
    "        for idx, pred_ent in enumerate(y_pred):\n",
    "            pred_type, pred_start, pred_end = pred_ent\n",
    "\n",
    "            if idx not in matched_pred_indices and true_type == pred_type and true_start == pred_start and true_end == pred_end:\n",
    "                confusion_matrix[true_type]['TP'] += 1\n",
    "                matched_pred_indices.add(idx)\n",
    "                match_found = True\n",
    "                break\n",
    "        \n",
    "        if not match_found:\n",
    "            confusion_matrix[true_type]['FN'] += 1\n",
    "\n",
    "\n",
    "    # Any unmatched prediction is a false positive\n",
    "    for idx, pred_ent in enumerate(y_pred):\n",
    "        if idx not in matched_pred_indices:\n",
    "            pred_type = pred_ent[0]\n",
    "            confusion_matrix[pred_type]['FP'] += 1\n",
    "\n",
    "    return confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "def compute_false_negatives(y_true, y_pred):\n",
    "    fn_counts = defaultdict(Counter)\n",
    "    true_indexed = {(t[1], t[2]): t[0] for t in y_true}  # Index true entities by boundaries\n",
    "    pred_indexed = {(p[1], p[2]): p[0] for p in y_pred}  # Index predicted entities by boundaries\n",
    "\n",
    "    # Iterate through true entities to find false negatives\n",
    "    for (t_start, t_end), t_type in true_indexed.items():\n",
    "        if (t_start, t_end) not in pred_indexed or pred_indexed[(t_start, t_end)] != t_type:\n",
    "            # No matching prediction or type mismatch at the same position\n",
    "            matched_type = pred_indexed.get((t_start, t_end), 'Boundary')\n",
    "            fn_counts[t_type][matched_type] += 1\n",
    "\n",
    "    return fn_counts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_false_positives(y_true, y_pred):\n",
    "    fp_counts = defaultdict(Counter)\n",
    "    true_indexed = {(t[1], t[2]): t[0] for t in y_true}  # Index true entities by boundaries\n",
    "    pred_indexed = {(p[1], p[2]): p[0] for p in y_pred}  # Index predicted entities by boundaries\n",
    "\n",
    "    # Iterate through predicted entities to find false positives\n",
    "    for (p_start, p_end), p_type in pred_indexed.items():\n",
    "        if (p_start, p_end) not in true_indexed or true_indexed[(p_start, p_end)] != p_type:\n",
    "            # No matching true entity or type mismatch at the same position\n",
    "            matched_type = true_indexed.get((p_start, p_end), 'Boundary')\n",
    "            fp_counts[p_type][matched_type] += 1\n",
    "\n",
    "    return fp_counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_y_true = get_entities(y_true)\n",
    "entity_y_pred = get_entities(y_pred)\n",
    "\n",
    "conf_matrix = calculate_confusion_matrix(entity_y_true, entity_y_pred)\n",
    "print(conf_matrix)\n",
    "fn_errors = compute_false_negatives(entity_y_true, entity_y_pred)\n",
    "fp_errors = compute_false_positives(entity_y_true, entity_y_pred)\n",
    "\n",
    "print(\"False Negatives:\", dict(fn_errors))\n",
    "print(\"False Positives:\", dict(fp_errors))\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme = auto_detect(y_true, False)\n",
    "\n",
    "entities_true = Entities(y_true, scheme, False)\n",
    "entities_pred = Entities(y_pred, scheme, False)\n",
    "true_entity_type = flatten_strict_entities(entities_true)\n",
    "pred_entity_type = flatten_strict_entities(entities_pred)\n",
    "# Example usage\n",
    "# conf_matrix = calculate_confusion_matrix([e.to_tuple()[1:] for sen in entities_true.entities for e in sen], [e.to_tuple()[1:] for sen in entities_pred.entities for e in sen])\n",
    "\n",
    "\n",
    "\n",
    "conf_matrix = calculate_confusion_matrix(true_entity_type, pred_entity_type)\n",
    "print(conf_matrix)\n",
    "fn_errors = compute_false_negatives(true_entity_type, pred_entity_type)\n",
    "fp_errors = compute_false_positives(true_entity_type, pred_entity_type)\n",
    "\n",
    "print(\"False Negatives:\", dict(fn_errors))\n",
    "print(\"False Positives:\", dict(fp_errors))\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "entity_y_true = get_entities(entity_outputs['entity_outputs']['y_true'])\n",
    "entity_y_pred = get_entities(entity_outputs['entity_outputs']['y_pred'])\n",
    "\n",
    "\n",
    "entities_true = Entities(entity_outputs['entity_outputs']['y_true'], scheme, False)\n",
    "entities_pred = Entities(entity_outputs['entity_outputs']['y_pred'], scheme, False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = calculate_confusion_matrix(entity_y_true, entity_y_pred)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(627+151+751+338) / ((627+92)+(151+154)+(751+49)+(338+121))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1867 / 2201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = calculate_confusion_matrix(entity_y_true, entity_y_pred)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_metrics = {}\n",
    "for metric in ['TP', 'FP', 'FN']:\n",
    "    total_metrics[metric] = sum(details[metric] for details in conf_matrix.values())\n",
    "print(total_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1867 / (1867+334)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Example usage\n",
    "fn_errors = compute_false_negatives(entity_y_true, entity_y_pred)\n",
    "fp_errors = compute_false_positives(entity_y_true, entity_y_pred)\n",
    "\n",
    "print(\"False Negatives:\", dict(fn_errors))\n",
    "print(\"False Positives:\", dict(fp_errors))\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Your original data\n",
    "data = conf_matrix\n",
    "\n",
    "# Prepare lists for DataFrame construction\n",
    "actual = []\n",
    "predicted = []\n",
    "counts = []\n",
    "\n",
    "for (act, pred), count in data.items():\n",
    "    actual.append(act)\n",
    "    predicted.append('None' if pred is None else pred)  # Replace None with 'None' for better visualization\n",
    "    counts.append(count)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'Actual': actual, 'Predicted': predicted, 'Count': counts})\n",
    "\n",
    "# Pivot to format suitable for heatmap\n",
    "pivot_table = df.pivot(index='Actual', columns='Predicted', values='Count').fillna(0)\n",
    "\n",
    "# Generate heatmap\n",
    "fig = px.imshow(pivot_table,\n",
    "                labels=dict(x=\"Predicted Entity Type\", y=\"Actual Entity Type\", color=\"Count\"),\n",
    "                x=pivot_table.columns,\n",
    "                y=pivot_table.index,\n",
    "                text_auto=True,\n",
    "                aspect=\"auto\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Entity Recognition Confusion Matrix\",\n",
    "    xaxis_title=\"Predicted Entity Type\",\n",
    "    yaxis_title=\"Actual Entity Type\"\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Your original data\n",
    "data = conf_matrix1\n",
    "\n",
    "# Prepare lists for DataFrame construction\n",
    "actual = []\n",
    "predicted = []\n",
    "counts = []\n",
    "\n",
    "for (act, pred), count in data.items():\n",
    "    actual.append(act)\n",
    "    predicted.append('None' if pred is None else pred)  # Replace None with 'None' for better visualization\n",
    "    counts.append(count)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'Actual': actual, 'Predicted': predicted, 'Count': counts})\n",
    "\n",
    "# Pivot to format suitable for heatmap\n",
    "pivot_table = df.pivot(index='Actual', columns='Predicted', values='Count').fillna(0)\n",
    "\n",
    "# Generate heatmap\n",
    "fig = px.imshow(pivot_table,\n",
    "                labels=dict(x=\"Predicted Entity Type\", y=\"Actual Entity Type\", color=\"Count\"),\n",
    "                x=pivot_table.columns,\n",
    "                y=pivot_table.index,\n",
    "                text_auto=True,\n",
    "                aspect=\"auto\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Entity Recognition Confusion Matrix\",\n",
    "    xaxis_title=\"Predicted Entity Type\",\n",
    "    yaxis_title=\"Actual Entity Type\"\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_tag = 'LOC'\n",
    "\n",
    "false_negatives = set([e for e in entity_y_true if e[0] == entity_tag]) - set([e for e in entity_y_pred if e[0] == entity_tag])\n",
    "for fn in false_negatives:\n",
    "  t, fn_s, fn_e = fn\n",
    "  for entity in entity_y_true:\n",
    "      t_t, t_s, t_e = entity\n",
    "      if fn_s == t_s or fn_e == t_e:\n",
    "        if t_s!=t_e:\n",
    "          print(entity)\n",
    "id = 5594\n",
    "for entity in entity_y_pred:\n",
    "    t, s, e = entity\n",
    "    if s == id:\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_tag = 'LOC'\n",
    "\n",
    "false_postive = set([e for e in entity_y_pred if e[0] == entity_tag]) - set([e for e in entity_y_true if e[0] == entity_tag])\n",
    "\n",
    "for fn in false_postive:\n",
    "  t, fn_s, fn_e = fn\n",
    "  for entity in entity_y_pred:\n",
    "      t_t, t_s, t_e = entity\n",
    "      if fn_s == t_s or fn_e == t_e:\n",
    "        if t_s!=t_e:\n",
    "          print(entity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so the false positive doesn't have to be false positive. \n",
    "id = 6445\n",
    "for entity in entity_y_true:\n",
    "    t, s, e = entity\n",
    "    if s == id:\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr =[\n",
    "        tok for sen in entity_outputs['entity_outputs']['y_pred']\n",
    "        for tok in sen\n",
    "        ]\n",
    "\n",
    "\n",
    "tr =[\n",
    "        tok for sen in entity_outputs['entity_outputs']['y_true']\n",
    "        for tok in sen\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr[8760:8790] == tr[8760:8790]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr[8760:8790]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "('LOC', 8786, 8787)\n",
    "('LOC', 8864, 8865)\n",
    "('LOC', 16466, 16467)\n",
    "('LOC', 5593, 5594)\n",
    "('LOC', 2545, 2546)\n",
    "('LOC', 25446, 25447)\n",
    "('LOC', 1615, 1616)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITY = 'LOC'\n",
    "entity_false_negatives = {ENTITY: Counter()}\n",
    "false_negatives = set([e for e in entity_y_true if e[0] == 'LOC']) - set([e for e in entity_y_pred if e[0] == 'LOC'])\n",
    "for e in false_negatives:\n",
    "    t_type, t_start, t_end = e\n",
    "    for pred_ent in entity_y_pred:\n",
    "        p_type, p_start, p_end = pred_ent\n",
    "        if t_start == p_start and t_start == p_end:\n",
    "            if p_type == 'LOC':\n",
    "                print(pred_ent)\n",
    "            entity_false_negatives[t_type][p_type]+=1\n",
    "            \n",
    "\n",
    "ENTITY = 'LOC'\n",
    "entity_false_positives = {ENTITY: Counter()}\n",
    "false_positive = set([e for e in entity_y_pred if e[0] == ENTITY]) - set([e for e in entity_y_true if e[0] == ENTITY]) \n",
    "for e in false_positive:\n",
    "    p_type, p_start, p_end = e\n",
    "    for true_ent in entity_y_true:\n",
    "        t_type, t_start, t_end = true_ent\n",
    "        if t_start == p_start and t_end == p_end:\n",
    "            # if p_type == 'ORG':\n",
    "            #     # if t_type == 'ORG':\n",
    "            #         print(true_ent)\n",
    "            if p_type == t_type:\n",
    "                entity_false_positives[p_type][t_type]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 8786\n",
    "for entity in entity_y_true:\n",
    "    t, s, e = entity\n",
    "    if s == id:\n",
    "        print(entity)\n",
    "for entity in entity_y_pred:\n",
    "    t, s, e = entity\n",
    "    if s == id or e == id+1:\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in false_negatives:\n",
    "    t, s, e = entity\n",
    "    # if t == 'LOC':\n",
    "    #     print(entity)\n",
    "    if s == 8786:\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in false_positive:\n",
    "    t, s, e = entity\n",
    "   \n",
    "    if s == 16466 or e == 16467:\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in entity_y_true:\n",
    "    t, s, e = entity\n",
    "    if s == 16963:\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in entity_y_pred:\n",
    "    t, s, e = entity\n",
    "    if s == 16963:\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
