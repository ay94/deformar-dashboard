{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 06:36:16 - INFO - PyTorch version 2.2.2 available.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "# This appends the directory one level up (the root of your project) to the sys.path.\n",
    "# Modify the path depending on the location of modules you want to import.\n",
    "sys.path.append(os.path.abspath('../../'))\n",
    "\n",
    "from config.config_managers import DashboardConfigManager\n",
    "from dataManager import DataManager\n",
    "from dash import Dash\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG_PATH = Path(\"/Users/ay227/Desktop/Final-Year/Thesis-Experiments/Online-Dashboard-Phase/dashboard-config.yaml\")\n",
    "CONFIG_PATH = Path(\"/Users/ahmed/Desktop/Dashboard/analysis-config.yaml\")\n",
    "config_manager = DashboardConfigManager(CONFIG_PATH)\n",
    "dev_config = config_manager.development_config    \n",
    "\n",
    "app = Dash(__name__, suppress_callback_exceptions=True)\n",
    "\n",
    "app_config = config_manager.app_config\n",
    "server = app.server  # Flask server instance for caching\n",
    "variants_data = None\n",
    "\n",
    "data_manager = DataManager(config_manager, server)\n",
    "dash_data = data_manager.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "class BaseAnalysis:\n",
    "    \"\"\"Base class for analyzing different variable groups across model variants.\"\"\"\n",
    "\n",
    "    VARIANT_MAPPING = {\n",
    "        \"ANERCorp_CamelLab_arabertv02\": \"Arabic\",\n",
    "        \"conll2003_bert\": \"English\"\n",
    "    }\n",
    "\n",
    "    def __init__(self, dash_data):\n",
    "        \"\"\"Initialize the analysis with the provided dataset.\"\"\"\n",
    "        self.dash_data = dash_data\n",
    "        self.analysis_df = self._prepare_data()\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        \"\"\"Prepare and combine all dataset variants into a single DataFrame.\"\"\"\n",
    "        analysis_data = []\n",
    "        for data_name, data_content in self.dash_data.items():\n",
    "            data = data_content.analysis_data.copy()\n",
    "            data[\"Language\"] = self.VARIANT_MAPPING.get(data_name, data_name)\n",
    "            analysis_data.append(data)\n",
    "        return pd.concat(analysis_data)\n",
    "\n",
    "    def filter_and_standardize(self, df, variables):\n",
    "        \"\"\"Filter out special tokens and standardize entity label names.\"\"\"\n",
    "        # df = df[variables + [\"Language\", \"True Labels\"]].copy()\n",
    "        \n",
    "        # Remove special tokens\n",
    "        df = df[~df[\"True Labels\"].isin([\"[SEP]\", \"[CLS]\", \"IGNORED\"])].copy()\n",
    "\n",
    "        # Standardize label names\n",
    "        df.loc[:, \"True Labels\"] = df[\"True Labels\"].replace({\"B-PERS\": \"B-PER\", \"I-PERS\": \"I-PER\"})\n",
    "        df.loc[:, \"Pred Labels\"] = df[\"Pred Labels\"].replace({\"B-PERS\": \"B-PER\", \"I-PERS\": \"I-PER\"})\n",
    "    \n",
    "        \n",
    "        return df\n",
    "\n",
    "    def compute_summary(self, df, variables, groupby_cols):\n",
    "        \"\"\"Compute mean and standard deviation for each variable per group.\"\"\"\n",
    "        summary = df.groupby(groupby_cols)[variables].agg([\"mean\", \"std\"]).reset_index()\n",
    "        # Flatten column names\n",
    "        summary.columns = [' '.join(col).strip() if isinstance(col, tuple) else col for col in summary.columns]\n",
    "        return summary\n",
    "\n",
    "    def format_long_data(self, summary_df, groupby_cols, metric_label):\n",
    "        \"\"\"Convert summary DataFrame into long format for visualization.\"\"\"\n",
    "        mean_cols = [col for col in summary_df.columns if \"mean\" in col]\n",
    "        std_cols = [col.replace(\"mean\", \"std\") for col in mean_cols]\n",
    "\n",
    "        # Melt mean values\n",
    "        mean_long = summary_df.melt(\n",
    "            id_vars=[col for col in summary_df.columns if col not in mean_cols + std_cols],\n",
    "            value_vars=mean_cols,\n",
    "            var_name=metric_label,\n",
    "            value_name=\"Mean Value\"\n",
    "        )\n",
    "\n",
    "        # Melt standard deviation values\n",
    "        std_long = summary_df.melt(\n",
    "            id_vars=[col for col in summary_df.columns if col not in mean_cols + std_cols],\n",
    "            value_vars=std_cols,\n",
    "            var_name=metric_label,\n",
    "            value_name=\"Std Dev\"\n",
    "        )\n",
    "\n",
    "        # Clean column names\n",
    "        mean_long[metric_label] = mean_long[metric_label].str.replace(\" mean\", \"\")\n",
    "        std_long[metric_label] = std_long[metric_label].str.replace(\" std\", \"\")\n",
    "\n",
    "        # Merge mean and std DataFrames\n",
    "        summary_long = mean_long.merge(std_long, on= groupby_cols + [metric_label], how=\"left\")\n",
    "        \n",
    "\n",
    "        # Round values for readability\n",
    "        summary_long[\"Mean Value\"] = summary_long[\"Mean Value\"].round(3)\n",
    "        summary_long[\"Std Dev\"] = summary_long[\"Std Dev\"].round(3)\n",
    "\n",
    "        # Modify text labels to include Std Dev\n",
    "        summary_long[\"Text Label\"] = summary_long.apply(\n",
    "            lambda row: f\"{row['Mean Value']} <br>Â±<br>{row['Std Dev']}\", axis=1\n",
    "        )\n",
    "\n",
    "        return summary_long\n",
    "    \n",
    "    def plot_high_level(self, summary_long_df, metric_label, title):\n",
    "        \"\"\"Generate a bar plot for high-level (language-based) analysis.\"\"\"\n",
    "        fig = px.bar(\n",
    "            summary_long_df,\n",
    "            x=metric_label,\n",
    "            y=\"Mean Value\",\n",
    "            color=\"Language\",\n",
    "            barmode=\"group\",\n",
    "            text=\"Text Label\",\n",
    "            title=title,\n",
    "            labels={\"Mean Value\": \"Score\", metric_label: \"Metrics\"},\n",
    "        )\n",
    "\n",
    "        fig.update_traces(\n",
    "            textfont=dict(size=12)\n",
    "        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            template=\"plotly_white\",\n",
    "            showlegend=True,\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "        \n",
    "    def plot_confidence(self, summary_long_df, metric_label, title):\n",
    "        \"\"\"Generate a bar plot for high-level (language-based) analysis.\"\"\"\n",
    "        fig = px.bar(\n",
    "            summary_long_df,\n",
    "            x=metric_label,\n",
    "            y=\"Mean Value\",\n",
    "            color=\"Language\",\n",
    "            facet_row=\"Type\",\n",
    "            barmode=\"group\",\n",
    "            text=\"Text Label\",\n",
    "            title=title,\n",
    "            facet_row_spacing=0.15,\n",
    "            labels={\"Mean Value\": \"Average Score\", metric_label: \"Metrics\"},\n",
    "        )\n",
    "\n",
    "        fig.update_traces(\n",
    "            textfont=dict(size=12)\n",
    "        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            template=\"plotly_white\",\n",
    "            showlegend=True,\n",
    "            height=600,\n",
    "            margin=dict(t=50, b=50, l=50, r=50),\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "    \n",
    "    def plot_token_confidence(self, summary_long_df, metric_label, title):\n",
    "        \"\"\"Generate a bar plot for high-level (language-based) analysis.\"\"\"\n",
    "        fig = px.bar(\n",
    "            summary_long_df,\n",
    "            x=\"True Labels\",\n",
    "            y=\"Mean Value\",\n",
    "            color=\"Language\",\n",
    "            facet_row=\"Type\",\n",
    "            barmode=\"group\",\n",
    "            text=\"Text Label\",\n",
    "            title=title,\n",
    "            facet_row_spacing=0.15,\n",
    "            labels={\"Mean Value\": \"Average Score\", metric_label: \"Metrics\"},\n",
    "        )\n",
    "\n",
    "        fig.update_traces(\n",
    "            textfont=dict(size=12)\n",
    "        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            template=\"plotly_white\",\n",
    "            showlegend=True,\n",
    "            height=600,\n",
    "            margin=dict(t=50, b=50, l=50, r=50),\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "\n",
    "    def plot_entity_level(self, summary_long_df, metric_label, title, height):\n",
    "        \"\"\"Generate a bar plot for entity-based (NER tag) analysis.\"\"\"\n",
    "        fig = px.bar(\n",
    "            summary_long_df,\n",
    "            x=\"True Labels\",\n",
    "            y=\"Mean Value\",\n",
    "            color=\"Language\",\n",
    "            facet_row=metric_label,\n",
    "            barmode=\"group\",\n",
    "            text=\"Text Label\",\n",
    "            title=title,\n",
    "            labels={\"Mean Value\": \"Average Score\", metric_label: \"Metrics\"},\n",
    "            height=height,\n",
    "            facet_row_spacing=0.0001,\n",
    "        )\n",
    "\n",
    "        fig.update_traces(\n",
    "            textfont=dict(size=12)\n",
    "        )\n",
    "        \n",
    "\n",
    "        fig.update_layout(\n",
    "            template=\"plotly_white\",\n",
    "            showlegend=True,\n",
    "            margin=dict(t=60, b=60, l=80, r=80),\n",
    "        )\n",
    "        fig.for_each_yaxis(lambda yaxis: yaxis.update(matches=None, autorange=True))  # Independent y-axes\n",
    "        \n",
    "        \n",
    "        fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AmbiguityAnalysis(BaseAnalysis):\n",
    "    \"\"\"Class for analyzing ambiguity-related metrics.\"\"\"\n",
    "\n",
    "    VARIABLES = {\n",
    "        \"Token Level\": [\"Dataset Token Entropy\", \"Normalized Token Entropy\"],\n",
    "        \"Word Level\": [\"Dataset Word Entropy\", \"Normalized Word Entropy\"]\n",
    "    }\n",
    "\n",
    "    def analyze_token_high_level(self):\n",
    "        \"\"\"Perform high-level token ambiguity analysis across languages.\"\"\"\n",
    "        variables = self.VARIABLES[\"Token Level\"]\n",
    "        df = self.analysis_df.copy()\n",
    "        \n",
    "        # Remove OOV entropy (-1) specifically for token-level metrics\n",
    "        df = df[(df[variables] != -1).all(axis=1)]\n",
    "\n",
    "        summary = self.compute_summary(df, variables, groupby_cols=[\"Language\"])\n",
    "        summary_long = self.format_long_data(summary, [\"Language\"], \"Ambiguity Metrics\")\n",
    "        self.plot_high_level(summary_long, \"Ambiguity Metrics\", \"High-Level Token Ambiguity Analysis\")\n",
    "\n",
    "    def analyze_word_high_level(self):\n",
    "        \"\"\"Perform high-level word ambiguity analysis across languages.\"\"\"\n",
    "        variables = self.VARIABLES[\"Word Level\"]\n",
    "        df = self.analysis_df.copy()\n",
    "        \n",
    "        # Remove OOV entropy (-1) specifically for word-level metrics\n",
    "        df = df[(df[variables] != -1).all(axis=1)]\n",
    "\n",
    "        summary = self.compute_summary(df, variables, groupby_cols=[\"Language\"])\n",
    "        summary_long = self.format_long_data(summary, [\"Language\"], \"Ambiguity Metrics\")\n",
    "        self.plot_high_level(summary_long, \"Ambiguity Metrics\", \"High-Level Word Ambiguity Analysis\")\n",
    "\n",
    "    def analyze_token_entity_level(self, height=800):\n",
    "        \"\"\"Perform entity-level token ambiguity analysis.\"\"\"\n",
    "        variables = self.VARIABLES[\"Token Level\"]\n",
    "        df = self.filter_and_standardize(self.analysis_df, variables)\n",
    "\n",
    "        # Remove OOV entropy (-1) specifically for token-level metrics\n",
    "        df = df[(df[variables] != -1).all(axis=1)]\n",
    "\n",
    "        summary = self.compute_summary(df, variables, groupby_cols=[\"Language\", \"True Labels\"])\n",
    "        summary_long = self.format_long_data(summary, [\"Language\", \"True Labels\"], \"Ambiguity Metrics\")\n",
    "        self.plot_entity_level(summary_long, \"Ambiguity Metrics\", \"Entity-Level Token Ambiguity Analysis\", height)\n",
    "\n",
    "    def analyze_word_entity_level(self, height=800):\n",
    "        \"\"\"Perform entity-level word ambiguity analysis.\"\"\"\n",
    "        variables = self.VARIABLES[\"Word Level\"]\n",
    "        df = self.filter_and_standardize(self.analysis_df, variables)\n",
    "\n",
    "        # Remove OOV entropy (-1) specifically for word-level metrics\n",
    "        df = df[(df[variables] != -1).all(axis=1)]\n",
    "\n",
    "        summary = self.compute_summary(df, variables, groupby_cols=[\"Language\", \"True Labels\"])\n",
    "        summary_long = self.format_long_data(summary, [\"Language\", \"True Labels\"], \"Ambiguity Metrics\")\n",
    "        self.plot_entity_level(summary_long, \"Ambiguity Metrics\", \"Entity-Level Word Ambiguity Analysis\", height)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class ConsistencyAnalysis(BaseAnalysis):\n",
    "#     \"\"\"Class for analyzing consistency metrics across model variants.\"\"\"\n",
    "\n",
    "#     VARIABLES = {\n",
    "#         \"Consistency Variables\": [\"Consistency Ratio\", \"Inconsistency Ratio\"],\n",
    "#         \"Consistency Absolute\": [\"Consistency Count\", \"Inconsistency Count\"],\n",
    "#         \"OOV\": [\"Dataset Token Entropy\", \"Normalized Token Entropy\"]\n",
    "#     }\n",
    "\n",
    "#     def analyze_high_level(self):\n",
    "#         \"\"\"Perform high-level consistency analysis across languages.\"\"\"\n",
    "#         variables = self.VARIABLES[\"Consistency Variables\"]\n",
    "#         oov_variables = self.VARIABLES[\"OOV\"]\n",
    "#         df = self.analysis_df.copy()\n",
    "#         print(df.columns)\n",
    "#         df = df[(df[oov_variables] != -1).all(axis=1)]\n",
    "#         summary = self.compute_summary(df, variables, groupby_cols=[\"Language\"])\n",
    "#         summary_long = self.format_long_data(summary, [\"Language\"], \"Consistency Metrics\")\n",
    "#         self.plot_high_level(summary_long, \"Consistency Metrics\", \"High-Level Consistency Analysis\")\n",
    "\n",
    "#     def analyze_entity_level(self,  height=800):\n",
    "#         \"\"\"Perform entity-level consistency analysis.\"\"\"\n",
    "#         variables = self.VARIABLES[\"Consistency Variables\"]\n",
    "#         oov_variables = self.VARIABLES[\"OOV\"]\n",
    "#         df = self.filter_and_standardize(self.analysis_df, variables)\n",
    "#         df = df[(df[oov_variables] != -1).all(axis=1)]\n",
    "#         summary = self.compute_summary(df, variables, groupby_cols=[\"Language\", \"True Labels\"])\n",
    "#         summary_long = self.format_long_data(summary, [\"Language\", \"True Labels\"], \"Consistency Metrics\")\n",
    "#         self.plot_entity_level(summary_long, \"Consistency Metrics\", \"Entity-Level Consistency Analysis\", height)\n",
    "\n",
    "\n",
    "\n",
    "class ConsistencyAnalysis(BaseAnalysis):\n",
    "    \"\"\"Class for analyzing consistency metrics across model variants.\"\"\"\n",
    "\n",
    "    VARIABLES = {\n",
    "        \"Consistency Variables\": [\"Consistency Ratio\", \"Inconsistency Ratio\"],\n",
    "        \"Consistency Absolute\": [\"Consistency Count\", \"Inconsistency Count\"],\n",
    "        \"OOV\": [\"Dataset Token Entropy\", \"Normalized Token Entropy\"]\n",
    "    }\n",
    "\n",
    "    def analyze_high_level(self):\n",
    "        \"\"\"Perform high-level consistency analysis across languages (Graph).\"\"\"\n",
    "        variables = self.VARIABLES[\"Consistency Variables\"]\n",
    "        oov_variables = self.VARIABLES[\"OOV\"]\n",
    "        df = self.analysis_df.copy()\n",
    "        df = df[(df[oov_variables] != -1).all(axis=1)]\n",
    "        summary = self.compute_summary(df, variables, groupby_cols=[\"Language\"])\n",
    "        summary_long = self.format_long_data(summary, [\"Language\"], \"Consistency Metrics\")\n",
    "        self.plot_high_level(summary_long, \"Consistency Metrics\", \"High-Level Consistency Analysis\")\n",
    "\n",
    "    def analyze_entity_level(self, height=800):\n",
    "        \"\"\"Perform entity-level consistency analysis (Graph).\"\"\"\n",
    "        variables = self.VARIABLES[\"Consistency Variables\"]\n",
    "        oov_variables = self.VARIABLES[\"OOV\"]\n",
    "        df = self.analysis_df.copy()\n",
    "        df = df[(df[oov_variables] != -1).all(axis=1)]\n",
    "        df = self.filter_and_standardize(df, variables)\n",
    "        \n",
    "        summary = self.compute_summary(df, variables, groupby_cols=[\"Language\", \"True Labels\"])\n",
    "        summary_long = self.format_long_data(summary, [\"Language\", \"True Labels\"], \"Consistency Metrics\")\n",
    "        self.plot_entity_level(summary_long, \"Consistency Metrics\", \"Entity-Level Consistency Analysis\", height)\n",
    "\n",
    "    def generate_high_level_table(self):\n",
    "        \"\"\"Generate a table for high-level consistency analysis.\"\"\"\n",
    "        variables = self.VARIABLES[\"Consistency Absolute\"]\n",
    "        oov_variables = self.VARIABLES[\"OOV\"]\n",
    "        df = self.analysis_df.copy()\n",
    "        df = df[(df[oov_variables] != -1).all(axis=1)]\n",
    "        summary = self.compute_summary(df, variables, groupby_cols=[\"Language\"])\n",
    "        \n",
    "        \n",
    "        table = pd.DataFrame(summary)\n",
    "        table = table.round(2)\n",
    "        # table.rename(columns={\"Language\": \"Dataset\", \n",
    "        #                       \"Consistency Count\": \"Consistent Tokens\", \n",
    "        #                       \"Inconsistency Count\": \"Inconsistent Tokens\"}, inplace=True)\n",
    "        \n",
    "        display(table)\n",
    "    def generate_entity_level_table(self):\n",
    "        \"\"\"Generate a table for entity-level consistency analysis.\"\"\"\n",
    "        variables = self.VARIABLES[\"Consistency Absolute\"]\n",
    "        oov_variables = self.VARIABLES[\"OOV\"]\n",
    "        df = self.analysis_df.copy()\n",
    "        df = df[(df[oov_variables] != -1).all(axis=1)]\n",
    "        df = self.filter_and_standardize(self.analysis_df, variables)\n",
    "        summary = self.compute_summary(df, variables, groupby_cols=[\"Language\", \"True Labels\"])\n",
    "\n",
    "        \n",
    "        table = pd.DataFrame(summary)\n",
    "        table = table.round(2)\n",
    "        # table.rename(columns={\"Language\": \"Dataset\", \n",
    "        #                       \"True Labels\": \"Entity Tag\",\n",
    "        #                       \"Consistency Count\": \"Consistent Tokens\", \n",
    "        #                       \"Inconsistency Count\": \"Inconsistent Tokens\"}, inplace=True)\n",
    "        display(table)\n",
    "\n",
    "\n",
    "class LossAnalysis(BaseAnalysis):\n",
    "    \"\"\"Class for analyzing loss metrics across model variants.\"\"\"\n",
    "\n",
    "    VARIABLES = {\n",
    "        \"Loss Metrics\": [\"Losses\"]\n",
    "    }\n",
    "\n",
    "    def analyze_high_level(self):\n",
    "        \"\"\"Perform high-level loss analysis across languages.\"\"\"\n",
    "        variables = self.VARIABLES[\"Loss Metrics\"]\n",
    "        df = self.analysis_df.copy()\n",
    "        \n",
    "        summary = self.compute_summary(df, variables, groupby_cols=[\"Language\"])\n",
    "        summary_long = self.format_long_data(summary, [\"Language\"], \"Loss Metrics\")\n",
    "        self.plot_high_level(summary_long, \"Loss Metrics\", \"High-Level Loss Analysis\")\n",
    "\n",
    "    def analyze_entity_level(self, height=500):\n",
    "        \"\"\"Perform entity-level loss analysis.\"\"\"\n",
    "        variables = self.VARIABLES[\"Loss Metrics\"]\n",
    "        df = self.filter_and_standardize(self.analysis_df, variables)\n",
    "\n",
    "        summary = self.compute_summary(df, variables, groupby_cols=[\"Language\", \"True Labels\"])\n",
    "        summary_long = self.format_long_data(summary, [\"Language\", \"True Labels\"], \"Loss Metrics\")\n",
    "        self.plot_entity_level(summary_long, \"Loss Metrics\", \"Entity-Level Loss Analysis\", height)\n",
    "        \n",
    "\n",
    "\n",
    "class TokenizationAnalysis(BaseAnalysis):\n",
    "    \"\"\"Class for analyzing tokenization rates across model variants.\"\"\"\n",
    "\n",
    "    VARIABLES = {\n",
    "        \"Tokenization Metrics\": [\"Tokenization Rate\"]\n",
    "    }\n",
    "\n",
    "    def analyze_high_level(self):\n",
    "        \"\"\"Perform high-level tokenization analysis across languages.\"\"\"\n",
    "        variables = self.VARIABLES[\"Tokenization Metrics\"]\n",
    "        df = self.analysis_df.copy()\n",
    "\n",
    "        summary = self.compute_summary(df, variables, groupby_cols=[\"Language\"])\n",
    "        summary_long = self.format_long_data(summary, [\"Language\"], \"Tokenization Metrics\")\n",
    "        self.plot_high_level(summary_long, \"Tokenization Metrics\", \"High-Level Tokenization Analysis\")\n",
    "\n",
    "    def analyze_entity_level(self, height=500):\n",
    "        \"\"\"Perform entity-level tokenization analysis.\"\"\"\n",
    "        variables = self.VARIABLES[\"Tokenization Metrics\"]\n",
    "        df = self.filter_and_standardize(self.analysis_df, variables)\n",
    "\n",
    "        summary = self.compute_summary(df, variables, groupby_cols=[\"Language\", \"True Labels\"])\n",
    "        summary_long = self.format_long_data(summary, [\"Language\", \"True Labels\"], \"Tokenization Metrics\")\n",
    "        self.plot_entity_level(summary_long, \"Tokenization Metrics\", \"Entity-Level Tokenization Analysis\", height)\n",
    "        \n",
    "\n",
    "\n",
    "class PredictionAnalysis(BaseAnalysis):\n",
    "    \"\"\"Class for analyzing prediction variability across model variants.\"\"\"\n",
    "\n",
    "    VARIABLES = {\n",
    "        \"Prediction Metrics\": [\"Variability\", \"Normalized Prediction Entropy\"]\n",
    "    }\n",
    "\n",
    "    def analyze_high_level(self):\n",
    "        \"\"\"Perform high-level prediction variability analysis across languages.\"\"\"\n",
    "        variables = self.VARIABLES[\"Prediction Metrics\"]\n",
    "        df = self.analysis_df.copy()\n",
    "\n",
    "        summary = self.compute_summary(df, variables, groupby_cols=[\"Language\"])\n",
    "        summary_long = self.format_long_data(summary, [\"Language\"], \"Prediction Metrics\")\n",
    "        self.plot_high_level(summary_long, \"Prediction Metrics\", \"High-Level Prediction Distribution Analysis\")\n",
    "\n",
    "    def analyze_entity_level_correct(self, height=800):\n",
    "        \"\"\"Perform entity-level prediction variability analysis.\"\"\"\n",
    "        variables = self.VARIABLES[\"Prediction Metrics\"]\n",
    "        df = self.analysis_df.copy()\n",
    "        df = df[df['Agreements'] == True]\n",
    "        df = self.filter_and_standardize(df, variables)\n",
    "\n",
    "        summary = self.compute_summary(df, variables, groupby_cols=[\"Language\", \"True Labels\"])\n",
    "        summary_long = self.format_long_data(summary, [\"Language\", \"True Labels\"], \"Prediction Metrics\")\n",
    "        self.plot_entity_level(summary_long, \"Prediction Metrics\", \"Entity-Level Correct Prediction Distribution Analysis\", height)\n",
    "    \n",
    "    def analyze_entity_level_error(self, height=800):\n",
    "        \"\"\"Perform entity-level prediction variability analysis.\"\"\"\n",
    "        variables = self.VARIABLES[\"Prediction Metrics\"]\n",
    "        df = self.analysis_df.copy()\n",
    "        df = df[df['Agreements'] == False]\n",
    "        df = self.filter_and_standardize(df, variables)\n",
    "\n",
    "        summary = self.compute_summary(df, variables, groupby_cols=[\"Language\", \"True Labels\"])\n",
    "        summary_long = self.format_long_data(summary, [\"Language\", \"True Labels\"], \"Prediction Metrics\")\n",
    "        self.plot_entity_level(summary_long, \"Prediction Metrics\", \"Entity-Level Incorrect Prediction Distribution Analysis\", height)\n",
    "\n",
    "class SilhouetteAnalysis(BaseAnalysis):\n",
    "    \"\"\"Class for analyzing silhouette scores across model variants.\"\"\"\n",
    "\n",
    "    VARIABLES = {\n",
    "        \"Silhouette Scores\": [\"True Silhouette Score\", \"Pred Silhouette Score\"]\n",
    "    }\n",
    "\n",
    "    def analyze_high_level(self):\n",
    "        \"\"\"Perform high-level silhouette score analysis across languages.\"\"\"\n",
    "        variables = self.VARIABLES[\"Silhouette Scores\"]\n",
    "        df = self.analysis_df.copy()\n",
    "\n",
    "        summary = self.compute_summary(df, variables, groupby_cols=[\"Language\"])\n",
    "        summary_long = self.format_long_data(summary, [\"Language\"], \"Silhouette Scores\")\n",
    "        self.plot_high_level(summary_long, \"Silhouette Scores\", \"High-Level Silhouette Score Analysis\")\n",
    "\n",
    "    def analyze_entity_level(self, height=800):\n",
    "        \"\"\"Perform entity-level silhouette score analysis.\"\"\"\n",
    "        variables = self.VARIABLES[\"Silhouette Scores\"]\n",
    "        df = self.filter_and_standardize(self.analysis_df, variables)\n",
    "\n",
    "        summary = self.compute_summary(df, variables, groupby_cols=[\"Language\", \"True Labels\"])\n",
    "        summary_long = self.format_long_data(summary, [\"Language\", \"True Labels\"], \"Silhouette Scores\")\n",
    "        self.plot_entity_level(summary_long, \"Silhouette Scores\", \"Entity-Level Silhouette Score Analysis\", height)\n",
    "\n",
    "\n",
    "class ConfidenceAnalysis(BaseAnalysis):\n",
    "    \"\"\"Class for analyzing confidence metrics across model variants.\"\"\"\n",
    "\n",
    "    VARIABLES = {\n",
    "        \"High-Level Confidence\": [\n",
    "            'O Confidence', \n",
    "            'B-LOC Confidence', 'I-LOC Confidence',\n",
    "            'B-PER Confidence', 'I-PER Confidence', \n",
    "            'B-ORG Confidence', 'I-ORG Confidence',\n",
    "            'B-MISC Confidence', 'I-MISC Confidence'\n",
    "        ],\n",
    "        \"Entity-Level Confidence\": ['Token Confidence']\n",
    "    }\n",
    "\n",
    "    def analyze_high_level(self):\n",
    "        \"\"\"Perform high-level confidence analysis across languages.\"\"\"\n",
    "        variables = self.VARIABLES[\"High-Level Confidence\"]\n",
    "        df = self.analysis_df.copy()\n",
    "\n",
    "        summary = self.compute_summary(df, variables, groupby_cols=[\"Language\"])\n",
    "        summary_long = self.format_long_data(summary, [\"Language\"], \"Confidence Metrics\")\n",
    "        self.plot_high_level(summary_long, \"Confidence Metrics\", \"High-Level Confidence Analysis\")\n",
    "        \n",
    "    def analyze_high_level_error(self):\n",
    "        \"\"\"Perform high-level confidence analysis across languages.\"\"\"\n",
    "        variables = self.VARIABLES[\"High-Level Confidence\"]\n",
    "        df = self.analysis_df.copy()\n",
    "        df = df[df['Agreements'] == False]\n",
    "\n",
    "        summary = self.compute_summary(df, variables, groupby_cols=[\"Language\"])\n",
    "        summary_long = self.format_long_data(summary, [\"Language\"], \"Confidence Metrics\")\n",
    "        self.plot_high_level(summary_long, \"Confidence Metrics\", \"High-Level Error Prediction Confidence Analysis\")\n",
    "        \n",
    "    def analyze_high_level_correct(self):\n",
    "        \"\"\"Perform high-level confidence analysis across languages.\"\"\"\n",
    "        variables = self.VARIABLES[\"High-Level Confidence\"]\n",
    "        df = self.analysis_df.copy()\n",
    "        df = df[df['Agreements'] == True]\n",
    "\n",
    "        summary = self.compute_summary(df, variables, groupby_cols=[\"Language\"])\n",
    "        summary_long = self.format_long_data(summary, [\"Language\"], \"Confidence Metrics\")\n",
    "        self.plot_high_level(summary_long, \"Confidence Metrics\", \"High-Level Correct Prediction Confidence Analysis\")\n",
    "        \n",
    "\n",
    "    def analyze_high_level_summary(self):\n",
    "        \"\"\"Perform high-level confidence analysis across languages for both errors and correct predictions.\"\"\"\n",
    "        variables = self.VARIABLES[\"High-Level Confidence\"]\n",
    "        df = self.analysis_df.copy()\n",
    "\n",
    "        # Compute summary for incorrect predictions\n",
    "        df_errors = df[df['Agreements'] == False]\n",
    "        df_errors = self.filter_and_standardize(df_errors, variables)\n",
    "        summary_errors = self.compute_summary(df_errors, variables, groupby_cols=[\"Language\"])\n",
    "        summary_errors[\"Type\"] = \"Error\"\n",
    "\n",
    "        # Compute summary for correct predictions\n",
    "        df_correct = df[df['Agreements'] == True]\n",
    "        df_correct = self.filter_and_standardize(df_correct, variables)\n",
    "        summary_correct = self.compute_summary(df_correct, variables, groupby_cols=[\"Language\"])\n",
    "        summary_correct[\"Type\"] = \"Correct\"\n",
    "\n",
    "        # Merge both summaries\n",
    "        summary = pd.concat([summary_errors, summary_correct], ignore_index=True)\n",
    "\n",
    "        # Convert to long format for plotting\n",
    "        summary_long = self.format_long_data(summary, [\"Language\", \"Type\"], \"Confidence Metrics\")\n",
    "\n",
    "        # Plot the results in a single bar chart\n",
    "        self.plot_confidence(summary_long, \"Confidence Metrics\", \"Comparison of Confidence Scores Across Entity Tags in Correct and Incorrect Predictions\")\n",
    "\n",
    "\n",
    "    def analyze_entity_level_error(self, height=500):\n",
    "        \"\"\"Perform entity-level confidence analysis (Token Confidence only).\"\"\"\n",
    "        variables = self.VARIABLES[\"Entity-Level Confidence\"]\n",
    "        df = self.analysis_df.copy()\n",
    "        df = df[df['Agreements'] == False]\n",
    "        df = self.filter_and_standardize(df, variables)\n",
    "        \n",
    "        summary = self.compute_summary(df, variables, groupby_cols=[\"Language\", \"True Labels\"])\n",
    "        summary_long = self.format_long_data(summary, [\"Language\", \"True Labels\"], \"Token Confidence\")\n",
    "        self.plot_entity_level(summary_long, \"Token Confidence\", \"Entity-Level Error Prediction Confidence Analysis\", height)\n",
    "\n",
    "\n",
    "    def analyze_entity_level_correct(self, height=500):\n",
    "            \"\"\"Perform entity-level confidence analysis (Token Confidence only).\"\"\"\n",
    "            variables = self.VARIABLES[\"Entity-Level Confidence\"]\n",
    "            df = self.analysis_df.copy()\n",
    "            df = df[df['Agreements'] == True]\n",
    "            df = self.filter_and_standardize(df, variables)\n",
    "            \n",
    "            summary = self.compute_summary(df, variables, groupby_cols=[\"Language\", \"True Labels\"])\n",
    "            \n",
    "            summary_long = self.format_long_data(summary, [\"Language\", \"True Labels\"], \"Token Confidence\")\n",
    "            self.plot_entity_level(summary_long, \"Token Confidence\", \"Entity-Level Correct Prediction Confidence Analysis\", height)\n",
    "    \n",
    "    def analyze_entity_level_summary(self, height=500):\n",
    "        \"\"\"\n",
    "        Perform entity-level confidence analysis (Token Confidence only) for both correct and incorrect predictions\n",
    "        and present them in a single comparison plot.\n",
    "        \n",
    "        Args:\n",
    "            height (int): Height of the plot.\n",
    "        \"\"\"\n",
    "        variables = self.VARIABLES[\"Entity-Level Confidence\"]\n",
    "        df = self.analysis_df.copy()\n",
    "\n",
    "        # Compute summary for incorrect predictions\n",
    "        df_errors = df[df['Agreements'] == False]\n",
    "        df_errors = self.filter_and_standardize(df_errors, variables)\n",
    "        summary_errors = self.compute_summary(df_errors, variables, groupby_cols=[\"Language\", \"True Labels\"])\n",
    "        summary_errors[\"Type\"] = \"Error\"\n",
    "\n",
    "        # Compute summary for correct predictions\n",
    "        df_correct = df[df['Agreements'] == True]\n",
    "        df_correct = self.filter_and_standardize(df_correct, variables)\n",
    "        summary_correct = self.compute_summary(df_correct, variables, groupby_cols=[\"Language\", \"True Labels\"])\n",
    "        summary_correct[\"Type\"] = \"Correct\"\n",
    "\n",
    "        # Merge both summaries\n",
    "        summary = pd.concat([summary_errors, summary_correct], ignore_index=True)\n",
    "\n",
    "        # Convert to long format for plotting\n",
    "        summary_long = self.format_long_data(summary, [\"Language\", \"True Labels\", \"Type\"], \"Token Confidence\")\n",
    "\n",
    "        # Plot the results in a single bar chart\n",
    "        self.plot_token_confidence(summary_long, \"Token Confidence\", \"Comparison of Toke Confidence Score in Correct and Incorrect Predictions\")\n",
    "\n",
    "            \n",
    "            \n",
    "    # def detailed_entity_level_errors(self):\n",
    "    #     \"\"\"Perform entity-level confidence analysis (Token Confidence only).\"\"\"\n",
    "    #     variables = self.VARIABLES[\"Entity-Level Confidence\"]\n",
    "    #     df = self.analysis_df.copy()\n",
    "    #     df = df[df['Agreements'] == False].copy()\n",
    "    #     df = self.filter_and_standardize(df, variables+['Pred Labels'])\n",
    "        \n",
    "\n",
    "    #     # Group by 'True Labels' and 'Pred Labels' and calculate weighted average\n",
    "    #     grouped_df = df.groupby(['Language', 'True Labels', 'Pred Labels'])[['Token Confidence']].apply(\n",
    "    #         lambda x: pd.Series({\n",
    "    #             'Weighted Token Confidence': (x['Token Confidence'] * x['Token Confidence'].count()).sum() / x['Token Confidence'].count(),\n",
    "    #             'Count': x['Token Confidence'].count()  # Add count for reference\n",
    "    #         })\n",
    "    #     ).reset_index()\n",
    "\n",
    "    #     # Pivot to get heatmap format\n",
    "    #     heatmap_data = grouped_df.pivot_table(\n",
    "    #         index=['Language', 'True Labels'], \n",
    "    #         columns='Pred Labels', \n",
    "    #         values='Weighted Token Confidence', \n",
    "    #         fill_value=0\n",
    "    #     )\n",
    "        \n",
    "        \n",
    "    #     heatmap_arabic = heatmap_data.loc[\"Arabic\"]\n",
    "    #     heatmap_english = heatmap_data.loc[\"English\"]\n",
    "        \n",
    "\n",
    "    #     # Create subplots\n",
    "    #     fig = make_subplots(\n",
    "    #         rows=1, cols=2,  # 1 row, 2 columns\n",
    "    #         subplot_titles=[\"Arabic\", \"English\"],  # Titles above each subplot\n",
    "    #         shared_yaxes=True,  # Align y-axis labels\n",
    "    #         horizontal_spacing=0.15  # Adjust space between heatmaps\n",
    "    #     )\n",
    "\n",
    "    #     # Add Arabic heatmap\n",
    "    #     fig.add_trace(\n",
    "    #         go.Heatmap(\n",
    "    #             z=heatmap_arabic.values,\n",
    "    #             x=heatmap_arabic.columns,\n",
    "    #             y=heatmap_arabic.index,\n",
    "    #             colorscale=\"RdBu_r\",\n",
    "    #             text=heatmap_arabic.round(2).astype(str),  # Convert confidence values to text\n",
    "    #             texttemplate=\"%{text}\",  # Format text display\n",
    "    #             colorbar=dict(title=\"Confidence\"),\n",
    "    #             # zmin=0, zmax=1,  # Keep scale consistent\n",
    "    #         ),\n",
    "    #         row=1, col=1\n",
    "    #     )\n",
    "    #     # Add English heatmap\n",
    "    #     fig.add_trace(\n",
    "    #         go.Heatmap(\n",
    "    #             z=heatmap_english.values,\n",
    "    #             x=heatmap_english.columns,\n",
    "    #             y=heatmap_english.index,\n",
    "    #             colorscale=\"RdBu_r\",\n",
    "    #             text=heatmap_english.round(2).astype(str),  # Convert confidence values to text\n",
    "    #             texttemplate=\"%{text}\",  # Format text display\n",
    "    #             colorbar=dict(title=\"Confidence\"),\n",
    "    #             # zmin=0, zmax=1,  # Keep scale consistent\n",
    "    #         ),\n",
    "    #         row=1, col=2\n",
    "    #     )\n",
    "\n",
    "    #     # Update layout for better display\n",
    "    #     fig.update_layout(\n",
    "    #         title_text=\"Misclassification Heatmap - Arabic vs. English\",\n",
    "    #         width=1200, height=600,  # Adjust size\n",
    "    #         template=\"plotly_white\",\n",
    "    #         xaxis_tickangle=-45\n",
    "    #     )\n",
    "\n",
    "\n",
    "    #     fig.show()\n",
    "\n",
    "    def detailed_entity_level_errors(self):\n",
    "        \"\"\"Perform entity-level confidence analysis (Token Confidence only).\"\"\"\n",
    "        variables = self.VARIABLES[\"Entity-Level Confidence\"]\n",
    "        df = self.analysis_df.copy()\n",
    "        df = df[df['Agreements'] == False].copy()\n",
    "        df = self.filter_and_standardize(df, variables + ['Pred Labels'])\n",
    "\n",
    "        # Group by 'True Labels' and 'Pred Labels' and calculate weighted average\n",
    "        grouped_df = df.groupby(['Language', 'True Labels', 'Pred Labels'])[['Token Confidence']].apply(\n",
    "            lambda x: pd.Series({\n",
    "                'Weighted Token Confidence': (x['Token Confidence'] * x['Token Confidence'].count()).sum() / x['Token Confidence'].count(),\n",
    "                'Count': x['Token Confidence'].count()  # Add count for reference\n",
    "            })\n",
    "        ).reset_index()\n",
    "\n",
    "        # Pivot to get heatmap format\n",
    "        heatmap_data = grouped_df.pivot_table(\n",
    "            index=['Language', 'True Labels'],\n",
    "            columns='Pred Labels',\n",
    "            values='Weighted Token Confidence',\n",
    "            fill_value=0\n",
    "        )\n",
    "\n",
    "        heatmap_arabic = heatmap_data.loc[\"Arabic\"]\n",
    "        heatmap_english = heatmap_data.loc[\"English\"]\n",
    "\n",
    "        # Get global min and max values for consistent scaling\n",
    "        zmin, zmax = heatmap_data.min().min(), heatmap_data.max().max()\n",
    "\n",
    "        # Create subplots\n",
    "        fig = make_subplots(\n",
    "            rows=1, cols=2,\n",
    "            subplot_titles=[\"Arabic\", \"English\"],\n",
    "            shared_yaxes=True,  # Align y-axis labels\n",
    "            horizontal_spacing=0.15\n",
    "        )\n",
    "\n",
    "        # Add Arabic heatmap\n",
    "        fig.add_trace(\n",
    "            go.Heatmap(\n",
    "                z=heatmap_arabic.values,\n",
    "                x=heatmap_arabic.columns,\n",
    "                y=heatmap_arabic.index,\n",
    "                colorscale=\"RdBu_r\",\n",
    "                text=heatmap_arabic.round(2).astype(str),  # Convert confidence values to text\n",
    "                texttemplate=\"%{text}\",  # Format text display\n",
    "                colorbar=dict(title=\"Confidence\"),\n",
    "                zmin=zmin, zmax=zmax  # Set shared scale\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "\n",
    "        # Add English heatmap\n",
    "        fig.add_trace(\n",
    "            go.Heatmap(\n",
    "                z=heatmap_english.values,\n",
    "                x=heatmap_english.columns,\n",
    "                y=heatmap_english.index,\n",
    "                colorscale=\"RdBu_r\",\n",
    "                text=heatmap_english.round(2).astype(str),\n",
    "                texttemplate=\"%{text}\",\n",
    "                colorbar=dict(title=\"Confidence\"),\n",
    "                zmin=zmin, zmax=zmax  # Set shared scale\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "\n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title_text=\"Misclassification Heatmap - Arabic vs. English\",\n",
    "            width=1200, height=600,\n",
    "            template=\"plotly_white\",\n",
    "            xaxis_title=\"Predicted Entity\",  # Label x-axis\n",
    "            yaxis_title=\"True Entity\",  # Label y-axis\n",
    "        )\n",
    "\n",
    "        # Update x-axis labels to tilt for both subplots\n",
    "        fig.update_xaxes(tickangle=-45)  # Tilt x-axis labels for readability\n",
    "\n",
    "        fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = dash_data['ANERCorp_CamelLab_arabertv02'].analysis_data\n",
    "en = dash_data['conll2003_bert'].analysis_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ambiguity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Normalized Token Entropy'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ambiguity_analysis \u001b[38;5;241m=\u001b[39m AmbiguityAnalysis(dash_data)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mambiguity_analysis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze_token_high_level\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# High-level ambiguity analysis\u001b[39;00m\n\u001b[1;32m      3\u001b[0m ambiguity_analysis\u001b[38;5;241m.\u001b[39manalyze_word_high_level()  \u001b[38;5;66;03m# High-level ambiguity analysis\u001b[39;00m\n\u001b[1;32m      4\u001b[0m ambiguity_analysis\u001b[38;5;241m.\u001b[39manalyze_token_entity_level()  \u001b[38;5;66;03m# Entity-level ambiguity analysis\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m, in \u001b[0;36mAmbiguityAnalysis.analyze_token_high_level\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     12\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalysis_df\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Remove OOV entropy (-1) specifically for token-level metrics\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m df \u001b[38;5;241m=\u001b[39m df[(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mall(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m     17\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_summary(df, variables, groupby_cols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     18\u001b[0m summary_long \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_long_data(summary, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAmbiguity Metrics\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Dashboard/venv/lib/python3.10/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/Dashboard/venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Dashboard/venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Normalized Token Entropy'] not in index\""
     ]
    }
   ],
   "source": [
    "ambiguity_analysis = AmbiguityAnalysis(dash_data)\n",
    "ambiguity_analysis.analyze_token_high_level()  # High-level ambiguity analysis\n",
    "ambiguity_analysis.analyze_word_high_level()  # High-level ambiguity analysis\n",
    "ambiguity_analysis.analyze_token_entity_level()  # Entity-level ambiguity analysis\n",
    "ambiguity_analysis.analyze_word_entity_level()  # Entity-level ambiguity analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_analysis = TokenizationAnalysis(dash_data)\n",
    "variable_analysis.analyze_high_level()  # High-level ambiguity analysis\n",
    "variable_analysis.analyze_entity_level()  # Entity-level ambiguity analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_analysis = ConsistencyAnalysis(dash_data)\n",
    "variable_analysis.analyze_high_level()\n",
    "variable_analysis.generate_high_level_table()\n",
    "variable_analysis.analyze_entity_level()\n",
    "variable_analysis.generate_entity_level_table()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_analysis = LossAnalysis(dash_data)\n",
    "loss_analysis.analyze_high_level()\n",
    "loss_analysis.analyze_entity_level()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_analysis = ConfidenceAnalysis(dash_data)\n",
    "confidence_analysis.analyze_high_level_summary()\n",
    "confidence_analysis.analyze_entity_level_summary()\n",
    "# confidence_analysis.analyze_high_level_error()\n",
    "# confidence_analysis.analyze_high_level_correct()\n",
    "# confidence_analysis.analyze_entity_level_error()\n",
    "# confidence_analysis.analyze_entity_level_correct()\n",
    "confidence_analysis.detailed_entity_level_errors()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_analysis = PredictionAnalysis(dash_data)\n",
    "variable_analysis.analyze_high_level()\n",
    "variable_analysis.analyze_entity_level_correct()\n",
    "variable_analysis.analyze_entity_level_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar[(ar['Agreements'] ==  False)\n",
    "         &(ar['True Labels'] ==  'I-MISC')\n",
    "        #  &(ar['Pred Labels'] ==  'O')\n",
    "        ][\n",
    "            # confidence_variables + \n",
    "          ['True Labels', 'Pred Labels', 'Token Confidence', 'Losses', 'Prediction Max Entropy', 'Prediction Entropy', 'Normalized Prediction Entropy', 'Variability']].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en[(en['Agreements'] ==  False)\n",
    "         &(en['True Labels'] ==  'I-MISC')\n",
    "        #  &(ar['Pred Labels'] ==  'O')\n",
    "        ][\n",
    "            # confidence_variables +\n",
    "            ['True Labels', 'Pred Labels', 'Token Confidence', 'Losses', 'Prediction Max Entropy', 'Prediction Entropy', 'Normalized Prediction Entropy', 'Variability']].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_analysis = SilhouetteAnalysis(dash_data)\n",
    "silhouette_analysis.analyze_high_level()\n",
    "silhouette_analysis.analyze_entity_level()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointAnalysis:\n",
    "    \"\"\"Class to perform joint analysis on model variants.\"\"\"\n",
    "\n",
    "    # Define mapping for variant names\n",
    "    VARIANT_MAPPING = {\n",
    "        \"ANERCorp_CamelLab_arabertv02\": \"Arabic\",\n",
    "        \"conll2003_bert\": \"English\"\n",
    "    }\n",
    "\n",
    "    def __init__(self, dash_data):\n",
    "        \"\"\"\n",
    "        Initialize the analysis.\n",
    "\n",
    "        Parameters:\n",
    "        - dash_data (dict): Dictionary containing dataset variants.\n",
    "        \"\"\"\n",
    "        self.dash_data = dash_data\n",
    "        self.analysis_df = self._prepare_data()\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        \"\"\"Prepare and combine all dataset variants into a single DataFrame.\"\"\"\n",
    "        analysis_data = []\n",
    "        for data_name, data_content in self.dash_data.items():\n",
    "            data = data_content.analysis_data.copy()\n",
    "            data[\"Language\"] = self.VARIANT_MAPPING.get(data_name, data_name)  # Map variant names\n",
    "            analysis_data.append(data)\n",
    "        return pd.concat(analysis_data)\n",
    "\n",
    "    def compute_summary(self, data, variables):\n",
    "        \"\"\"Compute mean and standard deviation for each variable per model variant.\"\"\"\n",
    "        summary = data[variables + [\"Language\"]].groupby(\"Language\").agg([\"mean\", \"std\"]).reset_index()\n",
    "\n",
    "        # Flatten column names\n",
    "        summary.columns = [' '.join(col).strip() if isinstance(col, tuple) else col for col in summary.columns]\n",
    "        return summary\n",
    "\n",
    "    def format_long_data(self, summary_df, metric_label):\n",
    "        \"\"\"Convert summary DataFrame into long format for visualization.\"\"\"\n",
    "        mean_cols = [col for col in summary_df.columns if \"mean\" in col]\n",
    "        std_cols = [col.replace(\"mean\", \"std\") for col in mean_cols]\n",
    "\n",
    "        # Melt mean values\n",
    "        mean_long = summary_df.melt(\n",
    "            id_vars=[\"Language\"], \n",
    "            value_vars=mean_cols, \n",
    "            var_name=metric_label, \n",
    "            value_name=\"Mean Value\"\n",
    "        )\n",
    "\n",
    "        # Melt standard deviation values\n",
    "        std_long = summary_df.melt(\n",
    "            id_vars=[\"Language\"], \n",
    "            value_vars=std_cols, \n",
    "            var_name=metric_label, \n",
    "            value_name=\"Std Dev\"\n",
    "        )\n",
    "\n",
    "        # Clean column names (remove \"mean\"/\"std\" suffix from metric names)\n",
    "        mean_long[metric_label] = mean_long[metric_label].str.replace(\" mean\", \"\")\n",
    "        std_long[metric_label] = std_long[metric_label].str.replace(\" std\", \"\")\n",
    "\n",
    "        # Merge mean and std DataFrames\n",
    "        summary_long = mean_long.merge(\n",
    "            std_long, \n",
    "            on=[\"Language\", metric_label], \n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "        # Round values for better readability\n",
    "        summary_long[\"Mean Value\"] = summary_long[\"Mean Value\"].round(2)\n",
    "        summary_long[\"Std Dev\"] = summary_long[\"Std Dev\"].round(2)\n",
    "\n",
    "        # Create text label to display above error bars (mean Â± std)\n",
    "        summary_long[\"Text Label\"] = summary_long.apply(\n",
    "            lambda row: f\"{row['Mean Value']}<br>Â±<br>{row['Std Dev']}\", axis=1\n",
    "        )\n",
    "\n",
    "        return summary_long\n",
    "\n",
    "    def plot_summary(self, summary_long_df, metric_label, title):\n",
    "        \"\"\"Generate a bar plot with error bars representing standard deviation.\"\"\"\n",
    "        fig = px.bar(\n",
    "            summary_long_df,\n",
    "            x=metric_label,\n",
    "            y=\"Mean Value\",\n",
    "            color=\"Language\",\n",
    "            barmode=\"group\",\n",
    "            text=\"Text Label\",  # Show mean values inside bars\n",
    "            title=title,\n",
    "            labels={\"Mean Value\": \"Score\", metric_label: \"Metrics\"},\n",
    "            height=800,\n",
    "            width=1000\n",
    "        )\n",
    "\n",
    "        # Add standard deviation as error bars\n",
    "        fig.update_traces(\n",
    "            error_y=dict(\n",
    "                type=\"data\",\n",
    "                array=summary_long_df[\"Text Label\"],\n",
    "                width=2,\n",
    "                thickness=0.5,\n",
    "                visible=True\n",
    "            ),\n",
    "            # textposition=\"outside\",\n",
    "            textfont=dict(size=12)\n",
    "        )\n",
    "\n",
    "      \n",
    "     \n",
    "        # Adjust layout for readability\n",
    "        fig.update_layout(\n",
    "            template=\"plotly_white\",\n",
    "            showlegend=True,\n",
    "            # xaxis_tickangle=-45\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "\n",
    "    def analyze(self, variables, metric_label=\"Metric\", title=\"Comparison of Variables Across Model Variants\"):\n",
    "        \"\"\"Run the full analysis pipeline.\"\"\"\n",
    "        summary = self.compute_summary(variables)\n",
    "        summary_long = self.format_long_data(summary, metric_label)\n",
    "        self.plot_summary(summary_long, metric_label, title)\n",
    "        \n",
    "    def analyse_group(self):\n",
    "        \n",
    "        # Define variable groups\n",
    "        variable_groups = {\n",
    "            \"Consistency Metrics\": [\"Consistency Ratio\", \"Inconsistency Ratio\"],\n",
    "            \"Ambiguity Metrics\": [\n",
    "                \"Dataset Token Entropy\", \"Dataset Word Entropy\",\n",
    "                \"Normalized Token Entropy\", \"Normalized Word Entropy\"\n",
    "            ],\n",
    "            \"Tokenization Metrics\": ['Tokenization Rate'],\n",
    "            \"Loss Metrics\": ['Losses'],\n",
    "            \"Confidence Metrics\": [\n",
    "                'O Confidence', 'B-LOC Confidence', 'I-LOC Confidence',\n",
    "                'B-PER Confidence', 'I-PER Confidence', \n",
    "                'B-ORG Confidence', 'I-ORG Confidence',\n",
    "                'B-MISC Confidence', 'I-MISC Confidence'\n",
    "            ],\n",
    "            \"Prediction Metrics\": [\n",
    "                'Variability', 'Normalized Prediction Entropy'\n",
    "            ],\n",
    "            \"Silhouette Scores\": ['True Silhouette Score', 'Pred Silhouette Score']\n",
    "        }\n",
    "\n",
    "        # Loop through each variable category\n",
    "        for group_name, variables in variable_groups.items():\n",
    "            # Check if any of the requested variables exist in the dataset\n",
    "            relevant_vars = [var for var in variables if var in self.analysis_df.columns]\n",
    "            if not relevant_vars:\n",
    "                continue  # Skip this category if none of its variables exist\n",
    "            \n",
    "            # Handle Ambiguity Metrics separately (Remove -1 from OOV adjustments)\n",
    "            if group_name == \"Ambiguity Metrics\":\n",
    "                temp_df = self.analysis_df.copy()\n",
    "                for var in relevant_vars:\n",
    "                    temp_df = temp_df[temp_df[var] != -1] \n",
    "            else:\n",
    "                temp_df = self.analysis_df  # Use the original data for other metrics\n",
    "\n",
    "            # Temporarily update self.variables for the current category\n",
    "            self.variables = relevant_vars\n",
    "\n",
    "            # Compute summary statistics\n",
    "            summary = self.compute_summary(temp_df, relevant_vars)\n",
    "\n",
    "            # Format data for visualization\n",
    "            summary_long = self.format_long_data(summary, group_name)\n",
    "\n",
    "            # Generate a separate plot for each category\n",
    "            plot_title = f\"Comparison of {group_name} Across Languages\"\n",
    "            self.plot_summary(summary_long, group_name, plot_title)\n",
    "\n",
    "\n",
    "\n",
    "class EntityJointAnalysis:\n",
    "    \"\"\"Class to analyze consistency variables across model variants at the entity level.\"\"\"\n",
    "\n",
    "    # Define mapping for variant names\n",
    "    VARIANT_MAPPING = {\n",
    "        \"ANERCorp_CamelLab_arabertv02\": \"Arabic\",\n",
    "        \"conll2003_bert\": \"English\"\n",
    "    }\n",
    "\n",
    "    def __init__(self, dash_data):\n",
    "        \"\"\"\n",
    "        Initialize the entity-level analysis.\n",
    "\n",
    "        Parameters:\n",
    "        - dash_data (dict): Dictionary containing dataset variants.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.dash_data = dash_data\n",
    "        self.analysis_df = self._prepare_data()\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        \"\"\"Prepare and combine all dataset variants into a single DataFrame.\"\"\"\n",
    "        analysis_data = []\n",
    "        for data_name, data_content in self.dash_data.items():\n",
    "            data = data_content.analysis_data.copy()\n",
    "            data[\"Language\"] = self.VARIANT_MAPPING.get(data_name, data_name)  # Map variant names\n",
    "            analysis_data.append(data)\n",
    "        return pd.concat(analysis_data)\n",
    "\n",
    "    def filter_and_standardize(self, variables):\n",
    "        \"\"\"Filter out unwanted special tokens and standardize entity label names.\"\"\"\n",
    "        df = self.analysis_df[variables + [\"Language\", \"True Labels\"]].copy()\n",
    "        \n",
    "        # Remove special tokens\n",
    "        df = df[~df[\"True Labels\"].isin([\"[SEP]\", \"[CLS]\", \"IGNORED\"])]\n",
    "        \n",
    "        # Standardize label names\n",
    "        df[\"True Labels\"] = df[\"True Labels\"].replace({\"B-PERS\": \"B-PER\", \"I-PERS\": \"I-PER\"})\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def compute_summary(self, df, variables):\n",
    "        \"\"\"Compute mean and standard deviation for each variable per model variant and entity label.\"\"\"\n",
    "        summary = df.groupby([\"Language\", \"True Labels\"])[variables].agg([\"mean\", \"std\"]).reset_index()\n",
    "\n",
    "        # Flatten column names\n",
    "        summary.columns = [' '.join(col).strip() if isinstance(col, tuple) else col for col in summary.columns]\n",
    "        return summary\n",
    "\n",
    "    def format_long_data(self, summary_df, metric_label):\n",
    "        \"\"\"Convert summary DataFrame into long format for visualization.\"\"\"\n",
    "        mean_cols = [col for col in summary_df.columns if \"mean\" in col]\n",
    "        std_cols = [col.replace(\"mean\", \"std\") for col in mean_cols]\n",
    "\n",
    "        # Melt mean values\n",
    "        mean_long = summary_df.melt(\n",
    "            id_vars=[\"Language\", \"True Labels\"],\n",
    "            value_vars=mean_cols,\n",
    "            var_name=metric_label,\n",
    "            value_name=\"Average Value\"\n",
    "        )\n",
    "\n",
    "        # Melt standard deviation values\n",
    "        std_long = summary_df.melt(\n",
    "            id_vars=[\"Language\", \"True Labels\"],\n",
    "            value_vars=std_cols,\n",
    "            var_name=metric_label,\n",
    "            value_name=\"Std Dev\"\n",
    "        )\n",
    "\n",
    "        # Remove \"mean\" and \"std\" suffixes in \"Consistency Metric\" names\n",
    "        mean_long[metric_label] = mean_long[metric_label].str.replace(\" mean\", \"\")\n",
    "        std_long[metric_label] = std_long[metric_label].str.replace(\" std\", \"\")\n",
    "\n",
    "        # Merge mean and std DataFrames\n",
    "        summary_long = mean_long.merge(\n",
    "            std_long, \n",
    "            on=[\"Language\", \"True Labels\", metric_label], \n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "        # Round values for better readability\n",
    "        summary_long[\"Average Value\"] = summary_long[\"Average Value\"].round(3)\n",
    "        summary_long[\"Std Dev\"] = summary_long[\"Std Dev\"].round(3)\n",
    "\n",
    "        # Modify text labels to include Std Dev\n",
    "        summary_long[\"text_label\"] = summary_long.apply(\n",
    "            lambda row: f\"{row['Average Value']} <br>Â±<br>{row['Std Dev']}\", axis=1\n",
    "        )\n",
    "\n",
    "        return summary_long\n",
    "\n",
    "    def plot_summary(self, summary_long_df, metric_label, title):\n",
    "        \"\"\"Generate a bar plot comparing entity-based consistency variables.\"\"\"\n",
    "        fig = px.bar(\n",
    "            summary_long_df,\n",
    "            x=\"True Labels\",\n",
    "            y=\"Average Value\",\n",
    "            color=\"Language\",\n",
    "            facet_row=metric_label,  # Each entity type in a different row\n",
    "            barmode=\"group\",\n",
    "            text=\"text_label\",\n",
    "            title=title,\n",
    "            labels={\"Average Value\": \"Average Score\", metric_label: \"Metrics\"},\n",
    "            height=800,\n",
    "            width=1000\n",
    "        )\n",
    "\n",
    "        # Add standard deviation as error bars\n",
    "        fig.update_traces(\n",
    "            # textposition=\"outside\",  \n",
    "            textfont=dict(size=12),\n",
    "            # insidetextanchor=\"start\",  \n",
    "        )\n",
    "\n",
    "        # Adjust layout for readability\n",
    "        fig.update_layout(\n",
    "            template=\"plotly_white\",\n",
    "            showlegend=True,\n",
    "            margin=dict(t=120, b=80, l=80, r=80),\n",
    "            xaxis_tickangle=-45\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "\n",
    "    def analyze(self, variables, metric_label=\"Metric\", title=\"Comparison of Consistency Variables Across Model Variants and True Labels\"):\n",
    "        \"\"\"Run the full entity-based analysis pipeline.\"\"\"\n",
    "        filtered_df = self.filter_and_standardize(variables)\n",
    "        summary = self.compute_summary(filtered_df, variables)\n",
    "        summary_long = self.format_long_data(summary, metric_label)\n",
    "        self.plot_summary(summary_long, metric_label, title)\n",
    "        \n",
    "    def analyze_group(self):\n",
    "        \"\"\"\n",
    "        Generate separate plots for different categories of entity-level variables.\n",
    "        \"\"\"\n",
    "        # Define variable groups\n",
    "        variable_groups = {\n",
    "            \"Consistency Metrics\": [\"Consistency Ratio\", \"Inconsistency Ratio\"],\n",
    "            \"Ambiguity Metrics - Dataset Level\": [\n",
    "                \"Dataset Token Entropy\", \"Dataset Word Entropy\",\n",
    "            ],\n",
    "            \"Ambiguity Metrics - Token Level\": [\n",
    "                \"Normalized Token Entropy\", \"Normalized Word Entropy\"\n",
    "            ],\n",
    "            \"Tokenization Metrics\": ['Tokenization Rate'],\n",
    "            \"Loss Metrics\": ['Losses'],\n",
    "            # \"Confidence Metrics\": [\n",
    "            #     'Token Confidence'\n",
    "            #     # , 'B-LOC Confidence', 'I-LOC Confidence',\n",
    "            #     # 'B-PER Confidence', 'I-PER Confidence', \n",
    "            #     # 'B-ORG Confidence', 'I-ORG Confidence',\n",
    "            #     # 'B-MISC Confidence', 'I-MISC Confidence'\n",
    "            # ],\n",
    "            \"Prediction Metrics\": [\n",
    "                'Variability', 'Normalized Prediction Entropy'\n",
    "            ],\n",
    "            \"Silhouette Scores\": ['True Silhouette Score', 'Pred Silhouette Score']\n",
    "        }\n",
    "\n",
    "        # Filter and standardize data\n",
    "        \n",
    "\n",
    "        # Loop through each variable category\n",
    "        for group_name, variables in variable_groups.items():\n",
    "            # Filter variables that exist in the dataset\n",
    "            relevant_vars = [var for var in variables if var in self.analysis_df.columns]\n",
    "            if not relevant_vars:\n",
    "                continue  # Skip if none of the variables exist\n",
    "\n",
    "            # Temporarily update self.variables for the current category\n",
    "            self.variables = relevant_vars\n",
    "            \n",
    "            filtered_df = self.filter_and_standardize(relevant_vars)\n",
    "            if \"Ambiguity\" in group_name:\n",
    "                for var in relevant_vars:\n",
    "                    filtered_df = filtered_df[filtered_df[var] != -1]  # Remove OOV entropy (-1)\n",
    "                    \n",
    "            # Compute summary statistics\n",
    "            summary = self.compute_summary(filtered_df, relevant_vars)\n",
    "\n",
    "            # Format data for visualization\n",
    "            summary_long = self.format_long_data(summary, group_name)\n",
    "\n",
    "            # Generate a separate plot for each category\n",
    "            plot_title = f\"Comparison of {group_name} Across Model Variants and True Labels\"\n",
    "            self.plot_summary(summary_long, group_name, plot_title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consistency_variables = [\"Consistency Ratio\", \"Inconsistency Ratio\"]\n",
    "\n",
    "ambiguity_variables = [\n",
    "    # \"Local Token Entropy\", \"Local Word Entropy\",\n",
    "    \"Dataset Token Entropy\", \"Dataset Word Entropy\",\n",
    "    \"Normalized Token Entropy\", \"Normalized Word Entropy\"\n",
    "]\n",
    "\n",
    "tokenization_variables = ['Tokenization Rate']\n",
    "loss_variables = ['Losses']\n",
    "\n",
    "confidence_variables = [\n",
    "    'O Confidence', \n",
    "    'B-LOC Confidence', 'I-LOC Confidence',\n",
    "    'B-PER Confidence', 'I-PER Confidence', \n",
    "    'B-ORG Confidence', 'I-ORG Confidence',\n",
    "\t'B-MISC Confidence', 'I-MISC Confidence'\n",
    "]\n",
    "prediction_variables = [\n",
    "    # 'Prediction Entropy',\n",
    "    'Variability', 'Normalized Prediction Entropy'\n",
    "]\n",
    "\n",
    "silhouette_variables = ['True Silhouette Score', 'Pred Silhouette Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = dash_data['ANERCorp_CamelLab_arabertv02'].analysis_data\n",
    "en = dash_data['conll2003_bert'].analysis_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar[ar['Dataset Word Entropy'] > 0]['Dataset Word Entropy'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en[en['Dataset Word Entropy'] > 0]['Dataset Word Entropy'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en[ambiguity_variables]['Dataset Word Entropy'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.analysis_df[analysis.analysis_df['Dataset Token Entropy']>0].groupby('Language')['Dataset Token Entropy'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.analysis_df[analysis.analysis_df['Dataset Word Entropy']>0].groupby('Language')['Dataset Word Entropy'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = JointAnalysis(dash_data)\n",
    "analysis.analyse_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = EntityJointAnalysis(dash_data)\n",
    "analysis.analyze_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
