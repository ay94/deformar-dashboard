{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "# This appends the directory one level up (the root of your project) to the sys.path.\n",
    "# Modify the path depending on the location of modules you want to import.\n",
    "sys.path.append(os.path.abspath('../../'))\n",
    "\n",
    "from config.config_managers import DashboardConfigManager\n",
    "from dataManager import DataManager\n",
    "from dash import Dash\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ydata_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = Path(\"/Users/ay227/Desktop/Final-Year/Thesis-Experiments/Online-Dashboard-Phase/dashboard-config.yaml\")\n",
    "config_manager = DashboardConfigManager(CONFIG_PATH)\n",
    "dev_config = config_manager.development_config    \n",
    "\n",
    "app = Dash(__name__, suppress_callback_exceptions=True)\n",
    "\n",
    "app_config = config_manager.app_config\n",
    "server = app.server  # Flask server instance for caching\n",
    "variants_data = None\n",
    "\n",
    "data_manager = DataManager(config_manager, server)\n",
    "dash_data = data_manager.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = dash_data['ANERCorp_CamelLab_arabertv02']\n",
    "en = dash_data['conll2003_bert']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_data = ar.analysis_data\n",
    "train_data = ar.train_data\n",
    "attention_similarity_heatmap = ar.attention_similarity_heatmap\n",
    "attention_similarity_matrix = ar.attention_similarity_matrix\n",
    "attention_weights_similarity_heatmap = ar.attention_weights_similarity_heatmap\n",
    "attention_weights_similarity_matrix = ar.attention_weights_similarity_matrix\n",
    "centroids_avg_similarity_matrix = ar.centroids_avg_similarity_matrix\n",
    "entity_non_strict_confusion_data = ar.entity_non_strict_confusion_data\n",
    "\"\"\"\n",
    "These two columns are removed from the analysis for some reason.\n",
    "I believe we relied on the confusion data somehow.\n",
    "\n",
    "There was a bug in the naming of the files â€” the files in the saved\n",
    "results folder are named 'strict'/'non_strict', but in the data manager\n",
    "they were referred to as 'entity_strict', etc.\n",
    "\n",
    "I chose to keep them different to indicate that they are not added to the tool yet.\n",
    "I also think that the reason they were scrapped is because they are not aligned with the confusion data calculations \n",
    "because they are based on the sklearn. I also thing this is part of the quantitative analysis and it is not that important. We could have different configs\n",
    "one for the dashboard which changes the current and another for the quantitative. The quantitative will be added to the dashboard eventually but it is still not a priority now.\n",
    "So we can have a qual dashboard with the same tab structure however we change the naming slightly for the qual tab and then do the changes accordingly\n",
    "actually the misclassifications are calculated based on the token level entity annotation and if you want to confirm look at the entity additions notebook. \n",
    "\"\"\"\n",
    "\n",
    "strict_entity_misclassifications = ar.strict_entity_misclassifications\n",
    "non_strict_entity_misclassifications = ar.non_strict_entity_misclassifications\n",
    "entity_non_strict_report = ar.entity_non_strict_report\n",
    "entity_strict_confusion_data = ar.entity_strict_confusion_data\n",
    "\n",
    "entity_strict_report = ar.entity_strict_report\n",
    "kmeans_results = ar.kmeans_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(analysis_data, title=\"Entity Data Report\", explorative=True)\n",
    "profile.to_notebook_iframe()  # In Jupyter, or use `.to_file(\"report.html\")`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renamed the normalised columns into something less verbose such as Token Entropy and Prediction Uncertainty\n",
    "analysis_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "True Labels\n",
    "Pred Labels\n",
    "Agreements\n",
    "K=3\n",
    "K=4\n",
    "K=9\n",
    "Boundary Clusters\n",
    "Entity Clusters\n",
    "Token Clusters\n",
    "Error Type\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_data['Strict Pred Entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_data[analysis_data['Error Type'] == 'Boundary'].tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strict_entity_misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_data['Strict True Entities'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_df = en\n",
    "analysis_data = lang_df.analysis_data\n",
    "train_data = lang_df.train_data\n",
    "attention_similarity_heatmap = lang_df.attention_similarity_heatmap\n",
    "attention_similarity_matrix = lang_df.attention_similarity_matrix\n",
    "attention_weights_similarity_heatmap = lang_df.attention_weights_similarity_heatmap\n",
    "attention_weights_similarity_matrix = lang_df.attention_weights_similarity_matrix\n",
    "centroids_avg_similarity_matrix = lang_df.centroids_avg_similarity_matrix\n",
    "entity_non_strict_confusion_data = lang_df.entity_non_strict_confusion_data\n",
    "strict_entity_misclassifications = lang_df.strict_entity_misclassifications\n",
    "non_strict_entity_misclassifications = lang_df.non_strict_entity_misclassifications\n",
    "entity_non_strict_report = lang_df.entity_non_strict_report\n",
    "entity_strict_confusion_data = lang_df.entity_strict_confusion_data\n",
    "entity_strict_report = lang_df.entity_strict_report\n",
    "kmeans_results = lang_df.kmeans_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_strict_entity_misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
