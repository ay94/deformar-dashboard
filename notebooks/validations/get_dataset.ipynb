{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 00:35:12 - INFO - PyTorch version 2.2.2 available.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "# This appends the directory one level up (the root of your project) to the sys.path.\n",
    "# Modify the path depending on the location of modules you want to import.\n",
    "sys.path.append(os.path.abspath('../../'))\n",
    "\n",
    "from config.config_managers import DashboardConfigManager\n",
    "from dataManager import DataManager\n",
    "from dash import Dash\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from experiment_utils.utils import FileHandler\n",
    "from flask_caching import Cache\n",
    "from tqdm.autonotebook import tqdm\n",
    "import torch\n",
    "from torch.nn import Module\n",
    "from typing import Optional\n",
    "from transformers import PreTrainedModel\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from experiment_utils.train import DatasetManager\n",
    "from experiment_utils.config_managers import ExtractionConfigManager\n",
    "\n",
    "\n",
    "MODEL_MAP = {\n",
    "    \"ANERCorp_CamelLab_arabertv02\": \"aubmindlab/bert-base-arabertv02\",\n",
    "    \"conll2003_bert\": \"bert-base-cased\",\n",
    "}\n",
    "\n",
    "DATA_MAP = {\n",
    "    \"ANERCorp_CamelLab_arabertv02\": \"ANERCorp_CamelLab\",\n",
    "    \"conll2003_bert\": \"conll2003\",\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DashboardData:\n",
    "    analysis_data: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    train_data: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    kmeans_results: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    results: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    entity_non_strict_report: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    entity_strict_report: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    token_report: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    token_confusion_matrix: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    token_misclassifications: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    entity_non_strict_confusion_data: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    non_strict_entity_misclassifications: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    entity_strict_confusion_data: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    strict_entity_misclassifications: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    centroids_avg_similarity_matrix: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    attention_weights_similarity_heatmap: go.Figure = field(default_factory=go.Figure)\n",
    "    attention_weights_similarity_matrix: np.ndarray = field(\n",
    "        default_factory=lambda: np.array([])\n",
    "    )\n",
    "    attention_similarity_heatmap: go.Figure = field(default_factory=go.Figure)\n",
    "    attention_similarity_matrix: np.ndarray = field(\n",
    "        default_factory=lambda: np.array([])\n",
    "    )\n",
    "    pretrained_model: Optional[PreTrainedModel] = None\n",
    "    fine_tuned_model: Optional[Module] = None\n",
    "    train_dataset: Optional[Module] = None\n",
    "    test_dataset: Optional[Module] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Round float columns to four decimal places\n",
    "        self.round_floats(self.analysis_data)\n",
    "        self.round_floats(self.kmeans_results)\n",
    "        self.round_floats(self.results)\n",
    "\n",
    "        # Convert list to string in the 'Word Pieces' column of analysis_data if it exists\n",
    "        if \"Word Pieces\" in self.analysis_data.columns:\n",
    "            self.analysis_data[\"Word Pieces\"] = self.analysis_data[\"Word Pieces\"].apply(\n",
    "                lambda x: \", \".join(x) if isinstance(x, list) else (\"\" if pd.isna(x) else x)\n",
    "            )\n",
    "        \n",
    "        # Normalize PER Entity Tag\n",
    "        tag_mapping = {\n",
    "            'B-PERS': 'B-PER',\n",
    "            'I-PERS': 'I-PER'\n",
    "        }\n",
    "        \n",
    "        if \"True Labels\" in self.analysis_data.columns:\n",
    "            self.analysis_data[\"True Labels\"] = self.analysis_data[\"True Labels\"].replace(tag_mapping)\n",
    "        if \"Pred Labels\" in self.analysis_data.columns:\n",
    "            self.analysis_data[\"Pred Labels\"] = self.analysis_data[\"Pred Labels\"].replace(tag_mapping)\n",
    "\n",
    "        self.analysis_data[\"Consistency Ratio\"] = np.where(\n",
    "            self.analysis_data[\"Total Train Occurrences\"]\n",
    "            != 0,  # Condition to check for non-zero denominator\n",
    "            self.analysis_data[\"Consistency Count\"]\n",
    "            / self.analysis_data[\n",
    "                \"Total Train Occurrences\"\n",
    "            ],  # Normal calculation if denominator is not zero\n",
    "            0,\n",
    "        )\n",
    "\n",
    "        self.analysis_data[\"Inconsistency Ratio\"] = np.where(\n",
    "            self.analysis_data[\"Total Train Occurrences\"] != 0,\n",
    "            self.analysis_data[\"Inconsistency Count\"]\n",
    "            / self.analysis_data[\"Total Train Occurrences\"],\n",
    "            0,\n",
    "        )\n",
    "        self.analysis_data[\"Confusion Components\"] = DashboardData.classify_ner(\n",
    "            self.analysis_data, \"True Labels\", \"Pred Labels\"\n",
    "        )\n",
    "        self.analysis_data[\n",
    "            \"Token Ambiguity\"\n",
    "        ] = DashboardData.normalized_entropy(\n",
    "            self.analysis_data, \"Local Token Entropy\", \"Token Max Entropy\"\n",
    "        )  # filling 0/0 division as it generates Nan\n",
    "        self.analysis_data[\n",
    "            \"Word Ambiguity\"\n",
    "        ] = DashboardData.normalized_entropy(\n",
    "            self.analysis_data, \"Local Word Entropy\", \"Word Max Entropy\"\n",
    "        )  # filling 0/0 division as it generates Nan\n",
    "        self.analysis_data[\n",
    "            \"Prediction Uncertainty\"\n",
    "        ] = DashboardData.normalized_entropy(\n",
    "            self.analysis_data, \"Prediction Entropy\", \"Prediction Max Entropy\"\n",
    "        )  # filling 0/0 division as it generates Nan\n",
    "\n",
    "\n",
    "    def is_loaded(self, attribute):\n",
    "        \"\"\"Checks if the given attribute is loaded based on its type.\"\"\"\n",
    "        attr_value = getattr(self, attribute)\n",
    "        if isinstance(attr_value, pd.DataFrame):\n",
    "            return not attr_value.empty\n",
    "        elif isinstance(attr_value, go.Figure):\n",
    "            return len(attr_value.data) > 0\n",
    "        elif isinstance(attr_value, np.ndarray):\n",
    "            return attr_value.size > 0\n",
    "        elif isinstance(attr_value, dict):\n",
    "            return bool(attr_value)  # Returns True if the dictionary is non-empty\n",
    "        return False  # Default case if the attribute type is unrecognized\n",
    "\n",
    "    @staticmethod\n",
    "    def round_floats(df):\n",
    "        for col in df.select_dtypes(include=[\"float\"]).columns:\n",
    "            df[col] = df[col].round(4)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_dict(dict_data: Dict[str, Any]):\n",
    "        return DashboardData(**dict_data)\n",
    "\n",
    "    @staticmethod\n",
    "    def normalized_entropy(df, raw_entropy, max_entropy):\n",
    "        result = np.full(df.shape[0], np.nan)\n",
    "        zero_mask = (df[raw_entropy] == 0) & (df[max_entropy] == 0)\n",
    "        result[zero_mask] = 0\n",
    "        negative_one_mask = (df[raw_entropy] == -1) & (df[max_entropy] == -1)\n",
    "        result[negative_one_mask] = -1\n",
    "        valid_mask = (df[max_entropy] != 0) & ~zero_mask & ~negative_one_mask\n",
    "        result[valid_mask] = df[raw_entropy][valid_mask] / df[max_entropy][valid_mask]\n",
    "        zero_div_mask = (df[max_entropy] == 0) & (df[raw_entropy] != 0)\n",
    "        result[zero_div_mask] = 0\n",
    "        return result\n",
    "    \n",
    "    @staticmethod   \n",
    "    def classify_ner(df, true_label_col, pred_label_col):\n",
    "        conditions = [\n",
    "            (df[pred_label_col] == df[true_label_col]) & (df[pred_label_col] != \"O\"),\n",
    "            (df[pred_label_col] != df[true_label_col]) & (df[pred_label_col] != \"O\"),\n",
    "            (df[pred_label_col] != df[true_label_col]) & (df[true_label_col] != \"O\") & (df[pred_label_col] == \"O\"),\n",
    "        ]\n",
    "        choices = [\"TP\", \"FP\", \"FN\"]\n",
    "        return np.select(conditions, choices, default=\"TN\")  # Return only the classification column\n",
    "\n",
    "    \n",
    "class DataLoader:\n",
    "    def __init__(self, config_manager, variant_name):\n",
    "        self.config_manager = config_manager\n",
    "        \n",
    "        self.variant = variant_name\n",
    "        self.data_dir = config_manager.data_dir / variant_name\n",
    "        self.dashboard_data = {}\n",
    "\n",
    "    def load(self, file_name, file_config):\n",
    "        file_handler = FileHandler(self.data_dir / file_config[\"folder\"])\n",
    "        file_type = file_config.get(\"type\", None)\n",
    "        file_format = file_config[\"format\"]\n",
    "        file_path = file_handler.file_path / f\"{file_name}.{file_format}\"\n",
    "\n",
    "        try:\n",
    "            if file_path.exists():\n",
    "                match file_format:\n",
    "                    case \"npy\": \n",
    "                        return file_handler.load_numpy(file_path.with_suffix(\".npy\"))\n",
    "\n",
    "                    # Handle regular JSON data files\n",
    "                    case \"json\":\n",
    "                        # Check if there's a specific type of JSON handling required\n",
    "                        if file_type and file_type =='dict':\n",
    "                            return file_handler.load_json(file_path)\n",
    "                        elif file_type and file_type =='index':\n",
    "                            return pd.read_json(file_path, orient='index')\n",
    "                        else:\n",
    "                            data = file_handler.read_json(file_path)\n",
    "                        if (\n",
    "                            \"column_mappings\" in file_config\n",
    "                            and file_config[\"column_mappings\"]\n",
    "                        ):\n",
    "                            data = self.apply_column_mappings(\n",
    "                                data, file_config[\"column_mappings\"]\n",
    "                            )\n",
    "                        return data\n",
    "                    case _:\n",
    "                        logging.warning(\"File does not exist: %s\", file_path)\n",
    "                        \n",
    "        except Exception as e:\n",
    "            logging.error(\"Failed to load data from %s: %s\", file_path, e)\n",
    "            return None\n",
    "\n",
    "    def apply_column_mappings(\n",
    "        self, data: pd.DataFrame, column_mappings: dict\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Rename columns in the DataFrame based on provided mappings.\"\"\"\n",
    "        return data.rename(columns=column_mappings)\n",
    "    \n",
    "    def load_model(self):\n",
    "        \n",
    "        model_name = MODEL_MAP.get(self.variant)\n",
    "\n",
    "        if not model_name:\n",
    "            raise ValueError(f\"No pretrained model mapping found for variant: {self.variant}\")\n",
    "\n",
    "        # Path to the .bin file\n",
    "        model_path = self.data_dir / \"fine_tuning\" / \"model_binary.bin\"\n",
    "\n",
    "        if not model_path.exists():\n",
    "            raise FileNotFoundError(f\"Fine-tuned model not found at: {model_path}\")\n",
    "\n",
    "        model = torch.load(model_path, map_location=\"cpu\")\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def load_data(self, split):\n",
    "        extraction_config_dir = self.config_manager.data_dir / self.variant / 'configs/extraction_config.yaml'\n",
    "        \n",
    "        corpora_dir = self.config_manager.corpora_dir\n",
    "\n",
    "        extraction_config = ExtractionConfigManager(extraction_config_dir)\n",
    "\n",
    "        tokenization_config = extraction_config.tokenization_config\n",
    "        data_manager = DatasetManager(\n",
    "                corpora_dir,\n",
    "                DATA_MAP[self.variant],\n",
    "                tokenization_config,\n",
    "                False\n",
    "            )\n",
    "        return data_manager.get_dataset(split)\n",
    "\n",
    "            \n",
    "\n",
    "    def load_all(self):\n",
    "\n",
    "        logging.info(\"Loading Dashboard Data from  %s\", self.data_dir)\n",
    "        for file_name, file_config in tqdm(self.config_manager.data_config.items()):\n",
    "            self.dashboard_data[file_name] = self.load(file_name, file_config)\n",
    "        logging.info('Loading Fine tuned Model')\n",
    "        self.dashboard_data[\"fine_tuned_model\"] = self.load_model()\n",
    "        logging.info('Loading Pre Trained Model')\n",
    "        self.dashboard_data[\"pretrained_model\"] = AutoModelForTokenClassification.from_pretrained(\n",
    "            MODEL_MAP[self.variant]\n",
    "        )\n",
    "        logging.info('Loading Data')\n",
    "        self.dashboard_data[\"train_dataset\"] = self.load_data('train')\n",
    "        self.dashboard_data[\"test_dataset\"] = self.load_data('test')\n",
    "\n",
    "\n",
    "class DataManager:\n",
    "    def __init__(self, config_manager, server) -> None:\n",
    "        self.config_manager = config_manager\n",
    "        self.variants = config_manager.variants\n",
    "        self.cache = Cache(\n",
    "            server,\n",
    "            config={\n",
    "                \"CACHE_TYPE\": \"filesystem\",\n",
    "                \"CACHE_DIR\": \"cache-directory\",\n",
    "                \"CACHE_DEFAULT_TIMEOUT\": 3600,  # Cache timeout of 1 hour\n",
    "            },\n",
    "        )\n",
    "        self.cache.init_app(server)\n",
    "        self.variants_data = self.load_all_variants_from_cache()\n",
    "\n",
    "    def load_all_variants_from_cache(self):\n",
    "        data = {}\n",
    "        for variant in self.variants:\n",
    "            cached_data = self.cache.get(variant)\n",
    "            if cached_data:\n",
    "                data[variant] = cached_data\n",
    "        return data\n",
    "\n",
    "    def load_variant(self, variant):\n",
    "        \"\"\"Loads data for a specific variant, with caching.\"\"\"\n",
    "        cached_data = self.cache.get(variant)\n",
    "        if cached_data is None:\n",
    "            loader = DataLoader(self.config_manager, variant)\n",
    "            loader.load_all()\n",
    "            data = DashboardData.from_dict(loader.dashboard_data)\n",
    "            self.variants_data[variant] = data\n",
    "            self.cache.set(variant, data)  # Cache the newly loaded data\n",
    "            return data  # Return the new data\n",
    "        self.variants_data[variant] = cached_data\n",
    "        return cached_data  # Return the cached data if it was already loaded\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Loads data for all variants using the load_variant method for consistency.\"\"\"\n",
    "        for variant in self.variants:\n",
    "            # Delegate the loading and caching to load_variant method\n",
    "            self.variants_data[variant] = self.load_variant(variant)\n",
    "        return self.variants_data\n",
    "\n",
    "    def is_data_loaded(self):\n",
    "        \"\"\"Checks if all variants have data loaded in the cache.\"\"\"\n",
    "        for variant in self.variants:\n",
    "            if self.cache.get(variant) is None:\n",
    "                return False  # Return False if any variant is not loaded\n",
    "        return True  # Return True if all variants are loaded\n",
    "\n",
    "    def is_any_variant_loaded(self):\n",
    "        \"\"\"\n",
    "        Check if any variant is loaded in the cache.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if at least one variant is loaded, False otherwise.\n",
    "        \"\"\"\n",
    "        for variant in self.variants:\n",
    "            if self.cache.get(variant) is not None:\n",
    "                return True  # Return True if any variant is loaded\n",
    "        return False  # Return False if no variants are loaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 01:01:24 - WARNING - Resolved path does not exist, checking alternative paths: /Users/ay227/Desktop/Final-Year/Thesis-Experiments/Online-Dashboard-Phase/deformer-dashboard/notebooks/validations/My Drive\n",
      "2025-03-25 01:01:24 - INFO - Found Google Drive directory for account ahmed.younes.sam@gmail.com: /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com\n",
      "2025-03-25 01:01:24 - INFO - Loading Dashboard Data from  /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/BaseLineExperiment/ANERCorp_CamelLab_arabertv02\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27eaeedd35cd47b1a8c50a2371f81e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 01:01:28 - INFO - Loading Fine tuned Model\n",
      "2025-03-25 01:01:28 - INFO - Loading Pre Trained Model\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-03-25 01:01:29 - INFO - Loading Data\n",
      "2025-03-25 01:01:29 - WARNING - Resolved path does not exist, checking alternative paths: /Users/ay227/Desktop/Final-Year/Thesis-Experiments/Online-Dashboard-Phase/deformer-dashboard/notebooks/validations/My Drive\n",
      "2025-03-25 01:01:29 - INFO - Found Google Drive directory for account ahmed.younes.sam@gmail.com: /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com\n",
      "2025-03-25 01:01:29 - WARNING - Resolved path does not exist, checking alternative paths: /Users/ay227/Desktop/Final-Year/Thesis-Experiments/Online-Dashboard-Phase/deformer-dashboard/notebooks/validations/My Drive\n",
      "2025-03-25 01:01:29 - INFO - Found Google Drive directory for account ahmed.younes.sam@gmail.com: /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com\n",
      "2025-03-25 01:01:29 - INFO - Tokenization Config validated successfully\n",
      "2025-03-25 01:01:29 - INFO - Loading Preprocessor: aubmindlab/bert-base-arabertv02\n",
      "2025-03-25 01:01:29 - INFO - Loading Tokenizer: aubmindlab/bert-base-arabertv02, lower_case: False\n",
      "2025-03-25 01:01:29 - WARNING - Resolved path does not exist, checking alternative paths: /Users/ay227/Desktop/Final-Year/Thesis-Experiments/Online-Dashboard-Phase/deformer-dashboard/notebooks/validations/My Drive\n",
      "2025-03-25 01:01:29 - INFO - Found Google Drive directory for account ahmed.younes.sam@gmail.com: /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com\n",
      "2025-03-25 01:01:29 - WARNING - Resolved path does not exist, checking alternative paths: /Users/ay227/Desktop/Final-Year/Thesis-Experiments/Online-Dashboard-Phase/deformer-dashboard/notebooks/validations/My Drive\n",
      "2025-03-25 01:01:29 - INFO - Found Google Drive directory for account ahmed.younes.sam@gmail.com: /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com\n",
      "2025-03-25 01:01:29 - INFO - Tokenization Config validated successfully\n",
      "2025-03-25 01:01:30 - INFO - Loading Preprocessor: aubmindlab/bert-base-arabertv02\n",
      "2025-03-25 01:01:30 - INFO - Loading Tokenizer: aubmindlab/bert-base-arabertv02, lower_case: False\n",
      "2025-03-25 01:01:36 - WARNING - Resolved path does not exist, checking alternative paths: /Users/ay227/Desktop/Final-Year/Thesis-Experiments/Online-Dashboard-Phase/deformer-dashboard/notebooks/validations/My Drive\n",
      "2025-03-25 01:01:36 - INFO - Found Google Drive directory for account ahmed.younes.sam@gmail.com: /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com\n",
      "2025-03-25 01:01:36 - INFO - Loading Dashboard Data from  /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/BaseLineExperiment/conll2003_bert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94b85a09f8b42ff947726b8ba8344f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 01:01:44 - INFO - Loading Fine tuned Model\n",
      "2025-03-25 01:02:09 - INFO - Loading Pre Trained Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be40e9ced24d4144893cd5a4e51408ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-03-25 01:02:17 - INFO - Loading Data\n",
      "2025-03-25 01:02:17 - WARNING - Resolved path does not exist, checking alternative paths: /Users/ay227/Desktop/Final-Year/Thesis-Experiments/Online-Dashboard-Phase/deformer-dashboard/notebooks/validations/My Drive\n",
      "2025-03-25 01:02:17 - INFO - Found Google Drive directory for account ahmed.younes.sam@gmail.com: /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com\n",
      "2025-03-25 01:02:17 - WARNING - Resolved path does not exist, checking alternative paths: /Users/ay227/Desktop/Final-Year/Thesis-Experiments/Online-Dashboard-Phase/deformer-dashboard/notebooks/validations/My Drive\n",
      "2025-03-25 01:02:17 - INFO - Found Google Drive directory for account ahmed.younes.sam@gmail.com: /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com\n",
      "2025-03-25 01:02:17 - INFO - Tokenization Config validated successfully\n",
      "2025-03-25 01:02:17 - INFO - Loading Tokenizer: bert-base-cased, lower_case: False\n",
      "2025-03-25 01:02:17 - WARNING - Resolved path does not exist, checking alternative paths: /Users/ay227/Desktop/Final-Year/Thesis-Experiments/Online-Dashboard-Phase/deformer-dashboard/notebooks/validations/My Drive\n",
      "2025-03-25 01:02:17 - INFO - Found Google Drive directory for account ahmed.younes.sam@gmail.com: /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com\n",
      "2025-03-25 01:02:17 - WARNING - Resolved path does not exist, checking alternative paths: /Users/ay227/Desktop/Final-Year/Thesis-Experiments/Online-Dashboard-Phase/deformer-dashboard/notebooks/validations/My Drive\n",
      "2025-03-25 01:02:17 - INFO - Found Google Drive directory for account ahmed.younes.sam@gmail.com: /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com\n",
      "2025-03-25 01:02:17 - INFO - Tokenization Config validated successfully\n",
      "2025-03-25 01:02:18 - INFO - Loading Tokenizer: bert-base-cased, lower_case: False\n"
     ]
    }
   ],
   "source": [
    "CONFIG_PATH = Path(\"/Users/ay227/Desktop/Final-Year/Thesis-Experiments/Online-Dashboard-Phase/analysis-config.yaml\")\n",
    "config_manager = DashboardConfigManager(CONFIG_PATH)\n",
    "dev_config = config_manager.development_config    \n",
    "\n",
    "app = Dash(__name__, suppress_callback_exceptions=True)\n",
    "\n",
    "app_config = config_manager.app_config\n",
    "server = app.server  # Flask server instance for caching\n",
    "variants_data = None\n",
    "\n",
    "data_manager = DataManager(config_manager, server)\n",
    "dash_data = data_manager.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(64000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dash_data['ANERCorp_CamelLab_arabertv02'].pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 00:38:54 - INFO - Tokenization Config validated successfully\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ANERCorp'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variant.split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/ExperimentData')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpora_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANERCorp_CamelLab\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 00:44:28 - INFO - Loading Tokenizer: bert-base-cased, lower_case: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,   585, 19775, 21273, 28494, 28492, 28496, 19775, 28477,   113,\n",
       "           573,   566,   562,   114,   562, 28490, 28495, 17754,   565, 28477,\n",
       "         28480, 25717,   579, 17754, 28475, 28490, 23525,   565, 28495, 28484,\n",
       "         16070, 28475, 19775, 28475, 28477,   585, 16070,   562, 28495, 26259,\n",
       "         21273, 16070, 28475,   565, 26259, 28484,   565, 28495, 28475, 28496,\n",
       "         28495,   562, 17754,   578, 19775, 28494, 28475, 28477,   579, 17754,\n",
       "         28475, 28490, 23525,   565, 28495, 28484, 16070, 28475, 19775, 28475,\n",
       "         28477,   585, 16070,   562, 28495, 26259, 21273, 16070, 28475,   568,\n",
       "         28496, 28475, 28479, 15389,   583, 28475, 26259, 28475,   579, 28490,\n",
       "         28476, 28475,   585, 16070,   582, 28495,   575, 28494, 28496, 18191,\n",
       "           565, 28495, 28484, 28496, 28493,   565, 28495, 18191, 28475, 28481,\n",
       "         28495, 16070, 23525,   592, 28475, 28495, 28486, 25717, 19775, 28475,\n",
       "         28477,   592, 15389, 16070,   568, 28484, 28490, 16070,   588, 21273,\n",
       "           594, 28476, 28495, 28491,   565, 28495, 21273, 28477, 28475, 28479,\n",
       "           571, 28496, 28475, 28495, 16070,   572, 26259, 28484, 23525,   589,\n",
       "         28495, 28475, 16070, 16070, 17754,   577, 16070, 28475, 19775, 23525,\n",
       "           585, 16070,   583, 28475, 26259,  1617,   119,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]),\n",
       " 'labels': tensor([-100,    5, -100, -100, -100, -100, -100, -100, -100,    0, -100,    0,\n",
       "            0, -100,    0, -100, -100, -100,    3, -100, -100, -100,    4, -100,\n",
       "         -100, -100, -100,    4, -100, -100, -100, -100, -100, -100, -100,    0,\n",
       "         -100,    5, -100, -100, -100, -100, -100,    0, -100, -100,    0, -100,\n",
       "         -100, -100, -100,    0, -100,    0, -100, -100, -100, -100,    0, -100,\n",
       "         -100, -100, -100,    0, -100, -100, -100, -100, -100, -100, -100,    0,\n",
       "         -100,    5, -100, -100, -100, -100, -100,    0, -100, -100, -100, -100,\n",
       "            0, -100, -100, -100,    0, -100, -100, -100,    0, -100,    0, -100,\n",
       "            0, -100, -100, -100,    0, -100, -100, -100, -100,    0, -100, -100,\n",
       "         -100, -100, -100, -100, -100,    0, -100, -100, -100, -100, -100, -100,\n",
       "         -100,    0, -100, -100,    0, -100, -100, -100,    0, -100,    0, -100,\n",
       "         -100, -100,    0, -100, -100, -100, -100, -100,    0, -100, -100, -100,\n",
       "         -100,    0, -100, -100, -100,    0, -100, -100, -100, -100, -100,    0,\n",
       "         -100, -100, -100, -100,    0, -100,    0, -100, -100,    0,    0, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_manager.get_dataset('train').__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
