{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "# This appends the directory one level up (the root of your project) to the sys.path.\n",
    "# Modify the path depending on the location of modules you want to import.\n",
    "sys.path.append(os.path.abspath('../'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 09:50:07 - INFO - PyTorch version 2.2.2 available.\n"
     ]
    }
   ],
   "source": [
    "from config.config_managers import DashboardConfigManager\n",
    "from dataManager import DataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = Path(\"/Users/ay227/Desktop/Final-Year/Thesis-Experiments/Online-Dashboard-Phase/dashboard-config.yaml\")\n",
    "config_manager = DashboardConfigManager(CONFIG_PATH)\n",
    "dev_config = config_manager.development_config    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dash import Dash, dcc, html, Output, Input, State\n",
    "app = Dash(__name__, suppress_callback_exceptions=True)\n",
    "\n",
    "app_config = config_manager.app_config\n",
    "server = app.server  # Flask server instance for caching\n",
    "variants_data = None\n",
    "\n",
    "data_manager = DataManager(config_manager, server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 09:50:08 - WARNING - Resolved path does not exist, checking alternative paths: /Users/ay227/Desktop/Final-Year/Thesis-Experiments/Online-Dashboard-Phase/deformer-dashboard/notebooks/My Drive\n",
      "2024-12-10 09:50:08 - INFO - Found Google Drive directory for account ahmed.younes.sam@gmail.com: /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com\n",
      "2024-12-10 09:50:08 - INFO - Loading Dashboard Data from  /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/BaseLineExperiment/ANERCorp_CamelLab_arabertv02\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b241f65dbb402f8d05804a4675018a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 09:50:11 - WARNING - Resolved path does not exist, checking alternative paths: /Users/ay227/Desktop/Final-Year/Thesis-Experiments/Online-Dashboard-Phase/deformer-dashboard/notebooks/My Drive\n",
      "2024-12-10 09:50:11 - INFO - Found Google Drive directory for account ahmed.younes.sam@gmail.com: /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com\n",
      "2024-12-10 09:50:11 - INFO - Loading Dashboard Data from  /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/BaseLineExperiment/conll2003_bert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817d6ed06d894d7a95baa9a3eea8f8e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_data = data_manager.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ANERCorp_CamelLab_arabertv02', 'conll2003_bert'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anercor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence Ids</th>\n",
       "      <th>Token Positions</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Labels</th>\n",
       "      <th>Losses</th>\n",
       "      <th>Token Ids</th>\n",
       "      <th>Global Id</th>\n",
       "      <th>True Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-8.761537</td>\n",
       "      <td>5.038084</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>7270</td>\n",
       "      <td>7270_0_1_3</td>\n",
       "      <td>B-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.028062</td>\n",
       "      <td>-0.254672</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>22961</td>\n",
       "      <td>22961_0_2_0</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.212746</td>\n",
       "      <td>13.185966</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>1528</td>\n",
       "      <td>1528_0_3_7</td>\n",
       "      <td>B-MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.120432</td>\n",
       "      <td>-0.278172</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>1840</td>\n",
       "      <td>1840_0_4_0</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.050218</td>\n",
       "      <td>-0.425187</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>1106</td>\n",
       "      <td>1106_0_5_0</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300669</th>\n",
       "      <td>14039</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.354588</td>\n",
       "      <td>6.421616</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>1210</td>\n",
       "      <td>1210_14039_2_0</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300672</th>\n",
       "      <td>14040</td>\n",
       "      <td>1</td>\n",
       "      <td>-9.390898</td>\n",
       "      <td>5.436679</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>17057</td>\n",
       "      <td>17057_14040_1_3</td>\n",
       "      <td>B-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300673</th>\n",
       "      <td>14040</td>\n",
       "      <td>2</td>\n",
       "      <td>6.466621</td>\n",
       "      <td>-7.676339</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>122</td>\n",
       "      <td>122_14040_2_0</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300674</th>\n",
       "      <td>14040</td>\n",
       "      <td>3</td>\n",
       "      <td>-9.392007</td>\n",
       "      <td>5.417524</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>4617</td>\n",
       "      <td>4617_14040_3_3</td>\n",
       "      <td>B-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300675</th>\n",
       "      <td>14040</td>\n",
       "      <td>4</td>\n",
       "      <td>6.398606</td>\n",
       "      <td>-7.715851</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>123</td>\n",
       "      <td>123_14040_4_0</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203621 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sentence Ids  Token Positions         X          Y  Labels    Losses  \\\n",
       "1                  0                1 -8.761537   5.038084       3  0.000471   \n",
       "2                  0                2  0.028062  -0.254672       0  0.000020   \n",
       "3                  0                3  5.212746  13.185966       7  0.000606   \n",
       "4                  0                4  0.120432  -0.278172       0  0.000020   \n",
       "5                  0                5  0.050218  -0.425187       0  0.000019   \n",
       "...              ...              ...       ...        ...     ...       ...   \n",
       "300669         14039                2 -1.354588   6.421616       0  0.000056   \n",
       "300672         14040                1 -9.390898   5.436679       3  0.000231   \n",
       "300673         14040                2  6.466621  -7.676339       0  0.000021   \n",
       "300674         14040                3 -9.392007   5.417524       3  0.000269   \n",
       "300675         14040                4  6.398606  -7.715851       0  0.000021   \n",
       "\n",
       "        Token Ids        Global Id True Labels  \n",
       "1            7270       7270_0_1_3       B-ORG  \n",
       "2           22961      22961_0_2_0           O  \n",
       "3            1528       1528_0_3_7      B-MISC  \n",
       "4            1840       1840_0_4_0           O  \n",
       "5            1106       1106_0_5_0           O  \n",
       "...           ...              ...         ...  \n",
       "300669       1210   1210_14039_2_0           O  \n",
       "300672      17057  17057_14040_1_3       B-ORG  \n",
       "300673        122    122_14040_2_0           O  \n",
       "300674       4617   4617_14040_3_3       B-ORG  \n",
       "300675        123    123_14040_4_0           O  \n",
       "\n",
       "[203621 rows x 9 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_df = all_data['conll2003_bert'].train_data.copy()\n",
    "tr_df = all_data['conll2003_bert'].train_data.copy()\n",
    "tr_df[tr_df['Labels']!=-100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data['ANERCorp_CamelLab_arabertv02'].analysis_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['conll2003_bert'].analysis_data.columns[~(all_data['conll2003_bert'].analysis_data.columns == all_data['ANERCorp_CamelLab_arabertv02'].analysis_data.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.0004\n",
       "1        0.0001\n",
       "2        0.0005\n",
       "3        0.0003\n",
       "4        0.0004\n",
       "          ...  \n",
       "70362    0.0000\n",
       "70363    0.0000\n",
       "70364    0.9995\n",
       "70365    0.0000\n",
       "70366    0.0003\n",
       "Name: B-PER Confidence, Length: 70367, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['conll2003_bert'].analysis_data['B-PER Confidence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = all_data['ANERCorp_CamelLab_arabertv02'].analysis_data.copy()\n",
    "df = df[df['Labels'] != -100].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Sentence Ids', 'Token Positions', 'Words', 'Tokens', 'Word Pieces',\n",
       "       'Core Tokens', 'True Labels', 'Token Selector Id', 'Pred Labels',\n",
       "       'Agreements', 'X', 'Y', 'Labels', 'Losses', 'Token Ids', 'Global Id',\n",
       "       'True Silhouette Score', 'Pred Silhouette Score', 'K=3',\n",
       "       'Boundary Clusters', 'K=4', 'Entity Clusters', 'K=9', 'Token Clusters',\n",
       "       'Consistency Count', 'Inconsistency Count', 'Total Train Occurrences',\n",
       "       'Local Token Entropy', 'Token Max Entropy', 'Dataset Token Entropy',\n",
       "       'Local Word Entropy', 'Word Max Entropy', 'Dataset Word Entropy',\n",
       "       'Tokenization Rate', 'Error Type', 'O Confidence', 'B-PERS Confidence',\n",
       "       'I-PERS Confidence', 'B-ORG Confidence', 'I-ORG Confidence',\n",
       "       'B-LOC Confidence', 'I-LOC Confidence', 'B-MISC Confidence',\n",
       "       'I-MISC Confidence', 'Prediction Entropy', 'Prediction Max Entropy',\n",
       "       'Token Confidence', 'Variability', 'Pre X', 'Pre Y',\n",
       "       'Strict True Entities', 'Strict Pred Entities', 'True Entities',\n",
       "       'Pred Entities', 'True Aligned Scheme', 'Pred Aligned Scheme',\n",
       "       'Consistency Ratio', 'Inconsistency Ratio', 'Normalized Token Entropy',\n",
       "       'Normalized Word Entropy', 'Normalized Prediction Entropy'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True Entities  True Labels  True Aligned Scheme\n",
       "PERS           I-PERS       False                  57\n",
       "MISC           I-MISC       False                  10\n",
       "ORG            I-ORG        False                   9\n",
       "LOC            I-LOC        False                   8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['Strict True Entities'] == 'O') & (df['True Labels'] != 'O') ][['True Entities', 'True Labels', 'True Aligned Scheme']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pred Labels\n",
       "O         21783\n",
       "I-ORG        18\n",
       "I-MISC       14\n",
       "I-PERS       10\n",
       "I-LOC         4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['Strict Pred Entities'] == 'O')]['Pred Labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pred Labels\n",
       "O    21783\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['Pred Entities'] == 'O')]['Pred Labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True Labels\n",
       "O         21616\n",
       "I-PERS       57\n",
       "I-MISC       10\n",
       "I-ORG         9\n",
       "I-LOC         8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['Strict True Entities'] == 'O')]['True Labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1         B-LOC\n",
       "2         B-LOC\n",
       "3             O\n",
       "4        B-PERS\n",
       "5        I-PERS\n",
       "          ...  \n",
       "29704         O\n",
       "29705    B-PERS\n",
       "29707    I-PERS\n",
       "29708    I-PERS\n",
       "29709         O\n",
       "Name: True Labels, Length: 24991, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['True Labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Local Token Entropy', 'Token Max Entropy', 'Dataset Token Entropy',\n",
       "       'Local Word Entropy', 'Word Max Entropy', 'Dataset Word Entropy',\n",
       "       'Normalized Token Entropy', 'Normalized Word Entropy'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns[(df == -1).any(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = df[df['Variability'] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    22340.000000\n",
       "mean         0.093887\n",
       "std          0.237697\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          0.009250\n",
       "max          1.000000\n",
       "Name: Normalized Token Entropy, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['Normalized Token Entropy'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[~(df == -1).any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \n",
    "- multi token entity by extract entities and scan the length of the indeces and get the longest one\n",
    "- pay attention to rare entities because we have opposite behaviour to that, OOV they tend to be predicted fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence Ids</th>\n",
       "      <th>Token Positions</th>\n",
       "      <th>Words</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Word Pieces</th>\n",
       "      <th>Core Tokens</th>\n",
       "      <th>True Labels</th>\n",
       "      <th>Token Selector Id</th>\n",
       "      <th>Pred Labels</th>\n",
       "      <th>Agreements</th>\n",
       "      <th>...</th>\n",
       "      <th>Strict Pred Entities</th>\n",
       "      <th>True Entities</th>\n",
       "      <th>Pred Entities</th>\n",
       "      <th>True Aligned Scheme</th>\n",
       "      <th>Pred Aligned Scheme</th>\n",
       "      <th>Consistency Ratio</th>\n",
       "      <th>Inconsistency Ratio</th>\n",
       "      <th>Normalized Token Entropy</th>\n",
       "      <th>Normalized Word Entropy</th>\n",
       "      <th>Normalized Prediction Entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>المفرق</td>\n",
       "      <td>المفرق</td>\n",
       "      <td>المفرق</td>\n",
       "      <td>المفرق</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>المفرق@#2@#0</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>LOC</td>\n",
       "      <td>LOC</td>\n",
       "      <td>LOC</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>O</td>\n",
       "      <td>-@#3@#0</td>\n",
       "      <td>O</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>الطراونة</td>\n",
       "      <td>الطر</td>\n",
       "      <td>الطر, ##او, ##نة</td>\n",
       "      <td>الطر</td>\n",
       "      <td>I-PERS</td>\n",
       "      <td>الطر@#5@#0</td>\n",
       "      <td>I-PERS</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>PERS</td>\n",
       "      <td>PERS</td>\n",
       "      <td>PERS</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.591700</td>\n",
       "      <td>0.591700</td>\n",
       "      <td>0.073851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>O</td>\n",
       "      <td>-@#8@#0</td>\n",
       "      <td>O</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>أمر</td>\n",
       "      <td>أمر</td>\n",
       "      <td>أمر</td>\n",
       "      <td>أمر</td>\n",
       "      <td>O</td>\n",
       "      <td>أمر@#9@#0</td>\n",
       "      <td>O</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29703</th>\n",
       "      <td>960</td>\n",
       "      <td>7</td>\n",
       "      <td>الحياة</td>\n",
       "      <td>الحياة</td>\n",
       "      <td>الحياة</td>\n",
       "      <td>الحياة</td>\n",
       "      <td>O</td>\n",
       "      <td>الحياة@#7@#960</td>\n",
       "      <td>O</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.200600</td>\n",
       "      <td>0.200600</td>\n",
       "      <td>0.000379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29704</th>\n",
       "      <td>960</td>\n",
       "      <td>8</td>\n",
       "      <td>الشخصية</td>\n",
       "      <td>الشخصية</td>\n",
       "      <td>الشخصية</td>\n",
       "      <td>الشخصية</td>\n",
       "      <td>O</td>\n",
       "      <td>الشخصية@#8@#960</td>\n",
       "      <td>O</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29707</th>\n",
       "      <td>960</td>\n",
       "      <td>11</td>\n",
       "      <td>بن</td>\n",
       "      <td>بن</td>\n",
       "      <td>بن</td>\n",
       "      <td>بن</td>\n",
       "      <td>I-PERS</td>\n",
       "      <td>بن@#11@#960</td>\n",
       "      <td>I-PERS</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>PERS</td>\n",
       "      <td>PERS</td>\n",
       "      <td>PERS</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.903509</td>\n",
       "      <td>0.096491</td>\n",
       "      <td>0.250269</td>\n",
       "      <td>0.250269</td>\n",
       "      <td>0.020158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29708</th>\n",
       "      <td>960</td>\n",
       "      <td>12</td>\n",
       "      <td>طلال</td>\n",
       "      <td>طلال</td>\n",
       "      <td>طلال</td>\n",
       "      <td>طلال</td>\n",
       "      <td>I-PERS</td>\n",
       "      <td>طلال@#12@#960</td>\n",
       "      <td>I-PERS</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>PERS</td>\n",
       "      <td>PERS</td>\n",
       "      <td>PERS</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.020348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29709</th>\n",
       "      <td>960</td>\n",
       "      <td>13</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>.@#13@#960</td>\n",
       "      <td>O</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20710 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentence Ids  Token Positions     Words   Tokens       Word Pieces  \\\n",
       "2                 0                2    المفرق   المفرق            المفرق   \n",
       "3                 0                3         -        -                 -   \n",
       "5                 0                5  الطراونة     الطر  الطر, ##او, ##نة   \n",
       "8                 0                8         -        -                 -   \n",
       "9                 0                9       أمر      أمر               أمر   \n",
       "...             ...              ...       ...      ...               ...   \n",
       "29703           960                7    الحياة   الحياة            الحياة   \n",
       "29704           960                8   الشخصية  الشخصية           الشخصية   \n",
       "29707           960               11        بن       بن                بن   \n",
       "29708           960               12      طلال     طلال              طلال   \n",
       "29709           960               13         .        .                 .   \n",
       "\n",
       "      Core Tokens True Labels Token Selector Id Pred Labels  Agreements  ...  \\\n",
       "2          المفرق       B-LOC      المفرق@#2@#0       B-LOC        True  ...   \n",
       "3               -           O           -@#3@#0           O        True  ...   \n",
       "5            الطر      I-PERS        الطر@#5@#0      I-PERS        True  ...   \n",
       "8               -           O           -@#8@#0           O        True  ...   \n",
       "9             أمر           O         أمر@#9@#0           O        True  ...   \n",
       "...           ...         ...               ...         ...         ...  ...   \n",
       "29703      الحياة           O    الحياة@#7@#960           O        True  ...   \n",
       "29704     الشخصية           O   الشخصية@#8@#960           O        True  ...   \n",
       "29707          بن      I-PERS       بن@#11@#960      I-PERS        True  ...   \n",
       "29708        طلال      I-PERS     طلال@#12@#960      I-PERS        True  ...   \n",
       "29709           .           O        .@#13@#960           O        True  ...   \n",
       "\n",
       "       Strict Pred Entities  True Entities  Pred Entities  \\\n",
       "2                       LOC            LOC            LOC   \n",
       "3                         O              O              O   \n",
       "5                      PERS           PERS           PERS   \n",
       "8                         O              O              O   \n",
       "9                         O              O              O   \n",
       "...                     ...            ...            ...   \n",
       "29703                     O              O              O   \n",
       "29704                     O              O              O   \n",
       "29707                  PERS           PERS           PERS   \n",
       "29708                  PERS           PERS           PERS   \n",
       "29709                     O              O              O   \n",
       "\n",
       "       True Aligned Scheme  Pred Aligned Scheme Consistency Ratio  \\\n",
       "2                     True                 True          0.000000   \n",
       "3                     True                 True          1.000000   \n",
       "5                     True                 True          0.142857   \n",
       "8                     True                 True          1.000000   \n",
       "9                     True                 True          1.000000   \n",
       "...                    ...                  ...               ...   \n",
       "29703                 True                 True          0.968750   \n",
       "29704                 True                 True          1.000000   \n",
       "29707                 True                 True          0.903509   \n",
       "29708                 True                 True          0.500000   \n",
       "29709                 True                 True          1.000000   \n",
       "\n",
       "       Inconsistency Ratio  Normalized Token Entropy Normalized Word Entropy  \\\n",
       "2                 1.000000                  0.000000                0.000000   \n",
       "3                 0.000000                  0.000000                0.000000   \n",
       "5                 0.857143                  0.591700                0.591700   \n",
       "8                 0.000000                  0.000000                0.000000   \n",
       "9                 0.000000                  0.000000                0.000000   \n",
       "...                    ...                       ...                     ...   \n",
       "29703             0.031250                  0.200600                0.200600   \n",
       "29704             0.000000                  0.000000                0.000000   \n",
       "29707             0.096491                  0.250269                0.250269   \n",
       "29708             0.500000                  1.000000                1.000000   \n",
       "29709             0.000000                  0.000000                0.000000   \n",
       "\n",
       "      Normalized Prediction Entropy  \n",
       "2                          0.056689  \n",
       "3                          0.002177  \n",
       "5                          0.073851  \n",
       "8                          0.002398  \n",
       "9                          0.000158  \n",
       "...                             ...  \n",
       "29703                      0.000379  \n",
       "29704                      0.000284  \n",
       "29707                      0.020158  \n",
       "29708                      0.020348  \n",
       "29709                      0.000252  \n",
       "\n",
       "[20710 rows x 61 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'confusion_matrix': {'LOC': {'TP': 627, 'FP': 76, 'FN': 49},\n",
       "  'ORG': {'TP': 338, 'FP': 105, 'FN': 121},\n",
       "  'MISC': {'TP': 151, 'FP': 54, 'FN': 92},\n",
       "  'PERS': {'TP': 751, 'FP': 99, 'FN': 154}},\n",
       " 'false_negatives': {'LOC': {'O': 29,\n",
       "   'Boundary': 10,\n",
       "   'PERS': 1,\n",
       "   'MISC': 3,\n",
       "   'ORG': 3,\n",
       "   'Entity and Boundary': 3},\n",
       "  'PERS': {'O': 50,\n",
       "   'MISC': 5,\n",
       "   'Boundary': 75,\n",
       "   'ORG': 13,\n",
       "   'LOC': 6,\n",
       "   'Entity and Boundary': 5},\n",
       "  'ORG': {'O': 48,\n",
       "   'Entity and Boundary': 10,\n",
       "   'Boundary': 26,\n",
       "   'MISC': 6,\n",
       "   'PERS': 11,\n",
       "   'LOC': 20},\n",
       "  'MISC': {'O': 46,\n",
       "   'Entity and Boundary': 14,\n",
       "   'LOC': 6,\n",
       "   'Boundary': 17,\n",
       "   'ORG': 9}},\n",
       " 'false_positives': {'LOC': {'O': 15,\n",
       "   'Boundary': 14,\n",
       "   'Entity and Boundary': 15,\n",
       "   'MISC': 6,\n",
       "   'PERS': 6,\n",
       "   'ORG': 20},\n",
       "  'MISC': {'PERS': 5,\n",
       "   'O': 18,\n",
       "   'ORG': 6,\n",
       "   'LOC': 3,\n",
       "   'Boundary': 20,\n",
       "   'Entity and Boundary': 2},\n",
       "  'ORG': {'Boundary': 31,\n",
       "   'Entity and Boundary': 5,\n",
       "   'O': 44,\n",
       "   'MISC': 9,\n",
       "   'PERS': 13,\n",
       "   'LOC': 3},\n",
       "  'PERS': {'Boundary': 51,\n",
       "   'O': 29,\n",
       "   'LOC': 1,\n",
       "   'Entity and Boundary': 7,\n",
       "   'ORG': 11}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['ANERCorp_CamelLab_arabertv02'].entity_non_strict_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_manager' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m analysis_data \u001b[38;5;241m=\u001b[39m \u001b[43mdata_manager\u001b[49m\u001b[38;5;241m.\u001b[39mvariants_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mANERCorp_CamelLab_arabertv02\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39manalysis_data\n\u001b[1;32m      2\u001b[0m errors \u001b[38;5;241m=\u001b[39m analysis_data[analysis_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError Type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo Errors\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_manager' is not defined"
     ]
    }
   ],
   "source": [
    "analysis_data = data_manager.variants_data['ANERCorp_CamelLab_arabertv02'].analysis_data\n",
    "errors = analysis_data[analysis_data['Error Type'] != 'No Errors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from seqeval.scheme import auto_detect, Entities\n",
    "from seqeval.metrics.sequence_labeling import get_entities\n",
    "\n",
    "class EntityAnnotator:\n",
    "    def __init__(self, y_true, y_pred):\n",
    "        \"\"\"Initialize the annotator with true and predicted labels.\"\"\"\n",
    "        self.y_true = y_true\n",
    "        self.y_pred = y_pred\n",
    "        self.scheme = auto_detect(self.y_true, False)\n",
    "    \n",
    "    def extract_entities(self):\n",
    "        \"\"\"Extract entities based on the scheme detected.\"\"\"\n",
    "        self.entities_strict_true = Entities(self.y_true, self.scheme, False)\n",
    "        self.entities_strict_pred = Entities(self.y_pred, self.scheme, False)\n",
    "        self.entities_true = get_entities(self.y_true)\n",
    "        self.entities_pred = get_entities(self.y_pred)\n",
    "\n",
    "    def process_strict_entities(self, y_true, entities_true, sen_id):\n",
    "        \"\"\"Process entities strictly, labeling full spans in a sentence.\"\"\"\n",
    "        max_len = len(y_true[sen_id])\n",
    "        results = ['O'] * max_len\n",
    "        for idx in range(max_len):\n",
    "            for entity in entities_true[sen_id]:\n",
    "                _, t, s, e = entity.to_tuple()\n",
    "                if s == idx and (e-s) > 0:\n",
    "                    for i in range(e-s):\n",
    "                        results[s + i] = t\n",
    "                elif (e-s) == 0:\n",
    "                    results[s] = t\n",
    "        return results\n",
    "    \n",
    "    def process_non_strict_entities(self, y_true, sen_id):\n",
    "        \"\"\"Process entities non-strictly, marking only the start and end of each entity.\"\"\"\n",
    "        max_len = len(y_true[sen_id])\n",
    "        results = ['O'] * max_len\n",
    "        for entity in get_entities(y_true[sen_id]):\n",
    "            t, s, e = entity\n",
    "            if s == e:\n",
    "                # If start and end are the same, only mark the start\n",
    "                results[s] = t\n",
    "            else:\n",
    "                # Mark all indices from start to end inclusive\n",
    "                for i in range(s, e + 1):\n",
    "                    results[i] = t\n",
    "        return results\n",
    "\n",
    "    def process_sentences(self, analysis_data, y_data, entities, label_column, strict=False):\n",
    "        \"\"\"Annotate sentences with entity information, either strictly or non-strictly.\"\"\"\n",
    "        entity_annotations = []\n",
    "        for sentence_id, sentence_df in analysis_data.groupby('Sentence Ids'):\n",
    "            if strict: \n",
    "                results = self.process_strict_entities(y_data, entities, sentence_id)\n",
    "            else:\n",
    "                results = self.process_non_strict_entities(y_data, sentence_id)\n",
    "            original_series = sentence_df[label_column]\n",
    "            is_metadata = original_series.apply(lambda x: x not in ['[CLS]', '[SEP]', 'IGNORED'])\n",
    "            new_series = original_series.copy()\n",
    "            new_series.loc[is_metadata] = results\n",
    "            entity_annotations.append(new_series)\n",
    "        return pd.concat(entity_annotations)\n",
    "\n",
    "    def annotate_entity_info(self, analysis_data):\n",
    "        \"\"\"Add annotated entity information to the analysis data for both true and predicted labels.\"\"\"\n",
    "        self.extract_entities()  # Ensure entities are extracted before processing\n",
    "        analysis_data['Strict True Entities'] = self.process_sentences(analysis_data, self.y_true, self.entities_strict_true.entities, 'True Labels', True)\n",
    "        analysis_data['Strict Pred Entities'] = self.process_sentences(analysis_data, self.y_pred, self.entities_strict_pred.entities, 'Pred Labels', True)\n",
    "        analysis_data['True Entities'] = self.process_sentences(analysis_data, self.y_true, self.entities_true, 'True Labels')\n",
    "        analysis_data['Pred Entities'] = self.process_sentences(analysis_data, self.y_pred, self.entities_pred, 'Pred Labels')\n",
    "        \n",
    "        return analysis_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'analysis_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_entities\u001b[39m(y_data, scheme):\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;66;03m# Replace with the Entities() logic if provided\u001b[39;00m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Entities(y_data, scheme, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 14\u001b[0m core_data \u001b[38;5;241m=\u001b[39m \u001b[43manalysis_data\u001b[49m[analysis_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# y_true = entity_outputs['entity_outputs']['y_true']\u001b[39;00m\n\u001b[1;32m     18\u001b[0m y_true \u001b[38;5;241m=\u001b[39m core_data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentence Ids\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue Labels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mlist\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'analysis_data' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "from seqeval.scheme import auto_detect\n",
    "from seqeval.metrics.sequence_labeling import get_entities\n",
    "from seqeval.scheme import Entities\n",
    "\n",
    "file_name = '/Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/BaseLineExperiment/ANERCorp_CamelLab_arabertv02/fine_tuning/evaluation_metrics.json'\n",
    "with open(file_name, 'r') as file:\n",
    "    entity_outputs = json.load(file)  # Use json.load() to read file, not json.loads()\n",
    "\n",
    "def extract_entities(y_data, scheme):\n",
    "        # Replace with the Entities() logic if provided\n",
    "        return Entities(y_data, scheme, False)\n",
    "core_data = analysis_data[analysis_data['Labels']!= -100].copy()\n",
    "\n",
    "\n",
    "# y_true = entity_outputs['entity_outputs']['y_true']\n",
    "y_true = core_data.groupby('Sentence Ids')['True Labels'].apply(list).tolist()\n",
    "# y_pred = entity_outputs['entity_outputs']['y_pred']\n",
    "y_pred = core_data.groupby('Sentence Ids')['Pred Labels'].apply(list).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m annotator \u001b[38;5;241m=\u001b[39m EntityAnnotator(\u001b[43my_true\u001b[49m, y_pred)\n\u001b[1;32m      2\u001b[0m updated_analysis_data \u001b[38;5;241m=\u001b[39m annotator\u001b[38;5;241m.\u001b[39mannotate_entity_info(analysis_data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_true' is not defined"
     ]
    }
   ],
   "source": [
    "annotator = EntityAnnotator(y_true, y_pred)\n",
    "updated_analysis_data = annotator.annotate_entity_info(analysis_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'updated_analysis_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mupdated_analysis_data\u001b[49m[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue Entities\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTR Entity\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'updated_analysis_data' is not defined"
     ]
    }
   ],
   "source": [
    "updated_analysis_data[['True Entities', 'TR Entity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'updated_analysis_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m (\u001b[43mupdated_analysis_data\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue Entities\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m updated_analysis_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTR Entity\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mvalue_counts()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'updated_analysis_data' is not defined"
     ]
    }
   ],
   "source": [
    "(updated_analysis_data['True Entities'] != updated_analysis_data['TR Entity']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'updated_analysis_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mupdated_analysis_data\u001b[49m[updated_analysis_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue Entities\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m updated_analysis_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStrict True Entities\u001b[39m\u001b[38;5;124m'\u001b[39m]][[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentence Ids\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue Entities\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTR Entity\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'updated_analysis_data' is not defined"
     ]
    }
   ],
   "source": [
    "updated_analysis_data[updated_analysis_data['True Entities'] != updated_analysis_data['Strict True Entities']][['Sentence Ids', 'True Entities', 'TR Entity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_entities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sen_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mget_entities\u001b[49m(y_true[sen_id])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_entities' is not defined"
     ]
    }
   ],
   "source": [
    "sen_id = 15\n",
    "get_entities(y_true[sen_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'updated_analysis_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ts \u001b[38;5;241m=\u001b[39m \u001b[43mupdated_analysis_data\u001b[49m[updated_analysis_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentence Ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m sen_id]\n\u001b[1;32m      3\u001b[0m ts[\u001b[38;5;241m~\u001b[39mts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue Labels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[CLS]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[SEP]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIGNORED\u001b[39m\u001b[38;5;124m'\u001b[39m])][[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentence Ids\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue Entities\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStrict True Entities\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPred Entities\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStrict Pred Entities\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPred Labels\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue Labels\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'updated_analysis_data' is not defined"
     ]
    }
   ],
   "source": [
    "ts = updated_analysis_data[updated_analysis_data['Sentence Ids'] == sen_id]\n",
    "\n",
    "ts[~ts['True Labels'].isin(['[CLS]', '[SEP]', 'IGNORED'])][['Sentence Ids', 'True Entities', 'Strict True Entities', 'Pred Entities', 'Strict Pred Entities','Pred Labels', 'True Labels']].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme = auto_detect(y_true, False)\n",
    "entities_true = extract_entities(y_true)\n",
    "entities_pred = extract_entities(y_pred)\n",
    "true_entities = get_entities(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_strict_entities(y_true, entities_true, sen_id):\n",
    "    \"\"\"\n",
    "    Process entities for a given sentence ID and return a list of results where entities are labeled,\n",
    "    and non-entity indices are marked as 'O'.\n",
    "\n",
    "    Args:\n",
    "    y_true (dict): Dictionary where keys are sentence IDs and values are lists of true labels.\n",
    "    entities_true (dict): Dictionary where keys are sentence IDs and values are lists of entity objects with a to_tuple method.\n",
    "    sen_id (int): Sentence ID to process.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of labels with entities labeled accordingly and other indices as 'O'.\n",
    "    \"\"\"\n",
    "    # Determine the length needed for the results list\n",
    "    max_len = len(y_true[sen_id])\n",
    "\n",
    "    # Initialize results with 'O' for all expected indices\n",
    "    results = ['O'] * max_len\n",
    "\n",
    "    # Iterate over each index in the range of y_true for the given sentence ID\n",
    "    for idx in range(max_len):\n",
    "        # Check for entities at this index\n",
    "        for entity in entities_true[sen_id]:\n",
    "            _, t, s, e = entity.to_tuple()\n",
    "            # Check if the entity starts at this index and has length\n",
    "            if s == idx and (e-s) > 0:\n",
    "                for i in range(e-s):\n",
    "                    results[s + i] = t  # Replace 'some_label' with your intended label\n",
    "            elif (e-s) == 0:\n",
    "                results[s] = t  # Set your intended label for zero-length entities\n",
    "            # If none of the conditions are met, it will retain 'O'\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_annotations = []\n",
    "for sentence_id, sentence_df in analysis_data.groupby('Sentence Ids'):\n",
    "    results = process_strict_entities(y_true, entities_true.entities, sentence_id)\n",
    "    original_series = sentence_df['True Labels']\n",
    "    is_metadata = original_series.apply(lambda x: x not in ['[CLS]', '[SEP]', 'IGNORED'])\n",
    "    new_series = original_series.copy()\n",
    "    new_series.loc[is_metadata] = results\n",
    "    entity_annotations.append(new_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(entity_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_id = 18\n",
    "data = analysis_data[~analysis_data['TR Entity'].isin(['[CLS]', '[SEP]', 'IGNORED'])]\n",
    "sen_data = data[data['Sentence Ids'] == sen_id].reset_index()\n",
    "sen_data[['Sentence Ids', 'True Labels', 'TR Entity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the length needed for the results list\n",
    "max_len = len(y_true[sen_id])\n",
    "\n",
    "# Initialize results with 'O' for all expected indices\n",
    "results = ['O'] * max_len\n",
    "\n",
    "# Iterate over each index in the range of y_true[694]\n",
    "for idx in range(max_len):\n",
    "    # Check for entities at this index\n",
    "    for entity in entities_true.entities[sen_id]:\n",
    "        sen_id, t, s, e = entity.to_tuple()\n",
    "        # Check if the entity starts at this index and has length\n",
    "        if s == idx and (e-s) > 0:\n",
    "            for i in range(e-s):\n",
    "                results[s + i] = t  # Replace 'some_label' with your intended label\n",
    "        elif (e-s) == 0:\n",
    "            results[s] = t  # Again, set your intended label for zero-length entities\n",
    "        # If none of the conditions are met, it will retain 'O'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the length needed for the results list\n",
    "max_len = len(y_true[sen_id])\n",
    "\n",
    "# Initialize results with 'O' for all expected indices\n",
    "lresults = ['O'] * max_len\n",
    "\n",
    "# Iterate over each index in the range of y_true[694]\n",
    "for idx in range(max_len):\n",
    "    # Check for entities at this index\n",
    "    for entity in get_entities(y_true[sen_id]):\n",
    "        t, s, e = entity\n",
    "        # Check if the entity starts at this index and has length\n",
    "        \n",
    "        if s == e:\n",
    "            # If start and end are the same, only mark the start\n",
    "            lresults[s] = t\n",
    "        else:\n",
    "            # Mark all indices from start to end inclusive\n",
    "            for i in range(s, e + 1):\n",
    "                lresults[i] = t\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_series = analysis_data[analysis_data[\"Sentence Ids\"] == sen_id]['True Labels']\n",
    "is_metadata = original_series.apply(lambda x: x not in ['[CLS]', '[SEP]', 'IGNORED'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_series = original_series.copy()\n",
    "non_metadata_indices = original_series.index[is_metadata]\n",
    "new_series.loc[non_metadata_indices] = results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_data['TR Entity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_entities(y_true[sen_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_true.entities[sen_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, e in enumerate(lresults\n",
    "):\n",
    "    print(i, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the length needed for the results list\n",
    "max_len = len(y_true[0])\n",
    "\n",
    "# Initialize results with 'O' for all expected indices\n",
    "results = ['O'] * max_len\n",
    "\n",
    "# Iterate over each index in the range of y_true[694]\n",
    "for idx in range(max_len):\n",
    "    # Check for entities at this index\n",
    "    for entity in entities_true.entities[0]:\n",
    "        sen_id, t, s, e = entity.to_tuple()\n",
    "        # Check if the entity starts at this index and has length\n",
    "        if s == idx and (e-s) > 0:\n",
    "            for i in range(e-s):\n",
    "                results[s + i] = t  # Replace 'some_label' with your intended label\n",
    "        elif (e-s) == 0:\n",
    "            results[s] = t  # Again, set your intended label for zero-length entities\n",
    "        # If none of the conditions are met, it will retain 'O'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors[errors['Error Type'] == 'Type'][['True Labels', 'Pred Labels', 'Error Type']].sample(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors[['True Labels', 'Pred Labels', 'Error Type']].sample(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_error(true_label, pred_label):\n",
    "        # If both are the same, it's correct (no error)\n",
    "        if true_label == pred_label:\n",
    "            return \"No Errors\"\n",
    "        \n",
    "        # Handle cases where one or both labels are 'O'\n",
    "        if true_label == 'O' and pred_label != 'O':\n",
    "            return \"Chunk\"  # False entity predicted\n",
    "        if true_label != 'O' and pred_label == 'O':\n",
    "            return \"Exclusion\"  # Missed entity and chunk boundary\n",
    "        \n",
    "        # Extract entity types without position tags (like \"B-\", \"I-\")\n",
    "        true_entity = true_label.split(\"-\")[-1] if \"-\" in true_label else true_label\n",
    "        pred_entity = pred_label.split(\"-\")[-1] if \"-\" in pred_label else pred_label\n",
    "\n",
    "        # If entity types are different (e.g., LOC vs. PER)\n",
    "        if true_entity != pred_entity:\n",
    "            # If both entity type and position (B- vs I-) are wrong\n",
    "            return \"Type and Chunk\" if true_label[0] != pred_label[0] else \"Type\"\n",
    "\n",
    "        # If entity types are the same but position tags (B- vs I-) are wrong\n",
    "        return \"Chunk\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_name = '/Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/BaseLineExperiment/ANERCorp_CamelLab_arabertv02/fine_tuning/evaluation_metrics.json'\n",
    "with open(file_name, 'r') as file:\n",
    "    entity_outputs = json.load(file)  # Use json.load() to read file, not json.loads()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = entity_outputs['entity_outputs']['y_true']\n",
    "y_pred = entity_outputs['entity_outputs']['y_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, classification_report, f1_score\n",
    "from seqeval.scheme import IOB2\n",
    "\n",
    "print(classification_report(y_true, y_pred, mode='strict', digits=4))\n",
    "\n",
    "f1_score(y_true, y_pred, mode='strict', scheme=IOB2,  average='micro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred, mode=None, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "\n",
    "# Ground truth (true labels)\n",
    "y_true = [\n",
    "    ['O', 'B-PER', 'I-PER', 'O', 'B-LOC', 'O'],\n",
    "    ['O', 'B-ORG', 'I-ORG', 'O']\n",
    "]\n",
    "\n",
    "# Predicted labels (with minor errors)\n",
    "y_pred = [\n",
    "    ['O', 'B-PER', 'O', 'O', 'B-LOC', 'O'],  # Misses I-PER\n",
    "    ['O', 'B-ORG', 'O', 'O']                # Misses I-ORG\n",
    "]\n",
    "\n",
    "# Default mode\n",
    "f1_default = f1_score(y_true, y_pred, average='micro', mode=None)\n",
    "\n",
    "# Strict mode\n",
    "f1_strict = f1_score(y_true, y_pred, average='micro', mode='strict')\n",
    "\n",
    "print(\"Default Mode F1 Score:\", f1_default)\n",
    "print(\"Strict Mode F1 Score:\", f1_strict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, classification_report\n",
    "from seqeval.scheme import IOB1\n",
    "# Define data\n",
    "y_true = [['B-PER', 'I-PER', 'O', 'B-ORG', 'I-ORG', 'O']]\n",
    "y_pred = [['B-PER', 'I-ORG', 'O', 'B-ORG', 'I-ORG', 'O']]\n",
    "\n",
    "# Strict mode, no scheme\n",
    "# f1_no_scheme_strict = f1_score(y_true, y_pred, average='micro', mode='strict', scheme=None)\n",
    "f1_no_scheme_strict = classification_report(y_true, y_pred, mode='strict', zero_division='Warn')\n",
    "no_strict = classification_report(y_true, y_pred, mode=None,)\n",
    "\n",
    "print(\"Strict Mode with No Scheme F1 Score:\", f1_no_scheme_strict)\n",
    "\n",
    "print(\"No Strict Mode with F1 Score:\", no_strict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entity_y_true = get_entities(y_true)\n",
    "entity_y_pred = get_entities(y_pred)\n",
    "\n",
    "# Example usage\n",
    "# conf_matrix = calculate_confusion_matrix([e.to_tuple()[1:] for sen in entities_true.entities for e in sen], [e.to_tuple()[1:] for sen in entities_pred.entities for e in sen])\n",
    "\n",
    "\n",
    "\n",
    "conf_matrix = calculate_confusion_matrix(entity_y_true, entity_y_pred)\n",
    "print(conf_matrix)\n",
    "fn_errors = compute_false_negatives(entity_y_true, entity_y_pred)\n",
    "fp_errors = compute_false_positives(entity_y_true, entity_y_pred)\n",
    "\n",
    "print(\"False Negatives:\", dict(fn_errors))\n",
    "print(\"False Positives:\", dict(fp_errors))\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "entities_true = Entities(y_true, scheme, False)\n",
    "entities_pred = Entities(y_pred, scheme, False)\n",
    "true_entity_type = flatten_strict_entities(entities_true)\n",
    "pred_entity_type = flatten_strict_entities(entities_pred)\n",
    "# Example usage\n",
    "# conf_matrix = calculate_confusion_matrix([e.to_tuple()[1:] for sen in entities_true.entities for e in sen], [e.to_tuple()[1:] for sen in entities_pred.entities for e in sen])\n",
    "\n",
    "\n",
    "\n",
    "conf_matrix = calculate_confusion_matrix(true_entity_type, pred_entity_type)\n",
    "print(conf_matrix)\n",
    "fn_errors = compute_false_negatives(true_entity_type, pred_entity_type)\n",
    "fp_errors = compute_false_positives(true_entity_type, pred_entity_type)\n",
    "\n",
    "print(\"False Negatives:\", dict(fn_errors))\n",
    "print(\"False Positives:\", dict(fp_errors))\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_true.entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_pred.entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(no_strict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.scheme import IOB2, Tokens, auto_detect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme = auto_detect(y_true, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = Tokens(['B-PER', 'I-PER', 'O', 'B-LOC'], scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = {\n",
    "#     'y_true': [['O', 'O', 'B-MISC', 'I-MISC', 'B-MISC', 'O', 'O'], ['B-PER', 'I-PER', 'O']],\n",
    "# \t'y_pred': [['O', 'O', 'B-MISC', 'I-LOC', 'B-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics.sequence_labeling import get_entities\n",
    "# Example usage\n",
    "entity_y_true = get_entities(entity_outputs['entity_outputs']['y_true'])\n",
    "entity_y_pred = get_entities(entity_outputs['entity_outputs']['y_pred'])\n",
    "\n",
    "from seqeval.scheme import Entities\n",
    "entities_true = Entities(entity_outputs['entity_outputs']['y_true'], scheme, False)\n",
    "entities_pred = Entities(entity_outputs['entity_outputs']['y_pred'], scheme, False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "def flatten_strict_entities(entities):\n",
    "    return [e.to_tuple()[1:] for sen in entities.entities for e in sen]\n",
    "\n",
    "def calculate_confusion_matrix(y_true, y_pred):\n",
    "    # Initialize confusion matrix data structure\n",
    "    types = set([ent[0] for ent in y_true]).union([ent[0] for ent in y_pred])\n",
    "    confusion_matrix = {typ: {'TP': 0, 'FP': 0, 'FN': 0} for typ in types}\n",
    "\n",
    "\n",
    "    # Track matched predictions to avoid counting them more than once\n",
    "    matched_pred_indices = set()\n",
    "\n",
    "    # Check each true entity against predicted entities\n",
    "    for true_ent in y_true:\n",
    "        true_type, true_start, true_end = true_ent\n",
    "        match_found = False\n",
    "\n",
    "        for idx, pred_ent in enumerate(y_pred):\n",
    "            pred_type, pred_start, pred_end = pred_ent\n",
    "\n",
    "            if idx not in matched_pred_indices and true_type == pred_type and true_start == pred_start and true_end == pred_end:\n",
    "                confusion_matrix[true_type]['TP'] += 1\n",
    "                matched_pred_indices.add(idx)\n",
    "                match_found = True\n",
    "                break\n",
    "        \n",
    "        if not match_found:\n",
    "            confusion_matrix[true_type]['FN'] += 1\n",
    "\n",
    "\n",
    "    # Any unmatched prediction is a false positive\n",
    "    for idx, pred_ent in enumerate(y_pred):\n",
    "        if idx not in matched_pred_indices:\n",
    "            pred_type = pred_ent[0]\n",
    "            confusion_matrix[pred_type]['FP'] += 1\n",
    "\n",
    "    return confusion_matrix\n",
    "\n",
    "entities_true = Entities(entity_outputs['entity_outputs']['y_true'], scheme, False)\n",
    "entities_pred = Entities(entity_outputs['entity_outputs']['y_pred'], scheme, False)\n",
    "true_entity_type = flatten_strict_entities(entities_true)\n",
    "pred_entity_type = flatten_strict_entities(entities_pred)\n",
    "# Example usage\n",
    "# conf_matrix = calculate_confusion_matrix([e.to_tuple()[1:] for sen in entities_true.entities for e in sen], [e.to_tuple()[1:] for sen in entities_pred.entities for e in sen])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = calculate_confusion_matrix(entity_y_true, entity_y_pred)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(627+151+751+338) / ((627+92)+(151+154)+(751+49)+(338+121))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1867 / 2201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = calculate_confusion_matrix(true_entity_type, pred_entity_type)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_metrics = {}\n",
    "for metric in ['TP', 'FP', 'FN']:\n",
    "    total_metrics[metric] = sum(details[metric] for details in conf_matrix.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1976 / (1976+189)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "def compute_false_negatives(y_true, y_pred):\n",
    "    fn_counts = defaultdict(Counter)\n",
    "    true_indexed = {(t[1], t[2]): t[0] for t in y_true}  # Index true entities by boundaries\n",
    "    pred_indexed = {(p[1], p[2]): p[0] for p in y_pred}  # Index predicted entities by boundaries\n",
    "\n",
    "    # Iterate through true entities to find false negatives\n",
    "    for (t_start, t_end), t_type in true_indexed.items():\n",
    "        if (t_start, t_end) not in pred_indexed or pred_indexed[(t_start, t_end)] != t_type:\n",
    "            # No matching prediction or type mismatch at the same position\n",
    "            matched_type = pred_indexed.get((t_start, t_end), 'Boundary')\n",
    "            fn_counts[t_type][matched_type] += 1\n",
    "\n",
    "    return fn_counts\n",
    "\n",
    "def compute_false_positives(y_true, y_pred):\n",
    "    fp_counts = defaultdict(Counter)\n",
    "    true_indexed = {(t[1], t[2]): t[0] for t in y_true}  # Index true entities by boundaries\n",
    "    pred_indexed = {(p[1], p[2]): p[0] for p in y_pred}  # Index predicted entities by boundaries\n",
    "\n",
    "    # Iterate through predicted entities to find false positives\n",
    "    for (p_start, p_end), p_type in pred_indexed.items():\n",
    "        if (p_start, p_end) not in true_indexed or true_indexed[(p_start, p_end)] != p_type:\n",
    "            # No matching true entity or type mismatch at the same position\n",
    "            matched_type = true_indexed.get((p_start, p_end), 'Boundary')\n",
    "            fp_counts[p_type][matched_type] += 1\n",
    "\n",
    "    return fp_counts\n",
    "\n",
    "# Example usage\n",
    "fn_errors = compute_false_negatives(entity_y_true, entity_y_pred)\n",
    "fp_errors = compute_false_positives(entity_y_true, entity_y_pred)\n",
    "\n",
    "print(\"False Negatives:\", dict(fn_errors))\n",
    "print(\"False Positives:\", dict(fp_errors))\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Your original data\n",
    "data = conf_matrix\n",
    "\n",
    "# Prepare lists for DataFrame construction\n",
    "actual = []\n",
    "predicted = []\n",
    "counts = []\n",
    "\n",
    "for (act, pred), count in data.items():\n",
    "    actual.append(act)\n",
    "    predicted.append('None' if pred is None else pred)  # Replace None with 'None' for better visualization\n",
    "    counts.append(count)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'Actual': actual, 'Predicted': predicted, 'Count': counts})\n",
    "\n",
    "# Pivot to format suitable for heatmap\n",
    "pivot_table = df.pivot(index='Actual', columns='Predicted', values='Count').fillna(0)\n",
    "\n",
    "# Generate heatmap\n",
    "fig = px.imshow(pivot_table,\n",
    "                labels=dict(x=\"Predicted Entity Type\", y=\"Actual Entity Type\", color=\"Count\"),\n",
    "                x=pivot_table.columns,\n",
    "                y=pivot_table.index,\n",
    "                text_auto=True,\n",
    "                aspect=\"auto\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Entity Recognition Confusion Matrix\",\n",
    "    xaxis_title=\"Predicted Entity Type\",\n",
    "    yaxis_title=\"Actual Entity Type\"\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Your original data\n",
    "data = conf_matrix1\n",
    "\n",
    "# Prepare lists for DataFrame construction\n",
    "actual = []\n",
    "predicted = []\n",
    "counts = []\n",
    "\n",
    "for (act, pred), count in data.items():\n",
    "    actual.append(act)\n",
    "    predicted.append('None' if pred is None else pred)  # Replace None with 'None' for better visualization\n",
    "    counts.append(count)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'Actual': actual, 'Predicted': predicted, 'Count': counts})\n",
    "\n",
    "# Pivot to format suitable for heatmap\n",
    "pivot_table = df.pivot(index='Actual', columns='Predicted', values='Count').fillna(0)\n",
    "\n",
    "# Generate heatmap\n",
    "fig = px.imshow(pivot_table,\n",
    "                labels=dict(x=\"Predicted Entity Type\", y=\"Actual Entity Type\", color=\"Count\"),\n",
    "                x=pivot_table.columns,\n",
    "                y=pivot_table.index,\n",
    "                text_auto=True,\n",
    "                aspect=\"auto\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Entity Recognition Confusion Matrix\",\n",
    "    xaxis_title=\"Predicted Entity Type\",\n",
    "    yaxis_title=\"Actual Entity Type\"\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITY = 'LOC'\n",
    "entity_false_negatives = {ENTITY: Counter()}\n",
    "false_negatives = set([e for e in entity_y_true if e[0] == 'LOC']) - set([e for e in entity_y_pred if e[0] == 'LOC'])\n",
    "for e in false_negatives:\n",
    "    t_type, t_start, t_end = e\n",
    "    for pred_ent in entity_y_pred:\n",
    "        p_type, p_start, p_end = pred_ent\n",
    "        if t_start == p_start and t_start == p_end:\n",
    "            if p_type == 'LOC':\n",
    "                print(pred_ent)\n",
    "            entity_false_negatives[t_type][p_type]+=1\n",
    "            \n",
    "\n",
    "ENTITY = 'LOC'\n",
    "entity_false_positives = {ENTITY: Counter()}\n",
    "false_positive = set([e for e in entity_y_pred if e[0] == ENTITY]) - set([e for e in entity_y_true if e[0] == ENTITY]) \n",
    "for e in false_positive:\n",
    "    p_type, p_start, p_end = e\n",
    "    for true_ent in entity_y_true:\n",
    "        t_type, t_start, t_end = true_ent\n",
    "        if t_start == p_start and t_end == p_end:\n",
    "            # if p_type == 'ORG':\n",
    "            #     # if t_type == 'ORG':\n",
    "            #         print(true_ent)\n",
    "            if p_type == t_type:\n",
    "                entity_false_positives[p_type][t_type]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 8786\n",
    "for entity in entity_y_true:\n",
    "    t, s, e = entity\n",
    "    if s == id:\n",
    "        print(entity)\n",
    "for entity in entity_y_pred:\n",
    "    t, s, e = entity\n",
    "    if s == id or e == id+1:\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in false_negatives:\n",
    "    t, s, e = entity\n",
    "    # if t == 'LOC':\n",
    "    #     print(entity)\n",
    "    if s == 8786:\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in false_positive:\n",
    "    t, s, e = entity\n",
    "   \n",
    "    if s == 16466 or e == 16467:\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in entity_y_true:\n",
    "    t, s, e = entity\n",
    "    if s == 16963:\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in entity_y_pred:\n",
    "    t, s, e = entity\n",
    "    if s == 16963:\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_data.iloc[8780:8790]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
