{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 22:28:18 - INFO - PyTorch version 2.2.2 available.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from experiment_utils.env_setup import init\n",
    "from experiment_utils.utils import FileHandler\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_utils.utils import FileHandler\n",
    "from experiment_utils.env_setup import init\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Any, List\n",
    "import yaml\n",
    "\n",
    "@dataclass\n",
    "class DevelopmentConfig:\n",
    "    debug: bool = False\n",
    "    port: int = 8000\n",
    "    def __post_init__(self):\n",
    "        if not isinstance(self.debug, bool):\n",
    "            raise ValueError(f\"Expected boolean for debug, got {type(self.debug).__name__}\")\n",
    "        if not (1 <= self.port <= 65535):\n",
    "            raise ValueError(\"Port must be between 1 and 65535\")\n",
    "    @staticmethod\n",
    "    def from_dict(config_dict: Dict[str, Any]):\n",
    "        return DevelopmentConfig(**config_dict)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TabConfig:\n",
    "    tab_value: str\n",
    "    tab_label: str\n",
    "    \n",
    "@dataclass\n",
    "class AppConfig:\n",
    "    tabs: List[TabConfig] = field(default_factory=list)\n",
    "    variants: List[str] = field(default_factory=list)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if not all(isinstance(tab, TabConfig) for tab in self.tabs):\n",
    "            raise ValueError(\"Tabs must be a list of TabConfig instances\")\n",
    "        if not all(isinstance(variant, str) for variant in self.variants):\n",
    "            raise ValueError(\"Variants must be a list of strings\")\n",
    "\n",
    "    @staticmethod\n",
    "    def from_dict(config_dict: Dict[str, Any]):\n",
    "        tabs = [TabConfig(**tab) for tab in config_dict.get('tabs', [])]\n",
    "        variants = config_dict.get('variants', [])\n",
    "        return AppConfig(tabs=tabs, variants=variants)\n",
    "\n",
    "class DashboardConfigManager:\n",
    "    def __init__(self, config_path: Path):\n",
    "        self.config_path = config_path\n",
    "        config_fh = FileHandler(config_path.parent)\n",
    "        try:\n",
    "            self.config = config_fh.load_yaml(config_path.name)\n",
    "        except FileNotFoundError as e:\n",
    "            raise FileNotFoundError(f\"Configuration file not found at {config_path}\") from e\n",
    "        except yaml.YAMLError as e:\n",
    "            raise ValueError(\"Error parsing YAML configuration.\") from e\n",
    "        except ValueError as e:\n",
    "            raise ValueError(\"Validation error in configuration.\") from e\n",
    "\n",
    " \n",
    "    @property\n",
    "    def development_config(self) -> DevelopmentConfig:\n",
    "        return DevelopmentConfig.from_dict(\n",
    "            self.config.get(\"development\", {})\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def app_config(self) -> AppConfig:\n",
    "        return AppConfig.from_dict(\n",
    "            self.config.get(\"dashboard\", {})\n",
    "        )\n",
    "    @property\n",
    "    def data_dir(self) -> Path:\n",
    "        base_folder = init()\n",
    "        return base_folder / self.config.get(\"dashboard\", {}).get('data_dir', '')\n",
    "\n",
    "    @property\n",
    "    def data_config(self) -> Dict:\n",
    "        return self.config.get(\"dashboard\", {}).get(\"dashboard_data\", {}).get(\"data\", {})\n",
    "\n",
    "    @property\n",
    "    def variants(self) -> Dict:\n",
    "        return self.config.get(\"dashboard\", {}).get(\"variants\", {})\n",
    "    \n",
    "    @property\n",
    "    def dataset_tab(self) -> Dict:\n",
    "        return self.config.get(\"dashboard\", {}).get(\"dataset_tab\", {})\n",
    "    \n",
    "    @property\n",
    "    def decision_tab(self) -> Dict:\n",
    "        return self.config.get(\"dashboard\", {}).get(\"decision_tab\", {})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from tqdm.autonotebook import tqdm\n",
    "from experiment_utils.utils import FileHandler\n",
    "import pandas as pd\n",
    "from flask_caching import Cache\n",
    "import numpy as np\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Any\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "@dataclass\n",
    "class DashboardData:\n",
    "    analysis_data: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    train_data: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    kmeans_results: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    results: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    entity_report: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    token_report: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    entity_confusion_data: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    centroids_avg_similarity_matrix: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    attention_weights_similarity_heatmap: go.Figure = field(default_factory=go.Figure)\n",
    "    attention_weights_similarity_matrix: np.ndarray = field(default_factory=lambda: np.array([]))\n",
    "    attention_similarity_heatmap: go.Figure = field(default_factory=go.Figure)\n",
    "    attention_similarity_matrix: np.ndarray = field(default_factory=lambda: np.array([]))\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Round float columns to four decimal places\n",
    "        self.round_floats(self.analysis_data)\n",
    "        self.round_floats(self.kmeans_results)\n",
    "        self.round_floats(self.results)\n",
    "\n",
    "        # Convert list to string in the 'Word Pieces' column of analysis_data if it exists\n",
    "        if 'Word Pieces' in self.analysis_data.columns:\n",
    "            self.analysis_data['Word Pieces'] = self.analysis_data['Word Pieces'].apply(\n",
    "                lambda x: ', '.join(x) if isinstance(x, list) else x\n",
    "            )\n",
    "        \n",
    "        self.analysis_data['Consistency Ratio'] = np.where(\n",
    "            self.analysis_data['Total Train Occurrences'] != 0,  # Condition to check for non-zero denominator\n",
    "            self.analysis_data['Consistency Count'] / self.analysis_data['Total Train Occurrences'],  # Normal calculation if denominator is not zero\n",
    "            0\n",
    "        )\n",
    "\n",
    "        self.analysis_data['Inconsistency Ratio'] = np.where(\n",
    "            self.analysis_data['Total Train Occurrences'] != 0,\n",
    "            self.analysis_data['Inconsistency Count'] / self.analysis_data['Total Train Occurrences'],\n",
    "            0\n",
    "        )\n",
    "        self.analysis_data['Normalized Token Entropy'] = DashboardData.normalized_entropy(self.analysis_data, 'Local Token Entropy', 'Token Max Entropy')  # filling 0/0 division as it generates Nan\n",
    "        self.analysis_data['Normalized Word Entropy'] = DashboardData.normalized_entropy(self.analysis_data, 'Local Token Entropy', 'Token Max Entropy')  # filling 0/0 division as it generates Nan\n",
    "        self.analysis_data['Normalized Prediction Entropy'] = DashboardData.normalized_entropy(self.analysis_data, 'Prediction Entropy', 'Prediction Max Entropy')  # filling 0/0 division as it generates Nan\n",
    "    \n",
    "    def is_loaded(self, attribute):\n",
    "        \"\"\"Checks if the given attribute is loaded based on its type.\"\"\"\n",
    "        attr_value = getattr(self, attribute)\n",
    "        if isinstance(attr_value, pd.DataFrame):\n",
    "            return not attr_value.empty\n",
    "        elif isinstance(attr_value, go.Figure):\n",
    "            return len(attr_value.data) > 0  # Check if the figure has data\n",
    "        return False  # Default case if the attribute type is unrecognized\n",
    "\n",
    "    @staticmethod\n",
    "    def round_floats(df):\n",
    "        for col in df.select_dtypes(include=['float']).columns:\n",
    "            df[col] = df[col].round(4)\n",
    "            \n",
    "    @staticmethod\n",
    "    def from_dict(dict_data: Dict[str, Any]):\n",
    "        return DashboardData(**dict_data)\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalized_entropy(df, raw_entropy, max_entropy):\n",
    "        result = np.full(df.shape[0], np.nan)\n",
    "        zero_mask = (df[raw_entropy] == 0) & (df[max_entropy] == 0)\n",
    "        result[zero_mask] = 0\n",
    "        negative_one_mask = (df[raw_entropy] == -1) & (df[max_entropy] == -1)\n",
    "        result[negative_one_mask] = -1\n",
    "        valid_mask = (df[max_entropy] != 0) & ~zero_mask & ~negative_one_mask\n",
    "        result[valid_mask] = df[raw_entropy][valid_mask] / df[max_entropy][valid_mask]\n",
    "        zero_div_mask = (df[max_entropy] == 0) & (df[raw_entropy] != 0)\n",
    "        result[zero_div_mask] = 0\n",
    "        return result\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, config_manager, variant_name):\n",
    "        self.data_config = config_manager.data_config\n",
    "        self.data_dir = config_manager.data_dir / variant_name\n",
    "        self.dashboard_data = {}\n",
    "\n",
    "    def load(self, file_name, file_config):\n",
    "        file_handler = FileHandler(self.data_dir / file_config['folder'])\n",
    "        file_type = file_config[\"format\"]\n",
    "        file_path = file_handler.file_path / f\"{file_name}.{file_type}\"\n",
    "\n",
    "        try:\n",
    "            if file_path.exists():\n",
    "                # Load Plotly figures specifically\n",
    "                if file_type == \"npy\":\n",
    "                    return file_handler.load_numpy(file_path.with_suffix('.npy'))\n",
    "\n",
    "                # Handle regular JSON data files\n",
    "                elif file_type == \"json\":\n",
    "                    data = file_handler.read_json(file_path)\n",
    "                    if \"column_mappings\" in file_config and file_config[\"column_mappings\"]:\n",
    "                        data = self.apply_column_mappings(data, file_config[\"column_mappings\"])\n",
    "                    return data\n",
    "            else:\n",
    "                logging.warning(\"File does not exist: %s\", file_path)\n",
    "        except Exception as e:\n",
    "            logging.error(\"Failed to load data from %s: %s\", file_path, e)\n",
    "            return None\n",
    "        \n",
    "    def apply_column_mappings(self, data: pd.DataFrame, column_mappings: dict) -> pd.DataFrame:\n",
    "        \"\"\" Rename columns in the DataFrame based on provided mappings. \"\"\"\n",
    "        return data.rename(columns=column_mappings)\n",
    "\n",
    "\n",
    "    def load_all(self):\n",
    "        \n",
    "        logging.info(\"Loading Dashboard Data from  %s\", self.data_dir)\n",
    "        for file_name, file_config in tqdm(self.data_config.items()):\n",
    "            self.dashboard_data[file_name] = self.load(file_name, file_config)\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "class DataManager:\n",
    "    def __init__(self, config_manager, server) -> None:\n",
    "        self.config_manager = config_manager\n",
    "        self.variants = config_manager.variants\n",
    "        self.cache = Cache(server, config={\n",
    "            'CACHE_TYPE': 'filesystem',\n",
    "            'CACHE_DIR': 'cache-directory',\n",
    "            'CACHE_DEFAULT_TIMEOUT': 3600  # Cache timeout of 1 hour\n",
    "        })\n",
    "        self.cache.init_app(server)\n",
    "        self.variants_data = self.load_all_variants_from_cache()\n",
    "    \n",
    "    def load_all_variants_from_cache(self):\n",
    "        data = {}\n",
    "        for variant in self.variants:\n",
    "            cached_data = self.cache.get(variant)\n",
    "            if cached_data:\n",
    "                data[variant] = cached_data\n",
    "        return data\n",
    "\n",
    "    \n",
    "    def load_variant(self, variant):\n",
    "        \"\"\"Loads data for a specific variant, with caching.\"\"\"\n",
    "        cached_data = self.cache.get(variant)\n",
    "        if cached_data is None:\n",
    "            loader = DataLoader(self.config_manager, variant)\n",
    "            loader.load_all()\n",
    "            data = DashboardData.from_dict(loader.dashboard_data)\n",
    "            self.variants_data[variant] = data\n",
    "            self.cache.set(variant, data)  # Cache the newly loaded data\n",
    "            return data  # Return the new data\n",
    "        self.variants_data[variant] = cached_data\n",
    "        return cached_data  # Return the cached data if it was already loaded\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Loads data for all variants using the load_variant method for consistency.\"\"\"\n",
    "        for variant in self.variants:\n",
    "            # Delegate the loading and caching to load_variant method\n",
    "            self.variants_data[variant] = self.load_variant(variant)\n",
    "        return self.variants_data\n",
    "\n",
    "    def is_data_loaded(self):\n",
    "        \"\"\"Checks if all variants have data loaded in the cache.\"\"\"\n",
    "        for variant in self.variants:\n",
    "            if self.cache.get(variant) is None:\n",
    "                return False  # Return False if any variant is not loaded\n",
    "        return True  # Return True if all variants are loaded\n",
    "    \n",
    "    def is_any_variant_loaded(self):\n",
    "        \"\"\"\n",
    "        Check if any variant is loaded in the cache.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if at least one variant is loaded, False otherwise.\n",
    "        \"\"\"\n",
    "        for variant in self.variants:\n",
    "            if self.cache.get(variant) is not None:\n",
    "                return True  # Return True if any variant is loaded\n",
    "        return False  # Return False if no variants are loaded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = Path(\"/Users/ay227/Desktop/Final-Year/Thesis-Experiments/Online-Dashboard-Phase/dashboard-config.yaml\")\n",
    "config_manager = DashboardConfigManager(CONFIG_PATH)\n",
    "dev_config = config_manager.development_config    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dash import Dash, dcc, html, Output, Input, State\n",
    "app = Dash(__name__, suppress_callback_exceptions=True)\n",
    "\n",
    "app_config = config_manager.app_config\n",
    "server = app.server  # Flask server instance for caching\n",
    "variants_data = None\n",
    "\n",
    "data_manager = DataManager(config_manager, server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 22:28:37 - WARNING - Resolved path does not exist, checking alternative paths: /Users/ay227/Desktop/Final-Year/Thesis-Experiments/Online-Dashboard-Phase/deformer-dashboard/notebooks/My Drive\n",
      "2024-08-30 22:28:37 - INFO - Found Google Drive directory for account ahmed.younes.sam@gmail.com: /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com\n",
      "2024-08-30 22:28:37 - INFO - Loading Dashboard Data from  /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/DashboardTest-v0/ANERCorp_CamelLab_arabertv02\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78939ab24ac94914b9e33c1f8df7db67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 22:28:40 - WARNING - Resolved path does not exist, checking alternative paths: /Users/ay227/Desktop/Final-Year/Thesis-Experiments/Online-Dashboard-Phase/deformer-dashboard/notebooks/My Drive\n",
      "2024-08-30 22:28:40 - INFO - Found Google Drive directory for account ahmed.younes.sam@gmail.com: /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com\n",
      "2024-08-30 22:28:40 - INFO - Loading Dashboard Data from  /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/DashboardTest-v0/conll2003_bert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc9726f9ffc4fbe9635bdb5af8052de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 22:28:45 - WARNING - File does not exist: /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/DashboardTest-v0/conll2003_bert/extractions/results/token_report.json\n",
      "2024-08-30 22:28:45 - WARNING - File does not exist: /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Thesis-Experiments/Experiments/DashboardTest-v0/conll2003_bert/extractions/results/entity_report.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ANERCorp_CamelLab_arabertv02': DashboardData(analysis_data=       Sentence Ids  Token Positions     Words    Tokens Word Pieces  \\\n",
       " 0                 0                0     [CLS]     [CLS]       [CLS]   \n",
       " 1                 0                1  الصالحية  الصالحية    الصالحية   \n",
       " 2                 0                2    المفرق    المفرق      المفرق   \n",
       " 3                 0                3         -         -           -   \n",
       " 4                 0                4       غيث       غيث         غيث   \n",
       " ...             ...              ...       ...       ...         ...   \n",
       " 29706           960               10    للوليد     ##ليد  للو, ##ليد   \n",
       " 29707           960               11        بن        بن          بن   \n",
       " 29708           960               12      طلال      طلال        طلال   \n",
       " 29709           960               13         .         .           .   \n",
       " 29710           960               14     [SEP]     [SEP]       [SEP]   \n",
       " \n",
       "       Core Tokens True Labels Token Selector Id Pred Labels  Agreements  ...  \\\n",
       " 0           [CLS]       [CLS]       [CLS]@#0@#0       [CLS]        True  ...   \n",
       " 1        الصالحية       B-LOC    الصالحية@#1@#0       B-LOC        True  ...   \n",
       " 2          المفرق       B-LOC      المفرق@#2@#0       B-LOC        True  ...   \n",
       " 3               -           O           -@#3@#0           O        True  ...   \n",
       " 4             غيث      B-PERS         غيث@#4@#0      B-PERS        True  ...   \n",
       " ...           ...         ...               ...         ...         ...  ...   \n",
       " 29706     IGNORED     IGNORED  IGNORED@#10@#960     IGNORED        True  ...   \n",
       " 29707          بن      I-PERS       بن@#11@#960      I-PERS        True  ...   \n",
       " 29708        طلال      I-PERS     طلال@#12@#960      I-PERS        True  ...   \n",
       " 29709           .           O        .@#13@#960           O        True  ...   \n",
       " 29710       [SEP]       [SEP]    [SEP]@#14@#960       [SEP]        True  ...   \n",
       " \n",
       "        Prediction Max Entropy  Token Confidence  Variability    Pre X  \\\n",
       " 0                      3.1699            0.9999       0.3142   3.2159   \n",
       " 1                      3.1699            0.5035       0.2038   1.4666   \n",
       " 2                      3.1699            0.9719       0.3044   2.7272   \n",
       " 3                      3.1699            0.9998       0.3142   3.4185   \n",
       " 4                      3.1699            0.9858       0.3093   2.5611   \n",
       " ...                       ...               ...          ...      ...   \n",
       " 29706                  3.1699            0.9937       0.3120   3.2931   \n",
       " 29707                  3.1699            0.9705       0.3039   3.2259   \n",
       " 29708                  3.1699            0.9920       0.3115   3.2207   \n",
       " 29709                  3.1699            0.9999       0.3142  -1.4333   \n",
       " 29710                  3.1699            0.9418       0.2938  13.7063   \n",
       " \n",
       "          Pre Y Consistency Ratio  Inconsistency Ratio  \\\n",
       " 0       0.2505          0.000000             0.000000   \n",
       " 1       5.6270          0.000000             0.000000   \n",
       " 2       7.5411          0.000000             1.000000   \n",
       " 3       4.2370          1.000000             0.000000   \n",
       " 4       7.4964          0.000000             0.000000   \n",
       " ...        ...               ...                  ...   \n",
       " 29706   9.5442          0.000000             0.000000   \n",
       " 29707   9.4135          0.903509             0.096491   \n",
       " 29708   9.6970          0.500000             0.500000   \n",
       " 29709   1.3079          1.000000             0.000000   \n",
       " 29710  10.4078          0.000000             0.000000   \n",
       " \n",
       "        Normalized Token Entropy Normalized Word Entropy  \\\n",
       " 0                     -1.000000               -1.000000   \n",
       " 1                     -1.000000               -1.000000   \n",
       " 2                      0.000000                0.000000   \n",
       " 3                      0.000000                0.000000   \n",
       " 4                     -1.000000               -1.000000   \n",
       " ...                         ...                     ...   \n",
       " 29706                 -1.000000               -1.000000   \n",
       " 29707                  0.250269                0.250269   \n",
       " 29708                  1.000000                1.000000   \n",
       " 29709                  0.000000                0.000000   \n",
       " 29710                 -1.000000               -1.000000   \n",
       " \n",
       "       Normalized Prediction Entropy  \n",
       " 0                          0.000473  \n",
       " 1                          0.357109  \n",
       " 2                          0.076122  \n",
       " 3                          0.000820  \n",
       " 4                          0.042273  \n",
       " ...                             ...  \n",
       " 29706                      0.022335  \n",
       " 29707                      0.072400  \n",
       " 29708                      0.026625  \n",
       " 29709                      0.000442  \n",
       " 29710                      0.143916  \n",
       " \n",
       " [29711 rows x 57 columns], train_data=        Sentence Ids  Token Positions          X          Y  Labels    Losses  \\\n",
       " 0                  0                0   9.446189  19.382036    -100  0.000000   \n",
       " 1                  0                1  13.791077  -3.709414       5  0.006694   \n",
       " 2                  0                2  12.713058  12.945978       0  0.000059   \n",
       " 3                  0                3  11.556602  11.655669    -100  0.000000   \n",
       " 4                  0                4   9.195292  14.356680       0  0.000398   \n",
       " ...              ...              ...        ...        ...     ...       ...   \n",
       " 147077          4148               28  10.635909   3.877772       0  0.000046   \n",
       " 147078          4148               29   8.651112   3.159802       0  0.000044   \n",
       " 147079          4148               30   8.473076   3.941707       0  0.000104   \n",
       " 147080          4148               31  -0.578020  15.568364       0  0.000043   \n",
       " 147081          4148               32  18.218866  -5.174151    -100  0.000000   \n",
       " \n",
       "         Token Ids        Global Id True Labels  \n",
       " 0               2       2_0_0_-100     IGNORED  \n",
       " 1           19876      19876_0_1_5       B-LOC  \n",
       " 2              14         14_0_2_0           O  \n",
       " 3             120     120_0_3_-100     IGNORED  \n",
       " 4             113        113_0_4_0           O  \n",
       " ...           ...              ...         ...  \n",
       " 147077       1259   1259_4148_28_0           O  \n",
       " 147078       4537   4537_4148_29_0           O  \n",
       " 147079      10776  10776_4148_30_0           O  \n",
       " 147080         20     20_4148_31_0           O  \n",
       " 147081          3   3_4148_32_-100     IGNORED  \n",
       " \n",
       " [147082 rows x 9 columns], kmeans_results=     K  Homogeneity  Completeness  V Measure\n",
       " 0  k=3       0.6574        0.3904     0.4899\n",
       " 1  k=4       0.6948        0.4378     0.5371\n",
       " 2  k=9       0.7487        0.2878     0.4158, results=   Precision  Recall      F1    Loss    Type  True Score  Pred Score\n",
       " 0     0.8435  0.8121  0.8275  0.1542  Entity      0.6064      0.6497\n",
       " 1     0.8646  0.7977  0.8227  0.1542   Token      0.6064      0.6497, entity_report=        Tag  Precision    Recall        F1  Support\n",
       " 0       LOC   0.889204  0.926036  0.907246      676\n",
       " 1      MISC   0.712195  0.600823  0.651786      243\n",
       " 2       ORG   0.760181  0.732026  0.745838      459\n",
       " 3      PERS   0.880756  0.824309  0.851598      905\n",
       " 4     micro   0.843494  0.812089  0.827494     2283\n",
       " 5     macro   0.810584  0.770799  0.789117     2283\n",
       " 6  weighted   0.841074  0.812089  0.825545     2283, token_report=               Tag  Precision    Recall        F1  Support\n",
       " 0            B-LOC   0.903272  0.950599  0.926331      668\n",
       " 1           B-MISC   0.814433  0.672340  0.736597      235\n",
       " 2            B-ORG   0.810875  0.762222  0.785796      450\n",
       " 3           B-PERS   0.890346  0.870629  0.880377      858\n",
       " 4            I-LOC   0.802326  0.831325  0.816568       83\n",
       " 5           I-MISC   0.858823  0.442424  0.584000      165\n",
       " 6            I-ORG   0.804688  0.749091  0.775895      275\n",
       " 7           I-PERS   0.910798  0.907956  0.909375      641\n",
       " 8                O   0.985895  0.992737  0.989304    21616\n",
       " 9   accuracy/micro   0.971230  0.971230  0.971230    24991\n",
       " 10           macro   0.864606  0.797703  0.822694    24991\n",
       " 11        weighted   0.970274  0.971230  0.970192    24991, entity_confusion_data=     True Entity Pred Entity\n",
       " 0            LOC         LOC\n",
       " 1            LOC         LOC\n",
       " 2           PERS        PERS\n",
       " 3           PERS        PERS\n",
       " 4            LOC         LOC\n",
       " ...          ...         ...\n",
       " 2622        PERS        PERS\n",
       " 2623        MISC        MISC\n",
       " 2624        PERS        PERS\n",
       " 2625        PERS        PERS\n",
       " 2626        PERS        PERS\n",
       " \n",
       " [2627 rows x 2 columns], centroids_avg_similarity_matrix=  NER_Label  Centroid_0  Centroid_1  Centroid_2  Centroid_3  Centroid_4  \\\n",
       " 0     B-LOC    0.000159    0.110372    0.778067    0.238323    0.102824   \n",
       " 1    B-MISC    0.161146    0.276571    0.216935    0.239016    0.138915   \n",
       " 2     B-ORG    0.117952    0.198506    0.279765    0.265838    0.108693   \n",
       " 3    B-PERS    0.105312    0.220289    0.217902    0.147642    0.404442   \n",
       " 4     I-LOC   -0.052538    0.070811    0.417726    0.449961    0.154726   \n",
       " 5    I-MISC    0.151626    0.290172    0.177181    0.440383    0.214565   \n",
       " 6     I-ORG    0.097389    0.190633    0.144026    0.652162    0.236228   \n",
       " 7    I-PERS   -0.058109    0.132747    0.115143    0.310455    0.810527   \n",
       " 8         O    0.763198    0.841878    0.079293    0.194419    0.050664   \n",
       " \n",
       "    Centroid_5  Centroid_6  Centroid_7  Centroid_8  \n",
       " 0    0.105067    0.327364    0.069935    0.213443  \n",
       " 1    0.284259    0.467925    0.232873    0.235491  \n",
       " 2    0.223994    0.658015    0.169506    0.320092  \n",
       " 3    0.179764    0.375379    0.172710    0.786350  \n",
       " 4    0.042193    0.148059    0.025864    0.109649  \n",
       " 5    0.266436    0.255834    0.244708    0.140011  \n",
       " 6    0.190476    0.297658    0.162862    0.113194  \n",
       " 7    0.035190    0.161739    0.061255    0.426182  \n",
       " 8    0.836385    0.235145    0.856317    0.185435  , attention_weights_similarity_heatmap=                                                data  \\\n",
       " 0  [{'coloraxis': 'coloraxis', 'name': '0', 'z': ...   \n",
       " \n",
       "                                               layout  \n",
       " 0  {'template': {'data': {'barpolar': [{'marker':...  , attention_weights_similarity_matrix=array([[0.99985043, 0.99985491, 0.99984048, 0.9998638 , 0.99984272,\n",
       "         0.99985368, 0.99983957, 0.99986179, 0.99985418, 0.9998463 ,\n",
       "         0.99984583, 0.9998297 ],\n",
       "        [0.99984999, 0.9998481 , 0.99983555, 0.9998418 , 0.99984073,\n",
       "         0.99983812, 0.99983663, 0.9998457 , 0.99984184, 0.9998402 ,\n",
       "         0.99983968, 0.99983907],\n",
       "        [0.99984576, 0.99984427, 0.99983585, 0.99984545, 0.99983683,\n",
       "         0.99983681, 0.9998374 , 0.99985395, 0.99984058, 0.99983743,\n",
       "         0.99984185, 0.99983746],\n",
       "        [0.99987574, 0.99987806, 0.99987043, 0.99987492, 0.99987264,\n",
       "         0.99988057, 0.99987154, 0.99987631, 0.99988046, 0.99987404,\n",
       "         0.99987458, 0.9998702 ],\n",
       "        [0.99985626, 0.99986006, 0.99984564, 0.9998525 , 0.99985143,\n",
       "         0.99984769, 0.99984982, 0.99985605, 0.99985166, 0.9998496 ,\n",
       "         0.99984847, 0.9998487 ],\n",
       "        [0.99987014, 0.99986876, 0.99985356, 0.99986212, 0.99986298,\n",
       "         0.9998605 , 0.999859  , 0.99986443, 0.99986072, 0.99985986,\n",
       "         0.99985957, 0.99985823],\n",
       "        [0.99988448, 0.99988749, 0.99987149, 0.9998792 , 0.99987835,\n",
       "         0.99988593, 0.99987644, 0.99988193, 0.99988007, 0.99987947,\n",
       "         0.99987801, 0.99987743],\n",
       "        [0.99986023, 0.99985942, 0.99984376, 0.99984664, 0.99985375,\n",
       "         0.9998493 , 0.99984819, 0.99985394, 0.99985298, 0.99984792,\n",
       "         0.99984808, 0.99984714],\n",
       "        [0.99985273, 0.99985776, 0.99984055, 0.99984288, 0.99984673,\n",
       "         0.99984607, 0.99984495, 0.99985004, 0.99985122, 0.99984849,\n",
       "         0.99984445, 0.99984508],\n",
       "        [0.99987459, 0.99987509, 0.99986218, 0.99986901, 0.9998682 ,\n",
       "         0.99987145, 0.99987012, 0.99987332, 0.99987118, 0.99986902,\n",
       "         0.99986894, 0.99986862],\n",
       "        [0.99985984, 0.99985965, 0.99985293, 0.99985407, 0.99985479,\n",
       "         0.99985882, 0.99986079, 0.99985916, 0.99986204, 0.99986192,\n",
       "         0.99985728, 0.99985579],\n",
       "        [0.99988279, 0.99988332, 0.99987157, 0.99987393, 0.99987817,\n",
       "         0.99987992, 0.99988356, 0.9998824 , 0.99988366, 0.99988175,\n",
       "         0.9998802 , 0.99987837]]), attention_similarity_heatmap=                                                data  \\\n",
       " 0  [{'coloraxis': 'coloraxis', 'name': '0', 'z': ...   \n",
       " \n",
       "                                               layout  \n",
       " 0  {'template': {'data': {'barpolar': [{'marker':...  , attention_similarity_matrix=array([[0.99986648, 0.9966952 , 0.99860119, 0.99998992, 0.99800887,\n",
       "         0.99901466, 0.99788082, 0.99695247, 0.99805348, 0.99815053,\n",
       "         0.9977269 , 0.99808007],\n",
       "        [0.97577252, 0.98099651, 0.9888171 , 0.99686549, 0.99791769,\n",
       "         0.99406816, 0.99780185, 0.98454665, 0.99101431, 0.99308986,\n",
       "         0.9836597 , 0.97807556],\n",
       "        [0.99214519, 0.98240346, 0.9944721 , 0.97599752, 0.99058271,\n",
       "         0.96117443, 0.9950746 , 0.97887396, 0.98032732, 0.99118058,\n",
       "         0.98117481, 0.96982719],\n",
       "        [0.99961323, 0.99762053, 0.94177362, 0.94868995, 0.97310458,\n",
       "         0.97449027, 0.96828292, 0.9751897 , 0.975905  , 0.94243248,\n",
       "         0.97767571, 0.98640166],\n",
       "        [0.94985303, 0.96596976, 0.94609946, 0.96959513, 0.9598639 ,\n",
       "         0.98035952, 0.95122868, 0.96495381, 0.95060498, 0.96218903,\n",
       "         0.93666776, 0.97283731],\n",
       "        [0.96052356, 0.90670016, 0.95180952, 0.95174888, 0.9560535 ,\n",
       "         0.9729397 , 0.94910909, 0.94621873, 0.97063378, 0.95950999,\n",
       "         0.92510013, 0.96913113],\n",
       "        [0.88526424, 0.88243722, 0.9674858 , 0.9588392 , 0.88719972,\n",
       "         0.95720757, 0.93768054, 0.94025236, 0.93438945, 0.95234245,\n",
       "         0.94138772, 0.95432926],\n",
       "        [0.95100794, 0.94717807, 0.92577517, 0.90826391, 0.91249978,\n",
       "         0.93308018, 0.95955525, 0.94398185, 0.88476319, 0.94966536,\n",
       "         0.96055011, 0.94795708],\n",
       "        [0.91623544, 0.93987344, 0.9326963 , 0.83378596, 0.95235388,\n",
       "         0.98790562, 0.89313118, 0.79094253, 0.9012141 , 0.87019206,\n",
       "         0.90893692, 0.93325105],\n",
       "        [0.75465674, 0.97338332, 0.91566299, 0.94264487, 0.88207969,\n",
       "         0.76145924, 0.8820001 , 0.90129685, 0.93881698, 0.92472057,\n",
       "         0.84468754, 0.97662532],\n",
       "        [0.84285481, 0.9433445 , 0.99074068, 0.96978346, 0.93254214,\n",
       "         0.98942883, 0.82054437, 0.89377329, 0.89825307, 0.62678108,\n",
       "         0.94612181, 0.84995791],\n",
       "        [0.97799638, 0.9854097 , 0.98276825, 0.97772921, 0.9784754 ,\n",
       "         0.76919532, 0.98188955, 0.98334463, 0.7713172 , 0.97303129,\n",
       "         0.74536981, 0.87397374]])),\n",
       " 'conll2003_bert': DashboardData(analysis_data=       Sentence Ids  Token Positions    Words   Tokens         Word Pieces  \\\n",
       " 0                 0                0    [CLS]    [CLS]               [CLS]   \n",
       " 1                 0                1   SOCCER        S  S, ##OC, ##CE, ##R   \n",
       " 2                 0                2   SOCCER     ##OC  S, ##OC, ##CE, ##R   \n",
       " 3                 0                3   SOCCER     ##CE  S, ##OC, ##CE, ##R   \n",
       " 4                 0                4   SOCCER      ##R  S, ##OC, ##CE, ##R   \n",
       " ...             ...              ...      ...      ...                 ...   \n",
       " 70362          3452               39  brother  brother             brother   \n",
       " 70363          3452               40        ,        ,                   ,   \n",
       " 70364          3452               41    Bobby    Bobby               Bobby   \n",
       " 70365          3452               42        .        .                   .   \n",
       " 70366          3452               43    [SEP]    [SEP]               [SEP]   \n",
       " \n",
       "       Core Tokens True Labels  Token Selector Id Pred Labels  Agreements  ...  \\\n",
       " 0           [CLS]       [CLS]        [CLS]@#0@#0       [CLS]        True  ...   \n",
       " 1               S           O            S@#1@#0           O        True  ...   \n",
       " 2         IGNORED     IGNORED      IGNORED@#2@#0     IGNORED        True  ...   \n",
       " 3         IGNORED     IGNORED      IGNORED@#3@#0     IGNORED        True  ...   \n",
       " 4         IGNORED     IGNORED      IGNORED@#4@#0     IGNORED        True  ...   \n",
       " ...           ...         ...                ...         ...         ...  ...   \n",
       " 70362     brother           O  brother@#39@#3452           O        True  ...   \n",
       " 70363           ,           O        ,@#40@#3452           O        True  ...   \n",
       " 70364       Bobby       B-PER    Bobby@#41@#3452       B-PER        True  ...   \n",
       " 70365           .           O        .@#42@#3452           O        True  ...   \n",
       " 70366       [SEP]       [SEP]    [SEP]@#43@#3452       [SEP]        True  ...   \n",
       " \n",
       "        Prediction Max Entropy  Token Confidence  Variability    Pre X  \\\n",
       " 0                      3.1699            0.9989       0.3139  12.7496   \n",
       " 1                      3.1699            0.9999       0.3142  10.5721   \n",
       " 2                      3.1699            0.9993       0.3140   1.7940   \n",
       " 3                      3.1699            0.9952       0.3126  -2.4401   \n",
       " 4                      3.1699            0.9998       0.3142   6.4735   \n",
       " ...                       ...               ...          ...      ...   \n",
       " 70362                  3.1699            1.0000       0.3143   3.5682   \n",
       " 70363                  3.1699            1.0000       0.3143  15.0348   \n",
       " 70364                  3.1699            0.9994       0.3141   8.6971   \n",
       " 70365                  3.1699            1.0000       0.3143  -7.8484   \n",
       " 70366                  3.1699            0.9944       0.3123  -7.8839   \n",
       " \n",
       "          Pre Y Consistency Ratio  Inconsistency Ratio  \\\n",
       " 0      12.2885          0.000000             0.000000   \n",
       " 1      -2.9716          0.449393             0.550607   \n",
       " 2     -11.0997          0.000000             0.000000   \n",
       " 3      -4.4242          0.000000             0.000000   \n",
       " 4      -6.9634          0.000000             0.000000   \n",
       " ...        ...               ...                  ...   \n",
       " 70362   1.2847          1.000000             0.000000   \n",
       " 70363  -3.2029          0.997942             0.002058   \n",
       " 70364  -4.6001          1.000000             0.000000   \n",
       " 70365   2.3203          0.998185             0.001815   \n",
       " 70366   2.3142          0.000000             0.000000   \n",
       " \n",
       "        Normalized Token Entropy Normalized Word Entropy  \\\n",
       " 0                     -1.000000               -1.000000   \n",
       " 1                      0.754156                0.754156   \n",
       " 2                     -1.000000               -1.000000   \n",
       " 3                     -1.000000               -1.000000   \n",
       " 4                     -1.000000               -1.000000   \n",
       " ...                         ...                     ...   \n",
       " 70362                  0.000000                0.000000   \n",
       " 70363                  0.010121                0.010121   \n",
       " 70364                  0.000000                0.000000   \n",
       " 70365                  0.009690                0.009690   \n",
       " 70366                 -1.000000               -1.000000   \n",
       " \n",
       "       Normalized Prediction Entropy  \n",
       " 0                          0.004827  \n",
       " 1                          0.000379  \n",
       " 2                          0.003123  \n",
       " 3                          0.018234  \n",
       " 4                          0.001199  \n",
       " ...                             ...  \n",
       " 70362                      0.000189  \n",
       " 70363                      0.000158  \n",
       " 70364                      0.002776  \n",
       " 70365                      0.000158  \n",
       " 70366                      0.020348  \n",
       " \n",
       " [70367 rows x 57 columns], train_data=        Sentence Ids  Token Positions          X          Y  Labels    Losses  \\\n",
       " 0                  0                0   4.764799  -8.054308    -100  0.000000   \n",
       " 1                  0                1  -6.748017   5.602378       3  0.000353   \n",
       " 2                  0                2   0.758872   2.301976       0  0.000028   \n",
       " 3                  0                3  -1.787818  -9.239143       7  0.000467   \n",
       " 4                  0                4   1.128666   2.905388       0  0.000028   \n",
       " ...              ...              ...        ...        ...     ...       ...   \n",
       " 300672         14040                1  -7.254314   5.220507       3  0.000205   \n",
       " 300673         14040                2  -9.996812  -1.560116       0  0.000023   \n",
       " 300674         14040                3  -7.251644   5.241036       3  0.000212   \n",
       " 300675         14040                4 -10.050966  -1.459142       0  0.000022   \n",
       " 300676         14040                5   2.786764  15.792655    -100  0.000000   \n",
       " \n",
       "         Token Ids         Global Id True Labels  \n",
       " 0             101      101_0_0_-100     IGNORED  \n",
       " 1            7270        7270_0_1_3       B-ORG  \n",
       " 2           22961       22961_0_2_0           O  \n",
       " 3            1528        1528_0_3_7      B-MISC  \n",
       " 4            1840        1840_0_4_0           O  \n",
       " ...           ...               ...         ...  \n",
       " 300672      17057   17057_14040_1_3       B-ORG  \n",
       " 300673        122     122_14040_2_0           O  \n",
       " 300674       4617    4617_14040_3_3       B-ORG  \n",
       " 300675        123     123_14040_4_0           O  \n",
       " 300676        102  102_14040_5_-100     IGNORED  \n",
       " \n",
       " [300677 rows x 9 columns], kmeans_results=     K  Homogeneity  Completeness  V Measure\n",
       " 0  k=3       0.8136        0.7785     0.7957\n",
       " 1  k=4       0.6957        0.7339     0.7143\n",
       " 2  k=9       0.8852        0.7759     0.8269, results=   Precision  Recall      F1    Loss    Type  True Score  Pred Score\n",
       " 0     0.9043  0.9182  0.9112  0.1434  Entity      0.8694      0.8955\n",
       " 1     0.8951  0.9157  0.9050  0.1434   Token      0.8694      0.8955, entity_report=None, token_report=None, entity_confusion_data=     True Entity Pred Entity\n",
       " 0            LOC         PER\n",
       " 1              O           O\n",
       " 2              O         PER\n",
       " 3            PER         LOC\n",
       " 4              O           O\n",
       " ...          ...         ...\n",
       " 6192         ORG         ORG\n",
       " 6193         LOC         LOC\n",
       " 6194        MISC           O\n",
       " 6195           O        MISC\n",
       " 6196         PER         PER\n",
       " \n",
       " [6197 rows x 2 columns], centroids_avg_similarity_matrix=  NER_Label  Centroid_0  Centroid_1  Centroid_2  Centroid_3  Centroid_4  \\\n",
       " 0     B-LOC   -0.057194   -0.043105    0.075758   -0.111149    0.065242   \n",
       " 1    B-MISC    0.026084    0.058824    0.023692   -0.188499    0.038958   \n",
       " 2     B-ORG   -0.018843    0.055847    0.862862   -0.034486    0.116450   \n",
       " 3     B-PER   -0.042328    0.021124    0.127866   -0.208938    0.883016   \n",
       " 4     I-LOC   -0.005234   -0.061703   -0.235045    0.063070   -0.123568   \n",
       " 5    I-MISC   -0.025662    0.001543   -0.216025    0.068369   -0.169908   \n",
       " 6     I-ORG    0.042792    0.058185   -0.034477    0.868990   -0.202326   \n",
       " 7     I-PER   -0.030851   -0.127170   -0.178200   -0.015860    0.047371   \n",
       " 8         O    0.960073    0.863047   -0.025999    0.036744   -0.047219   \n",
       " \n",
       "    Centroid_5  Centroid_6  Centroid_7  Centroid_8  \n",
       " 0   -0.060930   -0.121941    0.021203    0.854913  \n",
       " 1   -0.106985    0.721324   -0.139671   -0.058776  \n",
       " 2   -0.167609   -0.059115   -0.250251    0.073515  \n",
       " 3    0.043638   -0.030927   -0.130684    0.071263  \n",
       " 4    0.045587   -0.144351    0.813560    0.025915  \n",
       " 5   -0.057141    0.270897    0.027002   -0.184260  \n",
       " 6   -0.016499   -0.199816    0.048790   -0.113365  \n",
       " 7    0.938068   -0.144888    0.051803   -0.062435  \n",
       " 8   -0.034262   -0.012009   -0.021915   -0.067686  , attention_weights_similarity_heatmap=                                                data  \\\n",
       " 0  [{'coloraxis': 'coloraxis', 'name': '0', 'z': ...   \n",
       " \n",
       "                                               layout  \n",
       " 0  {'template': {'data': {'barpolar': [{'marker':...  , attention_weights_similarity_matrix=array([[0.9985279 , 0.9985664 , 0.99855676, 0.99853633, 0.99857797,\n",
       "         0.9986076 , 0.99853178, 0.99863511, 0.99855474, 0.99856706,\n",
       "         0.99853985, 0.99857795],\n",
       "        [0.99879132, 0.99878674, 0.99879619, 0.99877834, 0.99884412,\n",
       "         0.99886935, 0.99879202, 0.9989181 , 0.99881713, 0.998762  ,\n",
       "         0.99881396, 0.99881276],\n",
       "        [0.99900976, 0.9989578 , 0.99901246, 0.99898973, 0.99904057,\n",
       "         0.99904928, 0.99901259, 0.9990505 , 0.99903868, 0.99897163,\n",
       "         0.99901967, 0.99901883],\n",
       "        [0.99900164, 0.99897562, 0.99900279, 0.99898612, 0.99903164,\n",
       "         0.9990071 , 0.99900648, 0.99903786, 0.99901297, 0.9989845 ,\n",
       "         0.99900207, 0.99899863],\n",
       "        [0.9990335 , 0.9990134 , 0.99903076, 0.99901392, 0.99905038,\n",
       "         0.99904733, 0.99904541, 0.99907686, 0.99904278, 0.99902916,\n",
       "         0.99903364, 0.99903109],\n",
       "        [0.99906121, 0.99906594, 0.9990448 , 0.99903885, 0.99909648,\n",
       "         0.9990675 , 0.99906695, 0.99909908, 0.99904844, 0.99907347,\n",
       "         0.99906058, 0.99905845],\n",
       "        [0.99904906, 0.99903366, 0.99901869, 0.9990247 , 0.99904497,\n",
       "         0.99903229, 0.99904187, 0.99907212, 0.99902431, 0.999024  ,\n",
       "         0.99903352, 0.99903079],\n",
       "        [0.99900908, 0.99903673, 0.99902534, 0.99901628, 0.9990364 ,\n",
       "         0.9990337 , 0.9990356 , 0.99904615, 0.9990088 , 0.99899307,\n",
       "         0.9990171 , 0.99903006],\n",
       "        [0.99912875, 0.9991608 , 0.99914254, 0.99912834, 0.99916654,\n",
       "         0.99915699, 0.99918906, 0.99916902, 0.99912289, 0.99914506,\n",
       "         0.99915777, 0.99916038],\n",
       "        [0.99926418, 0.99925771, 0.99924722, 0.9992584 , 0.99929245,\n",
       "         0.9992709 , 0.99925685, 0.99926692, 0.9992415 , 0.9992755 ,\n",
       "         0.99928527, 0.99928245],\n",
       "        [0.99938614, 0.99938545, 0.99939421, 0.99938449, 0.99941366,\n",
       "         0.99937293, 0.99939411, 0.99938852, 0.9993884 , 0.99940462,\n",
       "         0.99940655, 0.99941164],\n",
       "        [0.99945043, 0.99945355, 0.99943499, 0.99943753, 0.99945683,\n",
       "         0.99943901, 0.99944382, 0.99943713, 0.99945491, 0.99945539,\n",
       "         0.99946616, 0.99946221]]), attention_similarity_heatmap=                                                data  \\\n",
       " 0  [{'coloraxis': 'coloraxis', 'name': '0', 'z': ...   \n",
       " \n",
       "                                               layout  \n",
       " 0  {'template': {'data': {'barpolar': [{'marker':...  , attention_similarity_matrix=array([[0.96622194, 0.91649526, 0.98078069, 0.97170165, 0.98192725,\n",
       "         0.94059137, 0.98101486, 0.97354677, 0.98299081, 0.98458529,\n",
       "         0.97183117, 0.98551212],\n",
       "        [0.9155131 , 0.95052134, 0.96468241, 0.9243355 , 0.94781898,\n",
       "         0.93405361, 0.93259284, 0.9660278 , 0.95976176, 0.9625945 ,\n",
       "         0.97201509, 0.99303646],\n",
       "        [0.95138778, 0.93604357, 0.92548266, 0.99549879, 0.9328807 ,\n",
       "         0.9041832 , 0.90804665, 0.9561366 , 0.8990121 , 0.93438508,\n",
       "         0.95136602, 0.9084689 ],\n",
       "        [0.86108768, 0.93635957, 0.94301917, 0.91261015, 0.94143015,\n",
       "         0.91410184, 0.87001431, 0.91570199, 0.76524741, 0.9450571 ,\n",
       "         0.88044566, 0.99452642],\n",
       "        [0.85291248, 0.88726389, 0.91766554, 0.95780142, 0.94431172,\n",
       "         0.90075895, 0.97657718, 0.96913061, 0.93054592, 0.9384918 ,\n",
       "         0.85870402, 0.95729258],\n",
       "        [0.90769426, 0.97605882, 0.96848901, 0.96651403, 0.91902228,\n",
       "         0.95157087, 0.89690605, 0.91836394, 0.91234904, 0.93559951,\n",
       "         0.93821545, 0.98124713],\n",
       "        [0.89520352, 0.87110171, 0.90898989, 0.94275427, 0.95408847,\n",
       "         0.8687692 , 0.96768639, 0.90089978, 0.94221531, 0.91553149,\n",
       "         0.88165593, 0.87046749],\n",
       "        [0.83200636, 0.8829421 , 0.93505081, 0.84386919, 0.82886986,\n",
       "         0.88183304, 0.90245297, 0.88500974, 0.89599569, 0.96565268,\n",
       "         0.87025803, 0.88980512],\n",
       "        [0.83615112, 0.92646664, 0.67664213, 0.90106022, 0.95068678,\n",
       "         0.86016316, 0.78599621, 0.79919136, 0.84871818, 0.85909074,\n",
       "         0.87484121, 0.82268614],\n",
       "        [0.83008937, 0.62911284, 0.64000089, 0.71435354, 0.84866058,\n",
       "         0.66852532, 0.82166497, 0.79420136, 0.70688476, 0.89450096,\n",
       "         0.70827325, 0.8000891 ],\n",
       "        [0.69676367, 0.61348078, 0.72368352, 0.89723282, 0.8144622 ,\n",
       "         0.86173018, 0.6434729 , 0.59941183, 0.79907111, 0.82155365,\n",
       "         0.69300939, 0.81594268],\n",
       "        [0.90567732, 0.72986877, 0.62098815, 0.65345114, 0.3936852 ,\n",
       "         0.86827108, 0.8373561 , 0.89124562, 0.59544648, 0.65482999,\n",
       "         0.86309041, 0.63969274]]))}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_manager.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(data_manager.variants_data['ANERCorp_CamelLab_arabertv02'].attention_similarity_matrix, np.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_manager.variants_data['ANERCorp_CamelLab_arabertv02'].analysis_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Sentence Ids', 'Token Positions', 'Words', 'Tokens', 'Word Pieces',\n",
       "       'Core Tokens', 'True Labels', 'Token Selector Id', 'Pred Labels',\n",
       "       'Agreements', 'X', 'Y', 'Labels', 'Losses', 'Token Ids', 'Global Id',\n",
       "       'True Token Score', 'Pred Token Score', 'K=3', 'Boundary Clusters',\n",
       "       'K=4', 'Entity Clusters', 'K=9', 'Token Clusters', 'Consistency Count',\n",
       "       'Inconsistency Count', 'Total Train Occurrences', 'Local Token Entropy',\n",
       "       'Token Max Entropy', 'Dataset Token Entropy', 'Local Word Entropy',\n",
       "       'Word Max Entropy', 'Dataset Word Entropy', 'Tokenization Rate',\n",
       "       'TR Entity', 'PR Entity', 'Error Type', 'O Confidence',\n",
       "       'B-PERS Confidence', 'I-PERS Confidence', 'B-ORG Confidence',\n",
       "       'I-ORG Confidence', 'B-LOC Confidence', 'I-LOC Confidence',\n",
       "       'B-MISC Confidence', 'I-MISC Confidence', 'Prediction Entropy',\n",
       "       'Prediction Max Entropy', 'Token Confidence', 'Variability', 'Pre X',\n",
       "       'Pre Y', 'Normalized Token Entropy', 'Normalized Word Entropy',\n",
       "       'Consistency Ratio', 'Inconsistency Ratio'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             [CLS]\n",
       "1          الصالحية\n",
       "2            المفرق\n",
       "3                 -\n",
       "4               غيث\n",
       "            ...    \n",
       "29706    للو, ##ليد\n",
       "29707            بن\n",
       "29708          طلال\n",
       "29709             .\n",
       "29710         [SEP]\n",
       "Name: Word Pieces, Length: 29711, dtype: object"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Word Pieces'].apply(\n",
    "                lambda x: ', '.join(x) if isinstance(x, list) else x\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Raw Entropy  Max Entropy  Normalized Entropy\n",
      "0          0.0            0               0.000\n",
      "1         -1.0           -1              -1.000\n",
      "2          5.0           10               0.500\n",
      "3          2.5            4               0.625\n",
      "4          3.0            0                 inf\n",
      "5          7.0            0                 inf\n"
     ]
    }
   ],
   "source": [
    "# Example DataFrame\n",
    "data = {\n",
    "    'Raw Entropy': [0, -1, 5, 2.5, 3, 7],\n",
    "    'Max Entropy': [0, -1, 10, 4, 0, 0]  # Including zero and non-zero cases\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply the function\n",
    "df['Normalized Entropy'] = normalized_entropy(df, 'Raw Entropy', 'Max Entropy')\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "334"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Inconsistency Ratio'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Local Token Entropy\n",
       "False    16035\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Token Max Entropy']==0]['Local Token Entropy'].isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
